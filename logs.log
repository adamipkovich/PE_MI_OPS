2024-09-11 10:37:55,079:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-11 10:37:55,079:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-11 10:37:55,079:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-11 10:37:55,079:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-11 10:37:57,610:INFO:PyCaret ClassificationExperiment
2024-09-11 10:37:57,610:INFO:Logging name: clf-default-name
2024-09-11 10:37:57,610:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2024-09-11 10:37:57,611:INFO:version 3.3.2
2024-09-11 10:37:57,611:INFO:Initializing setup()
2024-09-11 10:37:57,611:INFO:self.USI: dbb4
2024-09-11 10:37:57,611:INFO:self._variable_keys: {'log_plots_param', 'USI', 'pipeline', 'y_test', 'y_train', 'y', 'X_train', 'X', 'exp_id', 'is_multiclass', 'n_jobs_param', 'X_test', 'gpu_n_jobs_param', '_ml_usecase', 'seed', 'data', 'idx', 'html_param', 'memory', 'fold_generator', 'fold_shuffle_param', 'gpu_param', '_available_plots', 'exp_name_log', 'fix_imbalance', 'logging_param', 'target_param', 'fold_groups_param'}
2024-09-11 10:37:57,611:INFO:Checking environment
2024-09-11 10:37:57,611:INFO:python_version: 3.11.8
2024-09-11 10:37:57,611:INFO:python_build: ('tags/v3.11.8:db85d51', 'Feb  6 2024 22:03:32')
2024-09-11 10:37:57,611:INFO:machine: AMD64
2024-09-11 10:37:57,611:INFO:platform: Windows-10-10.0.22631-SP0
2024-09-11 10:37:57,614:INFO:Memory: svmem(total=17096892416, available=6686035968, percent=60.9, used=10410856448, free=6686035968)
2024-09-11 10:37:57,614:INFO:Physical Core: 6
2024-09-11 10:37:57,614:INFO:Logical Core: 12
2024-09-11 10:37:57,614:INFO:Checking libraries
2024-09-11 10:37:57,614:INFO:System:
2024-09-11 10:37:57,614:INFO:    python: 3.11.8 (tags/v3.11.8:db85d51, Feb  6 2024, 22:03:32) [MSC v.1937 64 bit (AMD64)]
2024-09-11 10:37:57,614:INFO:executable: h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Scripts\python.exe
2024-09-11 10:37:57,614:INFO:   machine: Windows-10-10.0.22631-SP0
2024-09-11 10:37:57,614:INFO:PyCaret required dependencies:
2024-09-11 10:37:57,689:INFO:                 pip: 24.2
2024-09-11 10:37:57,689:INFO:          setuptools: 65.5.0
2024-09-11 10:37:57,689:INFO:             pycaret: 3.3.2
2024-09-11 10:37:57,689:INFO:             IPython: 8.27.0
2024-09-11 10:37:57,689:INFO:          ipywidgets: 8.1.5
2024-09-11 10:37:57,690:INFO:                tqdm: 4.66.5
2024-09-11 10:37:57,690:INFO:               numpy: 1.26.4
2024-09-11 10:37:57,690:INFO:              pandas: 2.1.4
2024-09-11 10:37:57,690:INFO:              jinja2: 3.1.4
2024-09-11 10:37:57,690:INFO:               scipy: 1.11.4
2024-09-11 10:37:57,690:INFO:              joblib: 1.3.2
2024-09-11 10:37:57,690:INFO:             sklearn: 1.4.2
2024-09-11 10:37:57,690:INFO:                pyod: 2.0.2
2024-09-11 10:37:57,690:INFO:            imblearn: 0.12.3
2024-09-11 10:37:57,690:INFO:   category_encoders: 2.6.3
2024-09-11 10:37:57,690:INFO:            lightgbm: 4.5.0
2024-09-11 10:37:57,691:INFO:               numba: 0.60.0
2024-09-11 10:37:57,691:INFO:            requests: 2.32.3
2024-09-11 10:37:57,691:INFO:          matplotlib: 3.7.5
2024-09-11 10:37:57,691:INFO:          scikitplot: 0.3.7
2024-09-11 10:37:57,691:INFO:         yellowbrick: 1.5
2024-09-11 10:37:57,691:INFO:              plotly: 5.24.0
2024-09-11 10:37:57,691:INFO:    plotly-resampler: Not installed
2024-09-11 10:37:57,691:INFO:             kaleido: 0.2.1
2024-09-11 10:37:57,691:INFO:           schemdraw: 0.15
2024-09-11 10:37:57,691:INFO:         statsmodels: 0.14.2
2024-09-11 10:37:57,691:INFO:              sktime: 0.26.0
2024-09-11 10:37:57,691:INFO:               tbats: 1.1.3
2024-09-11 10:37:57,691:INFO:            pmdarima: 2.0.4
2024-09-11 10:37:57,692:INFO:              psutil: 6.0.0
2024-09-11 10:37:57,692:INFO:          markupsafe: 2.1.5
2024-09-11 10:37:57,692:INFO:             pickle5: Not installed
2024-09-11 10:37:57,692:INFO:         cloudpickle: 3.0.0
2024-09-11 10:37:57,692:INFO:         deprecation: 2.1.0
2024-09-11 10:37:57,692:INFO:              xxhash: 3.5.0
2024-09-11 10:37:57,692:INFO:           wurlitzer: Not installed
2024-09-11 10:37:57,692:INFO:PyCaret optional dependencies:
2024-09-11 10:37:57,723:INFO:                shap: Not installed
2024-09-11 10:37:57,723:INFO:           interpret: Not installed
2024-09-11 10:37:57,723:INFO:                umap: Not installed
2024-09-11 10:37:57,723:INFO:     ydata_profiling: Not installed
2024-09-11 10:37:57,723:INFO:  explainerdashboard: Not installed
2024-09-11 10:37:57,723:INFO:             autoviz: Not installed
2024-09-11 10:37:57,723:INFO:           fairlearn: Not installed
2024-09-11 10:37:57,723:INFO:          deepchecks: Not installed
2024-09-11 10:37:57,723:INFO:             xgboost: Not installed
2024-09-11 10:37:57,723:INFO:            catboost: Not installed
2024-09-11 10:37:57,723:INFO:              kmodes: Not installed
2024-09-11 10:37:57,723:INFO:             mlxtend: Not installed
2024-09-11 10:37:57,724:INFO:       statsforecast: Not installed
2024-09-11 10:37:57,724:INFO:        tune_sklearn: Not installed
2024-09-11 10:37:57,724:INFO:                 ray: Not installed
2024-09-11 10:37:57,724:INFO:            hyperopt: Not installed
2024-09-11 10:37:57,724:INFO:              optuna: Not installed
2024-09-11 10:37:57,724:INFO:               skopt: Not installed
2024-09-11 10:37:57,724:INFO:              mlflow: Not installed
2024-09-11 10:37:57,724:INFO:              gradio: Not installed
2024-09-11 10:37:57,724:INFO:             fastapi: Not installed
2024-09-11 10:37:57,724:INFO:             uvicorn: Not installed
2024-09-11 10:37:57,724:INFO:              m2cgen: Not installed
2024-09-11 10:37:57,724:INFO:           evidently: Not installed
2024-09-11 10:37:57,725:INFO:               fugue: Not installed
2024-09-11 10:37:57,725:INFO:           streamlit: Not installed
2024-09-11 10:37:57,725:INFO:             prophet: Not installed
2024-09-11 10:37:57,725:INFO:None
2024-09-11 10:37:57,725:INFO:Set up data.
2024-09-11 10:37:57,737:INFO:Set up folding strategy.
2024-09-11 10:37:57,737:INFO:Set up train/test split.
2024-09-11 10:37:57,763:INFO:Set up index.
2024-09-11 10:37:57,763:INFO:Assigning column types.
2024-09-11 10:37:57,769:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2024-09-11 10:37:57,811:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-09-11 10:37:57,816:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-09-11 10:37:57,852:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-11 10:37:57,853:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-11 10:37:57,894:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-09-11 10:37:57,894:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-09-11 10:37:57,920:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-11 10:37:57,921:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-11 10:37:57,921:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2024-09-11 10:37:57,963:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-09-11 10:37:57,988:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-11 10:37:57,989:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-11 10:37:58,033:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-09-11 10:37:58,059:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-11 10:37:58,059:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-11 10:37:58,060:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2024-09-11 10:37:58,128:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-11 10:37:58,128:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-11 10:37:58,197:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-11 10:37:58,198:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-11 10:37:58,202:INFO:Preparing preprocessing pipeline...
2024-09-11 10:37:58,204:INFO:Set up label encoding.
2024-09-11 10:37:58,204:INFO:Set up simple imputation.
2024-09-11 10:37:58,207:INFO:Set up encoding of categorical features.
2024-09-11 10:37:58,329:INFO:Finished creating preprocessing pipeline.
2024-09-11 10:37:58,344:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\ipkov\AppData\Local\Temp\joblib),
         steps=[('label_encoding',
                 TransformerWrapperWithInverse(exclude=None, include=None,
                                               transformer=LabelEncoder())),
                ('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['MPG', 'Cylinders', 'Displacement',
                                             'Horsepower', 'Weight',
                                             'Acceleration', 'Model'],
                                    transformer=SimpleImp...
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='most_frequent'))),
                ('rest_encoding',
                 TransformerWrapper(exclude=None, include=['Car'],
                                    transformer=TargetEncoder(cols=['Car'],
                                                              drop_invariant=False,
                                                              handle_missing='return_nan',
                                                              handle_unknown='value',
                                                              hierarchy=None,
                                                              min_samples_leaf=20,
                                                              return_df=True,
                                                              smoothing=10,
                                                              verbose=0)))],
         verbose=False)
2024-09-11 10:37:58,344:INFO:Creating final display dataframe.
2024-09-11 10:37:58,749:INFO:Setup _display_container:                     Description                       Value
0                    Session id                        6678
1                        Target                      Origin
2                   Target type                  Multiclass
3                Target mapping  Europe: 0, Japan: 1, US: 2
4           Original data shape                    (406, 9)
5        Transformed data shape                    (406, 9)
6   Transformed train set shape                    (284, 9)
7    Transformed test set shape                    (122, 9)
8              Numeric features                           7
9          Categorical features                           1
10                   Preprocess                        True
11              Imputation type                      simple
12           Numeric imputation                        mean
13       Categorical imputation                        mode
14     Maximum one-hot encoding                          25
15              Encoding method                        None
16               Fold Generator             StratifiedKFold
17                  Fold Number                          10
18                     CPU Jobs                          -1
19                      Use GPU                       False
20               Log Experiment                       False
21              Experiment Name            clf-default-name
22                          USI                        dbb4
2024-09-11 10:37:58,833:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-11 10:37:58,834:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-11 10:37:58,904:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-11 10:37:58,904:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-11 10:37:58,908:INFO:setup() successfully completed in 1.3s...............
2024-09-11 10:37:58,908:INFO:Initializing compare_models()
2024-09-11 10:37:58,908:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001EEEE005650>, include=None, exclude=None, fold=None, round=4, cross_validation=True, sort=Accuracy, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x000001EEEE005650>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'Accuracy', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>})
2024-09-11 10:37:58,908:INFO:Checking exceptions
2024-09-11 10:37:58,913:INFO:Preparing display monitor
2024-09-11 10:37:58,921:INFO:Initializing Logistic Regression
2024-09-11 10:37:58,921:INFO:Total runtime is 1.6621748606363933e-05 minutes
2024-09-11 10:37:58,921:INFO:SubProcess create_model() called ==================================
2024-09-11 10:37:58,921:INFO:Initializing create_model()
2024-09-11 10:37:58,922:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001EEEE005650>, estimator=lr, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001EEEE1D0110>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-11 10:37:58,922:INFO:Checking exceptions
2024-09-11 10:37:58,922:INFO:Importing libraries
2024-09-11 10:37:58,922:INFO:Copying training dataset
2024-09-11 10:37:58,929:INFO:Defining folds
2024-09-11 10:37:58,929:INFO:Declaring metric variables
2024-09-11 10:37:58,929:INFO:Importing untrained model
2024-09-11 10:37:58,930:INFO:Logistic Regression Imported successfully
2024-09-11 10:37:58,930:INFO:Starting cross validation
2024-09-11 10:37:58,933:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-11 10:38:15,067:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-11 10:38:15,110:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-11 10:38:15,119:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-11 10:38:15,157:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:38:15,168:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:15,180:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-11 10:38:15,180:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:15,190:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:15,201:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:38:15,211:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:15,212:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:38:15,219:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:15,223:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:15,231:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:15,234:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:15,243:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:15,287:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:38:15,302:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:15,304:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-11 10:38:15,317:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:15,332:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:15,348:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-11 10:38:15,403:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-11 10:38:15,410:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:38:15,420:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:15,429:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:15,435:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:15,439:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:38:15,445:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:15,446:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-11 10:38:15,451:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:15,462:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:15,476:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:38:15,477:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-11 10:38:15,484:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:15,493:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:15,500:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:15,513:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-11 10:38:15,518:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:38:15,524:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:15,531:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:15,540:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:15,542:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:38:15,549:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:15,559:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:15,568:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:15,580:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:38:15,589:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:15,596:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:15,601:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:15,612:INFO:Calculating mean and std
2024-09-11 10:38:15,614:INFO:Creating metrics dataframe
2024-09-11 10:38:15,619:INFO:Uploading results into container
2024-09-11 10:38:15,620:INFO:Uploading model into container now
2024-09-11 10:38:15,620:INFO:_master_model_container: 1
2024-09-11 10:38:15,620:INFO:_display_container: 2
2024-09-11 10:38:15,621:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=6678, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2024-09-11 10:38:15,621:INFO:create_model() successfully completed......................................
2024-09-11 10:38:15,716:INFO:SubProcess create_model() end ==================================
2024-09-11 10:38:15,716:INFO:Creating metrics dataframe
2024-09-11 10:38:15,720:INFO:Initializing K Neighbors Classifier
2024-09-11 10:38:15,720:INFO:Total runtime is 0.27999579111735023 minutes
2024-09-11 10:38:15,720:INFO:SubProcess create_model() called ==================================
2024-09-11 10:38:15,721:INFO:Initializing create_model()
2024-09-11 10:38:15,721:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001EEEE005650>, estimator=knn, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001EEEE1D0110>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-11 10:38:15,721:INFO:Checking exceptions
2024-09-11 10:38:15,721:INFO:Importing libraries
2024-09-11 10:38:15,721:INFO:Copying training dataset
2024-09-11 10:38:15,727:INFO:Defining folds
2024-09-11 10:38:15,728:INFO:Declaring metric variables
2024-09-11 10:38:15,728:INFO:Importing untrained model
2024-09-11 10:38:15,728:INFO:K Neighbors Classifier Imported successfully
2024-09-11 10:38:15,729:INFO:Starting cross validation
2024-09-11 10:38:15,731:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-11 10:38:16,416:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:16,431:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:16,432:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:16,432:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:16,441:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:16,445:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:16,450:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:16,452:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:16,452:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:16,464:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:16,467:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:16,471:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:16,473:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:16,477:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:16,484:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:16,491:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:16,500:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:16,509:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:16,512:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:16,518:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:16,528:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:16,530:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:16,535:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:16,545:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:20,567:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:20,569:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:20,573:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:20,575:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:20,577:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:20,580:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:20,597:INFO:Calculating mean and std
2024-09-11 10:38:20,598:INFO:Creating metrics dataframe
2024-09-11 10:38:20,602:INFO:Uploading results into container
2024-09-11 10:38:20,603:INFO:Uploading model into container now
2024-09-11 10:38:20,603:INFO:_master_model_container: 2
2024-09-11 10:38:20,603:INFO:_display_container: 2
2024-09-11 10:38:20,604:INFO:KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform')
2024-09-11 10:38:20,604:INFO:create_model() successfully completed......................................
2024-09-11 10:38:20,689:INFO:SubProcess create_model() end ==================================
2024-09-11 10:38:20,690:INFO:Creating metrics dataframe
2024-09-11 10:38:20,694:INFO:Initializing Naive Bayes
2024-09-11 10:38:20,694:INFO:Total runtime is 0.3628975709279378 minutes
2024-09-11 10:38:20,694:INFO:SubProcess create_model() called ==================================
2024-09-11 10:38:20,694:INFO:Initializing create_model()
2024-09-11 10:38:20,694:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001EEEE005650>, estimator=nb, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001EEEE1D0110>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-11 10:38:20,694:INFO:Checking exceptions
2024-09-11 10:38:20,694:INFO:Importing libraries
2024-09-11 10:38:20,694:INFO:Copying training dataset
2024-09-11 10:38:20,701:INFO:Defining folds
2024-09-11 10:38:20,701:INFO:Declaring metric variables
2024-09-11 10:38:20,701:INFO:Importing untrained model
2024-09-11 10:38:20,702:INFO:Naive Bayes Imported successfully
2024-09-11 10:38:20,702:INFO:Starting cross validation
2024-09-11 10:38:20,705:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-11 10:38:20,992:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:21,006:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:21,008:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:21,012:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:21,013:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:21,016:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:21,021:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:21,022:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:21,022:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:21,022:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:21,027:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:21,029:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:21,029:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 10:38:21,032:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:21,033:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:21,036:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:21,036:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:21,037:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 10:38:21,037:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:21,038:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:21,040:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:21,043:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:21,047:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:21,050:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:21,050:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:21,050:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:21,053:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:21,053:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:21,054:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:21,064:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:21,064:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:21,067:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:21,079:INFO:Calculating mean and std
2024-09-11 10:38:21,080:INFO:Creating metrics dataframe
2024-09-11 10:38:21,083:INFO:Uploading results into container
2024-09-11 10:38:21,083:INFO:Uploading model into container now
2024-09-11 10:38:21,084:INFO:_master_model_container: 3
2024-09-11 10:38:21,084:INFO:_display_container: 2
2024-09-11 10:38:21,084:INFO:GaussianNB(priors=None, var_smoothing=1e-09)
2024-09-11 10:38:21,084:INFO:create_model() successfully completed......................................
2024-09-11 10:38:21,145:INFO:SubProcess create_model() end ==================================
2024-09-11 10:38:21,145:INFO:Creating metrics dataframe
2024-09-11 10:38:21,149:INFO:Initializing Decision Tree Classifier
2024-09-11 10:38:21,149:INFO:Total runtime is 0.3704703291257222 minutes
2024-09-11 10:38:21,150:INFO:SubProcess create_model() called ==================================
2024-09-11 10:38:21,150:INFO:Initializing create_model()
2024-09-11 10:38:21,150:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001EEEE005650>, estimator=dt, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001EEEE1D0110>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-11 10:38:21,150:INFO:Checking exceptions
2024-09-11 10:38:21,150:INFO:Importing libraries
2024-09-11 10:38:21,150:INFO:Copying training dataset
2024-09-11 10:38:21,156:INFO:Defining folds
2024-09-11 10:38:21,156:INFO:Declaring metric variables
2024-09-11 10:38:21,156:INFO:Importing untrained model
2024-09-11 10:38:21,157:INFO:Decision Tree Classifier Imported successfully
2024-09-11 10:38:21,157:INFO:Starting cross validation
2024-09-11 10:38:21,160:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-11 10:38:21,437:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:21,437:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:21,443:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:21,447:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:21,452:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:21,452:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:21,453:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:21,459:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:21,463:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:21,464:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:21,467:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:21,468:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:21,468:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:21,470:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:21,474:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 10:38:21,476:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:21,478:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:21,481:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:21,481:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:21,482:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:21,483:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:21,487:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:21,488:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 10:38:21,488:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:21,494:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:21,496:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:21,498:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:21,500:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:21,502:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:21,503:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:21,504:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:21,522:INFO:Calculating mean and std
2024-09-11 10:38:21,523:INFO:Creating metrics dataframe
2024-09-11 10:38:21,527:INFO:Uploading results into container
2024-09-11 10:38:21,528:INFO:Uploading model into container now
2024-09-11 10:38:21,528:INFO:_master_model_container: 4
2024-09-11 10:38:21,528:INFO:_display_container: 2
2024-09-11 10:38:21,529:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=6678, splitter='best')
2024-09-11 10:38:21,529:INFO:create_model() successfully completed......................................
2024-09-11 10:38:21,588:INFO:SubProcess create_model() end ==================================
2024-09-11 10:38:21,588:INFO:Creating metrics dataframe
2024-09-11 10:38:21,593:INFO:Initializing SVM - Linear Kernel
2024-09-11 10:38:21,593:INFO:Total runtime is 0.37788011233011876 minutes
2024-09-11 10:38:21,593:INFO:SubProcess create_model() called ==================================
2024-09-11 10:38:21,594:INFO:Initializing create_model()
2024-09-11 10:38:21,594:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001EEEE005650>, estimator=svm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001EEEE1D0110>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-11 10:38:21,594:INFO:Checking exceptions
2024-09-11 10:38:21,594:INFO:Importing libraries
2024-09-11 10:38:21,594:INFO:Copying training dataset
2024-09-11 10:38:21,600:INFO:Defining folds
2024-09-11 10:38:21,600:INFO:Declaring metric variables
2024-09-11 10:38:21,600:INFO:Importing untrained model
2024-09-11 10:38:21,601:INFO:SVM - Linear Kernel Imported successfully
2024-09-11 10:38:21,601:INFO:Starting cross validation
2024-09-11 10:38:21,603:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-11 10:38:22,043:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:38:22,043:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:38:22,052:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:22,062:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:22,068:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:38:22,071:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:38:22,071:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:38:22,076:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:38:22,080:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:22,081:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:22,084:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 10:38:22,086:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:22,086:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:22,086:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 10:38:22,088:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:38:22,089:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:22,095:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:22,096:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:22,096:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:22,096:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:22,096:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:38:22,101:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:22,102:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:22,108:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 10:38:22,108:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:22,110:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 10:38:22,110:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:22,111:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:22,114:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 10:38:22,117:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 10:38:22,118:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 10:38:22,118:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:22,125:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:22,126:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:22,127:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:22,131:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:22,139:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:22,142:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:38:22,144:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:22,152:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:22,156:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:38:22,164:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:22,170:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:22,178:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:22,180:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:22,181:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 10:38:22,184:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:22,201:INFO:Calculating mean and std
2024-09-11 10:38:22,202:INFO:Creating metrics dataframe
2024-09-11 10:38:22,205:INFO:Uploading results into container
2024-09-11 10:38:22,205:INFO:Uploading model into container now
2024-09-11 10:38:22,206:INFO:_master_model_container: 5
2024-09-11 10:38:22,206:INFO:_display_container: 2
2024-09-11 10:38:22,208:INFO:SGDClassifier(alpha=0.0001, average=False, class_weight=None,
              early_stopping=False, epsilon=0.1, eta0=0.001, fit_intercept=True,
              l1_ratio=0.15, learning_rate='optimal', loss='hinge',
              max_iter=1000, n_iter_no_change=5, n_jobs=-1, penalty='l2',
              power_t=0.5, random_state=6678, shuffle=True, tol=0.001,
              validation_fraction=0.1, verbose=0, warm_start=False)
2024-09-11 10:38:22,208:INFO:create_model() successfully completed......................................
2024-09-11 10:38:22,267:INFO:SubProcess create_model() end ==================================
2024-09-11 10:38:22,268:INFO:Creating metrics dataframe
2024-09-11 10:38:22,271:INFO:Initializing Ridge Classifier
2024-09-11 10:38:22,271:INFO:Total runtime is 0.3891743024190266 minutes
2024-09-11 10:38:22,272:INFO:SubProcess create_model() called ==================================
2024-09-11 10:38:22,272:INFO:Initializing create_model()
2024-09-11 10:38:22,272:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001EEEE005650>, estimator=ridge, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001EEEE1D0110>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-11 10:38:22,272:INFO:Checking exceptions
2024-09-11 10:38:22,272:INFO:Importing libraries
2024-09-11 10:38:22,273:INFO:Copying training dataset
2024-09-11 10:38:22,279:INFO:Defining folds
2024-09-11 10:38:22,279:INFO:Declaring metric variables
2024-09-11 10:38:22,280:INFO:Importing untrained model
2024-09-11 10:38:22,280:INFO:Ridge Classifier Imported successfully
2024-09-11 10:38:22,281:INFO:Starting cross validation
2024-09-11 10:38:22,283:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-11 10:38:22,571:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:38:22,573:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:38:22,578:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:38:22,579:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:22,582:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:22,586:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:38:22,587:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:22,590:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:38:22,594:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:38:22,596:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:22,596:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:22,598:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:22,599:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:22,602:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:38:22,603:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:38:22,603:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:22,604:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:22,610:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:22,610:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:22,611:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:22,611:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:22,612:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:22,613:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:38:22,614:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:22,616:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:38:22,619:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:22,619:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:22,624:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:22,624:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 10:38:22,626:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:22,627:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:22,628:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:22,629:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:22,633:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:22,636:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:22,638:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:22,640:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:22,642:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:22,643:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:22,649:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:22,649:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:22,659:INFO:Calculating mean and std
2024-09-11 10:38:22,660:INFO:Creating metrics dataframe
2024-09-11 10:38:22,663:INFO:Uploading results into container
2024-09-11 10:38:22,663:INFO:Uploading model into container now
2024-09-11 10:38:22,664:INFO:_master_model_container: 6
2024-09-11 10:38:22,664:INFO:_display_container: 2
2024-09-11 10:38:22,664:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=6678, solver='auto',
                tol=0.0001)
2024-09-11 10:38:22,664:INFO:create_model() successfully completed......................................
2024-09-11 10:38:22,726:INFO:SubProcess create_model() end ==================================
2024-09-11 10:38:22,726:INFO:Creating metrics dataframe
2024-09-11 10:38:22,730:INFO:Initializing Random Forest Classifier
2024-09-11 10:38:22,730:INFO:Total runtime is 0.39682497183481846 minutes
2024-09-11 10:38:22,731:INFO:SubProcess create_model() called ==================================
2024-09-11 10:38:22,731:INFO:Initializing create_model()
2024-09-11 10:38:22,731:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001EEEE005650>, estimator=rf, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001EEEE1D0110>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-11 10:38:22,731:INFO:Checking exceptions
2024-09-11 10:38:22,731:INFO:Importing libraries
2024-09-11 10:38:22,731:INFO:Copying training dataset
2024-09-11 10:38:22,737:INFO:Defining folds
2024-09-11 10:38:22,737:INFO:Declaring metric variables
2024-09-11 10:38:22,737:INFO:Importing untrained model
2024-09-11 10:38:22,738:INFO:Random Forest Classifier Imported successfully
2024-09-11 10:38:22,738:INFO:Starting cross validation
2024-09-11 10:38:22,741:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-11 10:38:24,408:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:24,421:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:24,422:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:24,429:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:24,430:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:24,431:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:24,434:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:24,435:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:24,436:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:24,440:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:24,441:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:24,441:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:24,447:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:24,449:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:24,450:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:24,455:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:24,456:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:24,455:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:24,458:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:24,460:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:24,461:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:24,463:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 10:38:24,463:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:24,465:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:24,468:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 10:38:24,473:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:24,474:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:24,475:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:24,478:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:24,478:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:24,479:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:24,495:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:24,519:INFO:Calculating mean and std
2024-09-11 10:38:24,521:INFO:Creating metrics dataframe
2024-09-11 10:38:24,526:INFO:Uploading results into container
2024-09-11 10:38:24,528:INFO:Uploading model into container now
2024-09-11 10:38:24,528:INFO:_master_model_container: 7
2024-09-11 10:38:24,529:INFO:_display_container: 2
2024-09-11 10:38:24,530:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=6678, verbose=0,
                       warm_start=False)
2024-09-11 10:38:24,530:INFO:create_model() successfully completed......................................
2024-09-11 10:38:24,597:INFO:SubProcess create_model() end ==================================
2024-09-11 10:38:24,597:INFO:Creating metrics dataframe
2024-09-11 10:38:24,601:INFO:Initializing Quadratic Discriminant Analysis
2024-09-11 10:38:24,601:INFO:Total runtime is 0.4280048807462056 minutes
2024-09-11 10:38:24,602:INFO:SubProcess create_model() called ==================================
2024-09-11 10:38:24,602:INFO:Initializing create_model()
2024-09-11 10:38:24,602:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001EEEE005650>, estimator=qda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001EEEE1D0110>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-11 10:38:24,602:INFO:Checking exceptions
2024-09-11 10:38:24,602:INFO:Importing libraries
2024-09-11 10:38:24,602:INFO:Copying training dataset
2024-09-11 10:38:24,608:INFO:Defining folds
2024-09-11 10:38:24,609:INFO:Declaring metric variables
2024-09-11 10:38:24,609:INFO:Importing untrained model
2024-09-11 10:38:24,609:INFO:Quadratic Discriminant Analysis Imported successfully
2024-09-11 10:38:24,610:INFO:Starting cross validation
2024-09-11 10:38:24,612:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-11 10:38:24,862:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:38:24,871:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:24,874:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:38:24,883:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:24,883:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:38:24,883:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:38:24,886:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:24,890:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:38:24,891:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:24,898:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:24,899:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:38:24,899:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:24,900:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:24,904:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:38:24,908:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:24,908:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:24,909:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:24,910:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:38:24,913:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:24,914:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:24,914:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:24,916:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:24,916:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:38:24,921:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 10:38:24,923:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:38:24,923:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:24,923:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:24,924:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:24,924:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:24,929:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:24,930:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:24,931:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:24,931:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:24,938:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:24,939:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:24,941:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:24,941:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:24,943:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:24,948:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:24,951:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:24,963:INFO:Calculating mean and std
2024-09-11 10:38:24,964:INFO:Creating metrics dataframe
2024-09-11 10:38:24,967:INFO:Uploading results into container
2024-09-11 10:38:24,968:INFO:Uploading model into container now
2024-09-11 10:38:24,969:INFO:_master_model_container: 8
2024-09-11 10:38:24,969:INFO:_display_container: 2
2024-09-11 10:38:24,969:INFO:QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,
                              store_covariance=False, tol=0.0001)
2024-09-11 10:38:24,969:INFO:create_model() successfully completed......................................
2024-09-11 10:38:25,030:INFO:SubProcess create_model() end ==================================
2024-09-11 10:38:25,030:INFO:Creating metrics dataframe
2024-09-11 10:38:25,034:INFO:Initializing Ada Boost Classifier
2024-09-11 10:38:25,034:INFO:Total runtime is 0.4352257808049519 minutes
2024-09-11 10:38:25,034:INFO:SubProcess create_model() called ==================================
2024-09-11 10:38:25,034:INFO:Initializing create_model()
2024-09-11 10:38:25,034:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001EEEE005650>, estimator=ada, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001EEEE1D0110>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-11 10:38:25,035:INFO:Checking exceptions
2024-09-11 10:38:25,036:INFO:Importing libraries
2024-09-11 10:38:25,036:INFO:Copying training dataset
2024-09-11 10:38:25,041:INFO:Defining folds
2024-09-11 10:38:25,041:INFO:Declaring metric variables
2024-09-11 10:38:25,042:INFO:Importing untrained model
2024-09-11 10:38:25,042:INFO:Ada Boost Classifier Imported successfully
2024-09-11 10:38:25,043:INFO:Starting cross validation
2024-09-11 10:38:25,045:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-11 10:38:25,206:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-11 10:38:25,207:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-11 10:38:25,207:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-11 10:38:25,210:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-11 10:38:25,226:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-11 10:38:25,233:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-11 10:38:25,234:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-11 10:38:25,242:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-11 10:38:25,248:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-11 10:38:25,257:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-11 10:38:25,829:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:38:25,835:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:38:25,839:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:25,841:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:38:25,842:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:38:25,844:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:25,849:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:38:25,850:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:25,851:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:25,854:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:38:25,857:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:25,859:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:25,862:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:25,862:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:38:25,863:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:25,867:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:25,868:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:25,868:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:38:25,871:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:25,873:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:25,873:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:25,873:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:38:25,878:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:25,878:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:38:25,878:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:25,880:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:25,881:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 10:38:25,883:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:25,884:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:25,884:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:25,886:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:25,887:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 10:38:25,888:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:25,890:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:25,894:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:25,897:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:25,899:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:25,901:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:25,903:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:25,910:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:25,912:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:25,913:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:25,924:INFO:Calculating mean and std
2024-09-11 10:38:25,924:INFO:Creating metrics dataframe
2024-09-11 10:38:25,928:INFO:Uploading results into container
2024-09-11 10:38:25,928:INFO:Uploading model into container now
2024-09-11 10:38:25,929:INFO:_master_model_container: 9
2024-09-11 10:38:25,929:INFO:_display_container: 2
2024-09-11 10:38:25,929:INFO:AdaBoostClassifier(algorithm='SAMME.R', estimator=None, learning_rate=1.0,
                   n_estimators=50, random_state=6678)
2024-09-11 10:38:25,930:INFO:create_model() successfully completed......................................
2024-09-11 10:38:25,986:INFO:SubProcess create_model() end ==================================
2024-09-11 10:38:25,986:INFO:Creating metrics dataframe
2024-09-11 10:38:25,990:INFO:Initializing Gradient Boosting Classifier
2024-09-11 10:38:25,990:INFO:Total runtime is 0.4511582533518472 minutes
2024-09-11 10:38:25,990:INFO:SubProcess create_model() called ==================================
2024-09-11 10:38:25,991:INFO:Initializing create_model()
2024-09-11 10:38:25,991:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001EEEE005650>, estimator=gbc, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001EEEE1D0110>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-11 10:38:25,991:INFO:Checking exceptions
2024-09-11 10:38:25,991:INFO:Importing libraries
2024-09-11 10:38:25,991:INFO:Copying training dataset
2024-09-11 10:38:25,996:INFO:Defining folds
2024-09-11 10:38:25,996:INFO:Declaring metric variables
2024-09-11 10:38:25,996:INFO:Importing untrained model
2024-09-11 10:38:25,997:INFO:Gradient Boosting Classifier Imported successfully
2024-09-11 10:38:25,998:INFO:Starting cross validation
2024-09-11 10:38:26,000:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-11 10:38:27,404:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:38:27,408:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:38:27,414:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:27,417:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:27,430:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:27,434:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:27,438:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:38:27,443:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:27,444:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:38:27,446:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:38:27,448:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:27,448:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:38:27,450:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:27,451:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:38:27,452:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:27,454:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:27,457:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:27,461:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:27,461:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:38:27,464:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:27,466:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:38:27,468:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:27,470:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:27,471:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:27,472:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:27,473:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:27,476:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 10:38:27,476:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:27,479:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:27,480:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:38:27,480:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:27,481:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:27,481:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:27,484:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:27,486:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:27,486:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:27,486:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:27,491:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:27,496:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:27,496:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:27,499:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 10:38:27,504:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:27,515:INFO:Calculating mean and std
2024-09-11 10:38:27,516:INFO:Creating metrics dataframe
2024-09-11 10:38:27,519:INFO:Uploading results into container
2024-09-11 10:38:27,519:INFO:Uploading model into container now
2024-09-11 10:38:27,520:INFO:_master_model_container: 10
2024-09-11 10:38:27,520:INFO:_display_container: 2
2024-09-11 10:38:27,521:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=6678, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2024-09-11 10:38:27,521:INFO:create_model() successfully completed......................................
2024-09-11 10:38:27,580:INFO:SubProcess create_model() end ==================================
2024-09-11 10:38:27,580:INFO:Creating metrics dataframe
2024-09-11 10:38:27,584:INFO:Initializing Linear Discriminant Analysis
2024-09-11 10:38:27,584:INFO:Total runtime is 0.47773224115371693 minutes
2024-09-11 10:38:27,584:INFO:SubProcess create_model() called ==================================
2024-09-11 10:38:27,586:INFO:Initializing create_model()
2024-09-11 10:38:27,586:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001EEEE005650>, estimator=lda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001EEEE1D0110>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-11 10:38:27,586:INFO:Checking exceptions
2024-09-11 10:38:27,586:INFO:Importing libraries
2024-09-11 10:38:27,586:INFO:Copying training dataset
2024-09-11 10:38:27,592:INFO:Defining folds
2024-09-11 10:38:27,592:INFO:Declaring metric variables
2024-09-11 10:38:27,592:INFO:Importing untrained model
2024-09-11 10:38:27,593:INFO:Linear Discriminant Analysis Imported successfully
2024-09-11 10:38:27,593:INFO:Starting cross validation
2024-09-11 10:38:27,595:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-11 10:38:27,892:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:38:27,897:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:38:27,901:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:27,901:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:38:27,906:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:38:27,906:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:27,907:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:38:27,909:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:27,913:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:27,914:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:27,916:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:27,921:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:38:27,922:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:27,923:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:38:27,926:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:27,926:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:38:27,929:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:27,930:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:27,931:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:27,931:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:27,932:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:27,933:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 10:38:27,934:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:38:27,935:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:27,938:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:38:27,939:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:27,943:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:27,945:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:27,946:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:27,947:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:27,947:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:27,950:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:27,952:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:27,956:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 10:38:27,961:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:27,964:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:27,964:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:27,966:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:27,966:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:27,972:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:27,972:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:27,991:INFO:Calculating mean and std
2024-09-11 10:38:27,992:INFO:Creating metrics dataframe
2024-09-11 10:38:27,994:INFO:Uploading results into container
2024-09-11 10:38:27,994:INFO:Uploading model into container now
2024-09-11 10:38:27,996:INFO:_master_model_container: 11
2024-09-11 10:38:27,996:INFO:_display_container: 2
2024-09-11 10:38:27,997:INFO:LinearDiscriminantAnalysis(covariance_estimator=None, n_components=None,
                           priors=None, shrinkage=None, solver='svd',
                           store_covariance=False, tol=0.0001)
2024-09-11 10:38:27,997:INFO:create_model() successfully completed......................................
2024-09-11 10:38:28,058:INFO:SubProcess create_model() end ==================================
2024-09-11 10:38:28,058:INFO:Creating metrics dataframe
2024-09-11 10:38:28,062:INFO:Initializing Extra Trees Classifier
2024-09-11 10:38:28,062:INFO:Total runtime is 0.4856906255086262 minutes
2024-09-11 10:38:28,062:INFO:SubProcess create_model() called ==================================
2024-09-11 10:38:28,062:INFO:Initializing create_model()
2024-09-11 10:38:28,062:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001EEEE005650>, estimator=et, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001EEEE1D0110>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-11 10:38:28,063:INFO:Checking exceptions
2024-09-11 10:38:28,063:INFO:Importing libraries
2024-09-11 10:38:28,063:INFO:Copying training dataset
2024-09-11 10:38:28,067:INFO:Defining folds
2024-09-11 10:38:28,068:INFO:Declaring metric variables
2024-09-11 10:38:28,068:INFO:Importing untrained model
2024-09-11 10:38:28,069:INFO:Extra Trees Classifier Imported successfully
2024-09-11 10:38:28,069:INFO:Starting cross validation
2024-09-11 10:38:28,071:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-11 10:38:29,646:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:29,652:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:29,665:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:29,676:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:29,677:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:29,687:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:29,695:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:29,700:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:29,706:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:29,713:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:29,729:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:29,729:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:29,744:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:29,749:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:29,751:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:29,754:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:29,762:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:29,767:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:29,767:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:29,769:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:29,770:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:29,773:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 10:38:29,777:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:29,782:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:29,786:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:29,792:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:29,797:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:29,798:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 10:38:29,798:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:29,804:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:29,822:INFO:Calculating mean and std
2024-09-11 10:38:29,824:INFO:Creating metrics dataframe
2024-09-11 10:38:29,831:INFO:Uploading results into container
2024-09-11 10:38:29,831:INFO:Uploading model into container now
2024-09-11 10:38:29,832:INFO:_master_model_container: 12
2024-09-11 10:38:29,832:INFO:_display_container: 2
2024-09-11 10:38:29,833:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=6678, verbose=0,
                     warm_start=False)
2024-09-11 10:38:29,833:INFO:create_model() successfully completed......................................
2024-09-11 10:38:29,909:INFO:SubProcess create_model() end ==================================
2024-09-11 10:38:29,910:INFO:Creating metrics dataframe
2024-09-11 10:38:29,914:INFO:Initializing Light Gradient Boosting Machine
2024-09-11 10:38:29,914:INFO:Total runtime is 0.5165570298830667 minutes
2024-09-11 10:38:29,914:INFO:SubProcess create_model() called ==================================
2024-09-11 10:38:29,914:INFO:Initializing create_model()
2024-09-11 10:38:29,914:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001EEEE005650>, estimator=lightgbm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001EEEE1D0110>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-11 10:38:29,914:INFO:Checking exceptions
2024-09-11 10:38:29,915:INFO:Importing libraries
2024-09-11 10:38:29,915:INFO:Copying training dataset
2024-09-11 10:38:29,920:INFO:Defining folds
2024-09-11 10:38:29,920:INFO:Declaring metric variables
2024-09-11 10:38:29,920:INFO:Importing untrained model
2024-09-11 10:38:29,921:INFO:Light Gradient Boosting Machine Imported successfully
2024-09-11 10:38:29,922:INFO:Starting cross validation
2024-09-11 10:38:29,924:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-11 10:38:34,395:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:34,402:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:34,414:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:34,422:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:34,430:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:34,442:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:34,512:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:34,527:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:34,544:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:34,561:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:34,579:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:34,595:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:34,619:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:34,635:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:34,652:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:34,698:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:34,716:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:34,729:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 10:38:34,746:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:34,763:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:34,782:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:34,802:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:34,820:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:34,839:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:34,859:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:34,938:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:34,951:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:34,956:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 10:38:34,962:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:35,000:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:35,009:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:35,018:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:35,043:INFO:Calculating mean and std
2024-09-11 10:38:35,045:INFO:Creating metrics dataframe
2024-09-11 10:38:35,051:INFO:Uploading results into container
2024-09-11 10:38:35,052:INFO:Uploading model into container now
2024-09-11 10:38:35,053:INFO:_master_model_container: 13
2024-09-11 10:38:35,054:INFO:_display_container: 2
2024-09-11 10:38:35,057:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=6678, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-09-11 10:38:35,057:INFO:create_model() successfully completed......................................
2024-09-11 10:38:35,129:INFO:SubProcess create_model() end ==================================
2024-09-11 10:38:35,129:INFO:Creating metrics dataframe
2024-09-11 10:38:35,133:INFO:Initializing Dummy Classifier
2024-09-11 10:38:35,133:INFO:Total runtime is 0.6035357753435769 minutes
2024-09-11 10:38:35,133:INFO:SubProcess create_model() called ==================================
2024-09-11 10:38:35,133:INFO:Initializing create_model()
2024-09-11 10:38:35,134:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001EEEE005650>, estimator=dummy, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001EEEE1D0110>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-11 10:38:35,134:INFO:Checking exceptions
2024-09-11 10:38:35,134:INFO:Importing libraries
2024-09-11 10:38:35,134:INFO:Copying training dataset
2024-09-11 10:38:35,139:INFO:Defining folds
2024-09-11 10:38:35,139:INFO:Declaring metric variables
2024-09-11 10:38:35,139:INFO:Importing untrained model
2024-09-11 10:38:35,140:INFO:Dummy Classifier Imported successfully
2024-09-11 10:38:35,140:INFO:Starting cross validation
2024-09-11 10:38:35,142:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-11 10:38:35,468:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:35,471:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:35,489:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:35,492:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:35,497:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:35,501:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 10:38:35,505:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 10:38:35,512:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:35,514:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:35,516:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:35,517:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:35,520:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:35,526:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:35,528:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:35,531:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:35,534:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 10:38:35,538:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:35,538:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:35,544:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:35,546:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:35,549:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 10:38:35,549:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:35,551:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 10:38:35,553:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 10:38:35,554:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 10:38:35,556:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 10:38:35,558:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:35,563:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:35,564:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:35,565:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:35,566:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:35,569:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:35,575:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:35,584:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:35,590:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:35,591:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 10:38:35,594:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 10:38:35,596:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:35,599:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:38:35,612:INFO:Calculating mean and std
2024-09-11 10:38:35,613:INFO:Creating metrics dataframe
2024-09-11 10:38:35,616:INFO:Uploading results into container
2024-09-11 10:38:35,617:INFO:Uploading model into container now
2024-09-11 10:38:35,617:INFO:_master_model_container: 14
2024-09-11 10:38:35,617:INFO:_display_container: 2
2024-09-11 10:38:35,618:INFO:DummyClassifier(constant=None, random_state=6678, strategy='prior')
2024-09-11 10:38:35,618:INFO:create_model() successfully completed......................................
2024-09-11 10:38:35,680:INFO:SubProcess create_model() end ==================================
2024-09-11 10:38:35,680:INFO:Creating metrics dataframe
2024-09-11 10:38:35,686:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:339: FutureWarning: Styler.applymap has been deprecated. Use Styler.map instead.
  .applymap(highlight_cols, subset=["TT (Sec)"])

2024-09-11 10:38:35,688:INFO:Initializing create_model()
2024-09-11 10:38:35,689:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001EEEE005650>, estimator=ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=6678, verbose=0,
                     warm_start=False), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-11 10:38:35,689:INFO:Checking exceptions
2024-09-11 10:38:35,690:INFO:Importing libraries
2024-09-11 10:38:35,691:INFO:Copying training dataset
2024-09-11 10:38:35,698:INFO:Defining folds
2024-09-11 10:38:35,698:INFO:Declaring metric variables
2024-09-11 10:38:35,698:INFO:Importing untrained model
2024-09-11 10:38:35,698:INFO:Declaring custom model
2024-09-11 10:38:35,700:INFO:Extra Trees Classifier Imported successfully
2024-09-11 10:38:35,702:INFO:Cross validation set to False
2024-09-11 10:38:35,702:INFO:Fitting Model
2024-09-11 10:38:35,968:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=6678, verbose=0,
                     warm_start=False)
2024-09-11 10:38:35,969:INFO:create_model() successfully completed......................................
2024-09-11 10:38:36,072:INFO:_master_model_container: 14
2024-09-11 10:38:36,072:INFO:_display_container: 2
2024-09-11 10:38:36,073:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=6678, verbose=0,
                     warm_start=False)
2024-09-11 10:38:36,073:INFO:compare_models() successfully completed......................................
2024-09-11 10:44:23,076:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-11 10:44:23,076:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-11 10:44:23,076:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-11 10:44:23,076:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-11 10:44:24,888:INFO:PyCaret ClassificationExperiment
2024-09-11 10:44:24,888:INFO:Logging name: clf-default-name
2024-09-11 10:44:24,888:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2024-09-11 10:44:24,888:INFO:version 3.3.2
2024-09-11 10:44:24,888:INFO:Initializing setup()
2024-09-11 10:44:24,889:INFO:self.USI: e44d
2024-09-11 10:44:24,889:INFO:self._variable_keys: {'exp_id', 'fold_groups_param', 'y', 'X', 'gpu_n_jobs_param', 'X_train', 'target_param', '_ml_usecase', 'logging_param', 'html_param', 'fix_imbalance', 'exp_name_log', 'X_test', 'fold_shuffle_param', 'data', 'gpu_param', 'log_plots_param', 'fold_generator', 'memory', 'y_test', 'y_train', 'idx', 'USI', 'pipeline', 'seed', 'n_jobs_param', 'is_multiclass', '_available_plots'}
2024-09-11 10:44:24,889:INFO:Checking environment
2024-09-11 10:44:24,889:INFO:python_version: 3.11.8
2024-09-11 10:44:24,889:INFO:python_build: ('tags/v3.11.8:db85d51', 'Feb  6 2024 22:03:32')
2024-09-11 10:44:24,889:INFO:machine: AMD64
2024-09-11 10:44:24,889:INFO:platform: Windows-10-10.0.22631-SP0
2024-09-11 10:44:24,892:INFO:Memory: svmem(total=17096892416, available=6388547584, percent=62.6, used=10708344832, free=6388547584)
2024-09-11 10:44:24,892:INFO:Physical Core: 6
2024-09-11 10:44:24,892:INFO:Logical Core: 12
2024-09-11 10:44:24,892:INFO:Checking libraries
2024-09-11 10:44:24,892:INFO:System:
2024-09-11 10:44:24,892:INFO:    python: 3.11.8 (tags/v3.11.8:db85d51, Feb  6 2024, 22:03:32) [MSC v.1937 64 bit (AMD64)]
2024-09-11 10:44:24,893:INFO:executable: h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Scripts\python.exe
2024-09-11 10:44:24,893:INFO:   machine: Windows-10-10.0.22631-SP0
2024-09-11 10:44:24,893:INFO:PyCaret required dependencies:
2024-09-11 10:44:24,938:INFO:                 pip: 24.2
2024-09-11 10:44:24,938:INFO:          setuptools: 65.5.0
2024-09-11 10:44:24,938:INFO:             pycaret: 3.3.2
2024-09-11 10:44:24,938:INFO:             IPython: 8.27.0
2024-09-11 10:44:24,939:INFO:          ipywidgets: 8.1.5
2024-09-11 10:44:24,939:INFO:                tqdm: 4.66.5
2024-09-11 10:44:24,939:INFO:               numpy: 1.26.4
2024-09-11 10:44:24,939:INFO:              pandas: 2.1.4
2024-09-11 10:44:24,939:INFO:              jinja2: 3.1.4
2024-09-11 10:44:24,939:INFO:               scipy: 1.11.4
2024-09-11 10:44:24,939:INFO:              joblib: 1.3.2
2024-09-11 10:44:24,939:INFO:             sklearn: 1.4.2
2024-09-11 10:44:24,939:INFO:                pyod: 2.0.2
2024-09-11 10:44:24,939:INFO:            imblearn: 0.12.3
2024-09-11 10:44:24,939:INFO:   category_encoders: 2.6.3
2024-09-11 10:44:24,939:INFO:            lightgbm: 4.5.0
2024-09-11 10:44:24,940:INFO:               numba: 0.60.0
2024-09-11 10:44:24,940:INFO:            requests: 2.32.3
2024-09-11 10:44:24,940:INFO:          matplotlib: 3.7.5
2024-09-11 10:44:24,940:INFO:          scikitplot: 0.3.7
2024-09-11 10:44:24,940:INFO:         yellowbrick: 1.5
2024-09-11 10:44:24,940:INFO:              plotly: 5.24.0
2024-09-11 10:44:24,940:INFO:    plotly-resampler: Not installed
2024-09-11 10:44:24,940:INFO:             kaleido: 0.2.1
2024-09-11 10:44:24,940:INFO:           schemdraw: 0.15
2024-09-11 10:44:24,940:INFO:         statsmodels: 0.14.2
2024-09-11 10:44:24,940:INFO:              sktime: 0.26.0
2024-09-11 10:44:24,940:INFO:               tbats: 1.1.3
2024-09-11 10:44:24,940:INFO:            pmdarima: 2.0.4
2024-09-11 10:44:24,941:INFO:              psutil: 6.0.0
2024-09-11 10:44:24,941:INFO:          markupsafe: 2.1.5
2024-09-11 10:44:24,941:INFO:             pickle5: Not installed
2024-09-11 10:44:24,941:INFO:         cloudpickle: 3.0.0
2024-09-11 10:44:24,941:INFO:         deprecation: 2.1.0
2024-09-11 10:44:24,941:INFO:              xxhash: 3.5.0
2024-09-11 10:44:24,941:INFO:           wurlitzer: Not installed
2024-09-11 10:44:24,941:INFO:PyCaret optional dependencies:
2024-09-11 10:44:24,973:INFO:                shap: Not installed
2024-09-11 10:44:24,973:INFO:           interpret: Not installed
2024-09-11 10:44:24,973:INFO:                umap: Not installed
2024-09-11 10:44:24,973:INFO:     ydata_profiling: Not installed
2024-09-11 10:44:24,973:INFO:  explainerdashboard: Not installed
2024-09-11 10:44:24,973:INFO:             autoviz: Not installed
2024-09-11 10:44:24,973:INFO:           fairlearn: Not installed
2024-09-11 10:44:24,973:INFO:          deepchecks: Not installed
2024-09-11 10:44:24,973:INFO:             xgboost: Not installed
2024-09-11 10:44:24,973:INFO:            catboost: Not installed
2024-09-11 10:44:24,973:INFO:              kmodes: Not installed
2024-09-11 10:44:24,973:INFO:             mlxtend: Not installed
2024-09-11 10:44:24,973:INFO:       statsforecast: Not installed
2024-09-11 10:44:24,973:INFO:        tune_sklearn: Not installed
2024-09-11 10:44:24,973:INFO:                 ray: Not installed
2024-09-11 10:44:24,973:INFO:            hyperopt: Not installed
2024-09-11 10:44:24,973:INFO:              optuna: Not installed
2024-09-11 10:44:24,973:INFO:               skopt: Not installed
2024-09-11 10:44:24,974:INFO:              mlflow: Not installed
2024-09-11 10:44:24,974:INFO:              gradio: Not installed
2024-09-11 10:44:24,974:INFO:             fastapi: Not installed
2024-09-11 10:44:24,974:INFO:             uvicorn: Not installed
2024-09-11 10:44:24,974:INFO:              m2cgen: Not installed
2024-09-11 10:44:24,974:INFO:           evidently: Not installed
2024-09-11 10:44:24,974:INFO:               fugue: Not installed
2024-09-11 10:44:24,974:INFO:           streamlit: Not installed
2024-09-11 10:44:24,974:INFO:             prophet: Not installed
2024-09-11 10:44:24,974:INFO:None
2024-09-11 10:44:24,975:INFO:Set up data.
2024-09-11 10:44:24,987:INFO:Set up folding strategy.
2024-09-11 10:44:24,987:INFO:Set up train/test split.
2024-09-11 10:44:24,998:INFO:Set up index.
2024-09-11 10:44:24,998:INFO:Assigning column types.
2024-09-11 10:44:25,004:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2024-09-11 10:44:25,045:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-09-11 10:44:25,051:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-09-11 10:44:25,088:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-11 10:44:25,089:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-11 10:44:25,131:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-09-11 10:44:25,132:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-09-11 10:44:25,157:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-11 10:44:25,157:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-11 10:44:25,158:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2024-09-11 10:44:25,200:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-09-11 10:44:25,226:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-11 10:44:25,226:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-11 10:44:25,267:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-09-11 10:44:25,293:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-11 10:44:25,294:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-11 10:44:25,294:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2024-09-11 10:44:25,363:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-11 10:44:25,363:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-11 10:44:25,430:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-11 10:44:25,431:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-11 10:44:25,435:INFO:Preparing preprocessing pipeline...
2024-09-11 10:44:25,436:INFO:Set up label encoding.
2024-09-11 10:44:25,436:INFO:Set up simple imputation.
2024-09-11 10:44:25,440:INFO:Set up encoding of categorical features.
2024-09-11 10:44:25,573:INFO:Finished creating preprocessing pipeline.
2024-09-11 10:44:25,589:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\ipkov\AppData\Local\Temp\joblib),
         steps=[('label_encoding',
                 TransformerWrapperWithInverse(exclude=None, include=None,
                                               transformer=LabelEncoder())),
                ('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['MPG', 'Cylinders', 'Displacement',
                                             'Horsepower', 'Weight',
                                             'Acceleration', 'Model'],
                                    transformer=SimpleImp...
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='most_frequent'))),
                ('rest_encoding',
                 TransformerWrapper(exclude=None, include=['Car'],
                                    transformer=TargetEncoder(cols=['Car'],
                                                              drop_invariant=False,
                                                              handle_missing='return_nan',
                                                              handle_unknown='value',
                                                              hierarchy=None,
                                                              min_samples_leaf=20,
                                                              return_df=True,
                                                              smoothing=10,
                                                              verbose=0)))],
         verbose=False)
2024-09-11 10:44:25,589:INFO:Creating final display dataframe.
2024-09-11 10:44:25,996:INFO:Setup _display_container:                     Description                       Value
0                    Session id                        5062
1                        Target                      Origin
2                   Target type                  Multiclass
3                Target mapping  Europe: 0, Japan: 1, US: 2
4           Original data shape                    (406, 9)
5        Transformed data shape                    (406, 9)
6   Transformed train set shape                    (284, 9)
7    Transformed test set shape                    (122, 9)
8              Numeric features                           7
9          Categorical features                           1
10                   Preprocess                        True
11              Imputation type                      simple
12           Numeric imputation                        mean
13       Categorical imputation                        mode
14     Maximum one-hot encoding                          25
15              Encoding method                        None
16               Fold Generator             StratifiedKFold
17                  Fold Number                          10
18                     CPU Jobs                          -1
19                      Use GPU                       False
20               Log Experiment                       False
21              Experiment Name            clf-default-name
22                          USI                        e44d
2024-09-11 10:44:26,077:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-11 10:44:26,077:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-11 10:44:26,153:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-11 10:44:26,154:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-11 10:44:26,156:INFO:setup() successfully completed in 1.27s...............
2024-09-11 10:44:26,156:INFO:Initializing compare_models()
2024-09-11 10:44:26,156:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000175A84C1D50>, include=None, exclude=None, fold=None, round=4, cross_validation=True, sort=Accuracy, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x00000175A84C1D50>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'Accuracy', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>})
2024-09-11 10:44:26,156:INFO:Checking exceptions
2024-09-11 10:44:26,161:INFO:Preparing display monitor
2024-09-11 10:44:26,168:INFO:Initializing Logistic Regression
2024-09-11 10:44:26,168:INFO:Total runtime is 0.0 minutes
2024-09-11 10:44:26,169:INFO:SubProcess create_model() called ==================================
2024-09-11 10:44:26,169:INFO:Initializing create_model()
2024-09-11 10:44:26,169:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000175A84C1D50>, estimator=lr, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000175A88C4C90>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-11 10:44:26,169:INFO:Checking exceptions
2024-09-11 10:44:26,170:INFO:Importing libraries
2024-09-11 10:44:26,170:INFO:Copying training dataset
2024-09-11 10:44:26,175:INFO:Defining folds
2024-09-11 10:44:26,175:INFO:Declaring metric variables
2024-09-11 10:44:26,176:INFO:Importing untrained model
2024-09-11 10:44:26,176:INFO:Logistic Regression Imported successfully
2024-09-11 10:44:26,177:INFO:Starting cross validation
2024-09-11 10:44:26,179:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-11 10:44:43,743:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-11 10:44:43,857:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:44:43,869:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:43,877:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-11 10:44:43,883:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:43,897:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:43,900:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-11 10:44:43,961:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-11 10:44:43,989:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:44:43,995:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-11 10:44:43,999:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:44,016:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:44,033:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:44,033:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:44:44,046:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:44,060:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-11 10:44:44,063:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:44,081:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:44:44,092:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:44,107:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:44,109:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:44:44,115:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:44,118:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:44,129:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:44,140:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:44,158:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:44:44,167:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:44,178:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:44,189:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:44,195:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-11 10:44:44,221:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-11 10:44:44,239:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-11 10:44:44,260:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:44:44,266:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:44,269:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-11 10:44:44,276:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:44,277:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:44:44,281:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:44,282:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:44:44,285:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:44,287:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:44,288:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:44,291:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 10:44:44,293:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:44,293:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:44,297:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:44,307:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:44:44,313:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:44,319:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:44,325:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:44,343:INFO:Calculating mean and std
2024-09-11 10:44:44,345:INFO:Creating metrics dataframe
2024-09-11 10:44:44,349:INFO:Uploading results into container
2024-09-11 10:44:44,350:INFO:Uploading model into container now
2024-09-11 10:44:44,351:INFO:_master_model_container: 1
2024-09-11 10:44:44,351:INFO:_display_container: 2
2024-09-11 10:44:44,352:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=5062, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2024-09-11 10:44:44,352:INFO:create_model() successfully completed......................................
2024-09-11 10:44:44,449:INFO:SubProcess create_model() end ==================================
2024-09-11 10:44:44,449:INFO:Creating metrics dataframe
2024-09-11 10:44:44,453:INFO:Initializing K Neighbors Classifier
2024-09-11 10:44:44,453:INFO:Total runtime is 0.30475070476531985 minutes
2024-09-11 10:44:44,453:INFO:SubProcess create_model() called ==================================
2024-09-11 10:44:44,453:INFO:Initializing create_model()
2024-09-11 10:44:44,453:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000175A84C1D50>, estimator=knn, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000175A88C4C90>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-11 10:44:44,453:INFO:Checking exceptions
2024-09-11 10:44:44,454:INFO:Importing libraries
2024-09-11 10:44:44,454:INFO:Copying training dataset
2024-09-11 10:44:44,460:INFO:Defining folds
2024-09-11 10:44:44,460:INFO:Declaring metric variables
2024-09-11 10:44:44,460:INFO:Importing untrained model
2024-09-11 10:44:44,461:INFO:K Neighbors Classifier Imported successfully
2024-09-11 10:44:44,461:INFO:Starting cross validation
2024-09-11 10:44:44,463:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-11 10:44:45,230:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:45,238:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:45,240:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:45,247:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:45,252:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:45,258:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:45,266:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:45,271:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:45,278:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:45,281:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:45,288:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:45,293:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:45,303:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:45,307:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:45,308:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:45,311:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:45,323:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:45,325:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:45,327:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:45,328:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:45,343:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:45,343:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:45,348:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:45,356:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:49,557:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:49,565:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:49,589:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:49,635:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:49,642:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:49,646:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:49,666:INFO:Calculating mean and std
2024-09-11 10:44:49,668:INFO:Creating metrics dataframe
2024-09-11 10:44:49,671:INFO:Uploading results into container
2024-09-11 10:44:49,672:INFO:Uploading model into container now
2024-09-11 10:44:49,672:INFO:_master_model_container: 2
2024-09-11 10:44:49,672:INFO:_display_container: 2
2024-09-11 10:44:49,673:INFO:KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform')
2024-09-11 10:44:49,673:INFO:create_model() successfully completed......................................
2024-09-11 10:44:49,767:INFO:SubProcess create_model() end ==================================
2024-09-11 10:44:49,767:INFO:Creating metrics dataframe
2024-09-11 10:44:49,773:INFO:Initializing Naive Bayes
2024-09-11 10:44:49,773:INFO:Total runtime is 0.39342240889867147 minutes
2024-09-11 10:44:49,773:INFO:SubProcess create_model() called ==================================
2024-09-11 10:44:49,773:INFO:Initializing create_model()
2024-09-11 10:44:49,773:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000175A84C1D50>, estimator=nb, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000175A88C4C90>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-11 10:44:49,773:INFO:Checking exceptions
2024-09-11 10:44:49,773:INFO:Importing libraries
2024-09-11 10:44:49,773:INFO:Copying training dataset
2024-09-11 10:44:49,781:INFO:Defining folds
2024-09-11 10:44:49,781:INFO:Declaring metric variables
2024-09-11 10:44:49,781:INFO:Importing untrained model
2024-09-11 10:44:49,781:INFO:Naive Bayes Imported successfully
2024-09-11 10:44:49,782:INFO:Starting cross validation
2024-09-11 10:44:49,783:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-11 10:44:50,177:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:50,188:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:50,193:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:50,197:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:50,205:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:50,206:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:50,206:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:50,213:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:50,218:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:50,218:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:50,223:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:50,223:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:50,231:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:50,231:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 10:44:50,233:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:50,238:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:50,238:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:50,242:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:50,242:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:50,247:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 10:44:50,248:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:50,253:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:50,257:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:50,258:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:50,258:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:50,260:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 10:44:50,261:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:50,264:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:50,265:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:50,266:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:50,266:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:50,273:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:50,273:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:50,291:INFO:Calculating mean and std
2024-09-11 10:44:50,291:INFO:Creating metrics dataframe
2024-09-11 10:44:50,295:INFO:Uploading results into container
2024-09-11 10:44:50,295:INFO:Uploading model into container now
2024-09-11 10:44:50,296:INFO:_master_model_container: 3
2024-09-11 10:44:50,296:INFO:_display_container: 2
2024-09-11 10:44:50,296:INFO:GaussianNB(priors=None, var_smoothing=1e-09)
2024-09-11 10:44:50,296:INFO:create_model() successfully completed......................................
2024-09-11 10:44:50,356:INFO:SubProcess create_model() end ==================================
2024-09-11 10:44:50,356:INFO:Creating metrics dataframe
2024-09-11 10:44:50,360:INFO:Initializing Decision Tree Classifier
2024-09-11 10:44:50,360:INFO:Total runtime is 0.4032058755556742 minutes
2024-09-11 10:44:50,361:INFO:SubProcess create_model() called ==================================
2024-09-11 10:44:50,361:INFO:Initializing create_model()
2024-09-11 10:44:50,361:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000175A84C1D50>, estimator=dt, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000175A88C4C90>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-11 10:44:50,361:INFO:Checking exceptions
2024-09-11 10:44:50,361:INFO:Importing libraries
2024-09-11 10:44:50,361:INFO:Copying training dataset
2024-09-11 10:44:50,368:INFO:Defining folds
2024-09-11 10:44:50,368:INFO:Declaring metric variables
2024-09-11 10:44:50,369:INFO:Importing untrained model
2024-09-11 10:44:50,370:INFO:Decision Tree Classifier Imported successfully
2024-09-11 10:44:50,371:INFO:Starting cross validation
2024-09-11 10:44:50,373:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-11 10:44:50,657:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:50,664:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:50,667:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:50,671:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:50,673:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:50,680:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:50,685:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:50,687:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 10:44:50,687:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:50,689:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:50,691:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:50,695:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:50,697:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:50,701:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:50,705:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:50,713:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:50,713:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:50,714:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:50,717:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:50,719:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 10:44:50,722:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:50,723:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:50,725:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:50,726:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:50,726:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:50,728:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:50,730:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:50,730:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 10:44:50,732:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:50,735:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:50,736:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:50,749:INFO:Calculating mean and std
2024-09-11 10:44:50,749:INFO:Creating metrics dataframe
2024-09-11 10:44:50,752:INFO:Uploading results into container
2024-09-11 10:44:50,753:INFO:Uploading model into container now
2024-09-11 10:44:50,753:INFO:_master_model_container: 4
2024-09-11 10:44:50,753:INFO:_display_container: 2
2024-09-11 10:44:50,755:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=5062, splitter='best')
2024-09-11 10:44:50,755:INFO:create_model() successfully completed......................................
2024-09-11 10:44:50,812:INFO:SubProcess create_model() end ==================================
2024-09-11 10:44:50,812:INFO:Creating metrics dataframe
2024-09-11 10:44:50,817:INFO:Initializing SVM - Linear Kernel
2024-09-11 10:44:50,817:INFO:Total runtime is 0.4108197530110677 minutes
2024-09-11 10:44:50,817:INFO:SubProcess create_model() called ==================================
2024-09-11 10:44:50,818:INFO:Initializing create_model()
2024-09-11 10:44:50,818:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000175A84C1D50>, estimator=svm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000175A88C4C90>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-11 10:44:50,818:INFO:Checking exceptions
2024-09-11 10:44:50,818:INFO:Importing libraries
2024-09-11 10:44:50,818:INFO:Copying training dataset
2024-09-11 10:44:50,823:INFO:Defining folds
2024-09-11 10:44:50,823:INFO:Declaring metric variables
2024-09-11 10:44:50,823:INFO:Importing untrained model
2024-09-11 10:44:50,824:INFO:SVM - Linear Kernel Imported successfully
2024-09-11 10:44:50,825:INFO:Starting cross validation
2024-09-11 10:44:50,826:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-11 10:44:51,276:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:44:51,286:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:51,292:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:44:51,303:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:51,304:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:51,313:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 10:44:51,314:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:44:51,318:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:51,322:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:51,324:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:51,326:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 10:44:51,332:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:44:51,343:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:51,345:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:44:51,346:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:51,351:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:51,352:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:44:51,352:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 10:44:51,355:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:44:51,361:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:51,364:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:44:51,369:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:51,369:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:51,369:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:51,370:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:51,375:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:44:51,376:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:51,379:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:51,383:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:44:51,386:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:51,387:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:51,387:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 10:44:51,389:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:51,390:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:51,393:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:51,395:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 10:44:51,397:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:51,397:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 10:44:51,403:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:51,405:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:51,406:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 10:44:51,407:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:51,409:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:51,412:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:51,413:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 10:44:51,415:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:51,421:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 10:44:51,423:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:51,429:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:51,452:INFO:Calculating mean and std
2024-09-11 10:44:51,453:INFO:Creating metrics dataframe
2024-09-11 10:44:51,455:INFO:Uploading results into container
2024-09-11 10:44:51,456:INFO:Uploading model into container now
2024-09-11 10:44:51,456:INFO:_master_model_container: 5
2024-09-11 10:44:51,457:INFO:_display_container: 2
2024-09-11 10:44:51,458:INFO:SGDClassifier(alpha=0.0001, average=False, class_weight=None,
              early_stopping=False, epsilon=0.1, eta0=0.001, fit_intercept=True,
              l1_ratio=0.15, learning_rate='optimal', loss='hinge',
              max_iter=1000, n_iter_no_change=5, n_jobs=-1, penalty='l2',
              power_t=0.5, random_state=5062, shuffle=True, tol=0.001,
              validation_fraction=0.1, verbose=0, warm_start=False)
2024-09-11 10:44:51,458:INFO:create_model() successfully completed......................................
2024-09-11 10:44:51,516:INFO:SubProcess create_model() end ==================================
2024-09-11 10:44:51,516:INFO:Creating metrics dataframe
2024-09-11 10:44:51,520:INFO:Initializing Ridge Classifier
2024-09-11 10:44:51,520:INFO:Total runtime is 0.4225389719009399 minutes
2024-09-11 10:44:51,521:INFO:SubProcess create_model() called ==================================
2024-09-11 10:44:51,521:INFO:Initializing create_model()
2024-09-11 10:44:51,521:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000175A84C1D50>, estimator=ridge, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000175A88C4C90>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-11 10:44:51,521:INFO:Checking exceptions
2024-09-11 10:44:51,521:INFO:Importing libraries
2024-09-11 10:44:51,521:INFO:Copying training dataset
2024-09-11 10:44:51,527:INFO:Defining folds
2024-09-11 10:44:51,527:INFO:Declaring metric variables
2024-09-11 10:44:51,528:INFO:Importing untrained model
2024-09-11 10:44:51,528:INFO:Ridge Classifier Imported successfully
2024-09-11 10:44:51,528:INFO:Starting cross validation
2024-09-11 10:44:51,532:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-11 10:44:51,881:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:44:51,882:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:44:51,884:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:44:51,891:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:51,892:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:51,894:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:51,895:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:44:51,902:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:44:51,904:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:51,907:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:51,907:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:44:51,908:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:51,914:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:51,916:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:51,916:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 10:44:51,918:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:51,921:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:44:51,922:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:51,926:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 10:44:51,928:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:44:51,928:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:51,931:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:51,932:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:51,934:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:51,937:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:51,938:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:51,940:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:51,941:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:51,945:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:44:51,947:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 10:44:51,953:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:51,954:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:44:51,955:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:51,955:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:51,957:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:51,964:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 10:44:51,965:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:51,973:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:51,976:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:51,982:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:51,984:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:51,990:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:52,005:INFO:Calculating mean and std
2024-09-11 10:44:52,006:INFO:Creating metrics dataframe
2024-09-11 10:44:52,009:INFO:Uploading results into container
2024-09-11 10:44:52,010:INFO:Uploading model into container now
2024-09-11 10:44:52,010:INFO:_master_model_container: 6
2024-09-11 10:44:52,010:INFO:_display_container: 2
2024-09-11 10:44:52,011:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=5062, solver='auto',
                tol=0.0001)
2024-09-11 10:44:52,011:INFO:create_model() successfully completed......................................
2024-09-11 10:44:52,073:INFO:SubProcess create_model() end ==================================
2024-09-11 10:44:52,073:INFO:Creating metrics dataframe
2024-09-11 10:44:52,077:INFO:Initializing Random Forest Classifier
2024-09-11 10:44:52,077:INFO:Total runtime is 0.4318196098009745 minutes
2024-09-11 10:44:52,077:INFO:SubProcess create_model() called ==================================
2024-09-11 10:44:52,077:INFO:Initializing create_model()
2024-09-11 10:44:52,077:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000175A84C1D50>, estimator=rf, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000175A88C4C90>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-11 10:44:52,078:INFO:Checking exceptions
2024-09-11 10:44:52,078:INFO:Importing libraries
2024-09-11 10:44:52,078:INFO:Copying training dataset
2024-09-11 10:44:52,083:INFO:Defining folds
2024-09-11 10:44:52,083:INFO:Declaring metric variables
2024-09-11 10:44:52,083:INFO:Importing untrained model
2024-09-11 10:44:52,084:INFO:Random Forest Classifier Imported successfully
2024-09-11 10:44:52,084:INFO:Starting cross validation
2024-09-11 10:44:52,086:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-11 10:44:54,183:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:54,203:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:54,210:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:54,228:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:54,230:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:54,242:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:54,243:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:54,250:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:54,254:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 10:44:54,265:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:54,268:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:54,272:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:54,286:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:54,392:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:54,412:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:54,425:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:54,448:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:54,455:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:54,457:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:54,460:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:54,464:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:54,471:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 10:44:54,474:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:54,476:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:54,476:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:54,480:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:54,488:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:54,490:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:54,491:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:54,492:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:54,507:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:54,527:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:54,544:INFO:Calculating mean and std
2024-09-11 10:44:54,546:INFO:Creating metrics dataframe
2024-09-11 10:44:54,551:INFO:Uploading results into container
2024-09-11 10:44:54,552:INFO:Uploading model into container now
2024-09-11 10:44:54,553:INFO:_master_model_container: 7
2024-09-11 10:44:54,553:INFO:_display_container: 2
2024-09-11 10:44:54,554:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=5062, verbose=0,
                       warm_start=False)
2024-09-11 10:44:54,554:INFO:create_model() successfully completed......................................
2024-09-11 10:44:54,622:INFO:SubProcess create_model() end ==================================
2024-09-11 10:44:54,622:INFO:Creating metrics dataframe
2024-09-11 10:44:54,627:INFO:Initializing Quadratic Discriminant Analysis
2024-09-11 10:44:54,627:INFO:Total runtime is 0.47432436943054196 minutes
2024-09-11 10:44:54,628:INFO:SubProcess create_model() called ==================================
2024-09-11 10:44:54,628:INFO:Initializing create_model()
2024-09-11 10:44:54,628:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000175A84C1D50>, estimator=qda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000175A88C4C90>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-11 10:44:54,628:INFO:Checking exceptions
2024-09-11 10:44:54,628:INFO:Importing libraries
2024-09-11 10:44:54,629:INFO:Copying training dataset
2024-09-11 10:44:54,635:INFO:Defining folds
2024-09-11 10:44:54,636:INFO:Declaring metric variables
2024-09-11 10:44:54,636:INFO:Importing untrained model
2024-09-11 10:44:54,637:INFO:Quadratic Discriminant Analysis Imported successfully
2024-09-11 10:44:54,638:INFO:Starting cross validation
2024-09-11 10:44:54,640:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-11 10:44:55,015:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:44:55,027:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:55,040:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:44:55,046:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:55,049:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:44:55,050:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:55,051:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:44:55,054:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 10:44:55,054:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:44:55,059:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:55,062:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:55,066:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:55,067:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:55,069:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:44:55,074:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:55,079:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:44:55,080:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:55,084:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 10:44:55,084:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:55,087:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:55,088:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 10:44:55,091:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:55,092:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:55,099:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:55,100:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:55,106:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:55,109:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:55,110:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:55,111:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:44:55,121:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:55,119:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:55,127:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:44:55,131:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:55,139:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:55,139:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:44:55,140:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:55,144:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:55,148:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:55,150:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:55,152:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:55,159:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:55,159:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:55,163:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:55,178:INFO:Calculating mean and std
2024-09-11 10:44:55,179:INFO:Creating metrics dataframe
2024-09-11 10:44:55,182:INFO:Uploading results into container
2024-09-11 10:44:55,183:INFO:Uploading model into container now
2024-09-11 10:44:55,183:INFO:_master_model_container: 8
2024-09-11 10:44:55,183:INFO:_display_container: 2
2024-09-11 10:44:55,184:INFO:QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,
                              store_covariance=False, tol=0.0001)
2024-09-11 10:44:55,184:INFO:create_model() successfully completed......................................
2024-09-11 10:44:55,277:INFO:SubProcess create_model() end ==================================
2024-09-11 10:44:55,277:INFO:Creating metrics dataframe
2024-09-11 10:44:55,281:INFO:Initializing Ada Boost Classifier
2024-09-11 10:44:55,281:INFO:Total runtime is 0.4852279384930928 minutes
2024-09-11 10:44:55,282:INFO:SubProcess create_model() called ==================================
2024-09-11 10:44:55,282:INFO:Initializing create_model()
2024-09-11 10:44:55,282:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000175A84C1D50>, estimator=ada, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000175A88C4C90>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-11 10:44:55,282:INFO:Checking exceptions
2024-09-11 10:44:55,282:INFO:Importing libraries
2024-09-11 10:44:55,283:INFO:Copying training dataset
2024-09-11 10:44:55,290:INFO:Defining folds
2024-09-11 10:44:55,290:INFO:Declaring metric variables
2024-09-11 10:44:55,290:INFO:Importing untrained model
2024-09-11 10:44:55,291:INFO:Ada Boost Classifier Imported successfully
2024-09-11 10:44:55,291:INFO:Starting cross validation
2024-09-11 10:44:55,293:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-11 10:44:55,466:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-11 10:44:55,473:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-11 10:44:55,487:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-11 10:44:55,494:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-11 10:44:55,505:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-11 10:44:55,507:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-11 10:44:55,510:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-11 10:44:55,523:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-11 10:44:55,528:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-11 10:44:55,535:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-11 10:44:56,143:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:44:56,150:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:44:56,155:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:56,158:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:44:56,159:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:56,164:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:44:56,166:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:56,170:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:44:56,173:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:44:56,173:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:56,174:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:56,174:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:44:56,175:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:56,179:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:56,181:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 10:44:56,183:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:56,184:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:56,189:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:44:56,190:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:56,191:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:56,191:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:56,197:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:56,200:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:44:56,201:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:56,202:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:56,203:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 10:44:56,208:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:56,209:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:56,210:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:56,213:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:56,213:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:44:56,213:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:56,215:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:56,222:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:56,224:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:56,224:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:56,228:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 10:44:56,229:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:56,232:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:56,234:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:56,245:INFO:Calculating mean and std
2024-09-11 10:44:56,246:INFO:Creating metrics dataframe
2024-09-11 10:44:56,249:INFO:Uploading results into container
2024-09-11 10:44:56,249:INFO:Uploading model into container now
2024-09-11 10:44:56,250:INFO:_master_model_container: 9
2024-09-11 10:44:56,250:INFO:_display_container: 2
2024-09-11 10:44:56,251:INFO:AdaBoostClassifier(algorithm='SAMME.R', estimator=None, learning_rate=1.0,
                   n_estimators=50, random_state=5062)
2024-09-11 10:44:56,251:INFO:create_model() successfully completed......................................
2024-09-11 10:44:56,309:INFO:SubProcess create_model() end ==================================
2024-09-11 10:44:56,310:INFO:Creating metrics dataframe
2024-09-11 10:44:56,315:INFO:Initializing Gradient Boosting Classifier
2024-09-11 10:44:56,315:INFO:Total runtime is 0.5024626771608988 minutes
2024-09-11 10:44:56,315:INFO:SubProcess create_model() called ==================================
2024-09-11 10:44:56,315:INFO:Initializing create_model()
2024-09-11 10:44:56,316:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000175A84C1D50>, estimator=gbc, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000175A88C4C90>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-11 10:44:56,316:INFO:Checking exceptions
2024-09-11 10:44:56,316:INFO:Importing libraries
2024-09-11 10:44:56,316:INFO:Copying training dataset
2024-09-11 10:44:56,323:INFO:Defining folds
2024-09-11 10:44:56,323:INFO:Declaring metric variables
2024-09-11 10:44:56,323:INFO:Importing untrained model
2024-09-11 10:44:56,324:INFO:Gradient Boosting Classifier Imported successfully
2024-09-11 10:44:56,325:INFO:Starting cross validation
2024-09-11 10:44:56,327:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-11 10:44:57,973:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:44:57,982:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:58,002:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:58,019:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:44:58,020:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:44:58,023:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:44:58,027:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:58,030:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:44:58,032:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:44:58,033:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:58,035:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:44:58,035:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:58,040:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:44:58,043:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:58,048:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:58,049:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:58,049:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:58,056:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:58,060:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:58,062:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:44:58,069:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:58,069:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:58,071:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:58,072:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:58,072:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:44:58,073:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:58,078:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 10:44:58,082:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:58,083:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:58,090:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:58,092:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:58,092:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:58,094:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:58,103:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 10:44:58,104:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:58,105:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:58,110:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 10:44:58,112:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:58,121:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:58,136:INFO:Calculating mean and std
2024-09-11 10:44:58,137:INFO:Creating metrics dataframe
2024-09-11 10:44:58,140:INFO:Uploading results into container
2024-09-11 10:44:58,141:INFO:Uploading model into container now
2024-09-11 10:44:58,141:INFO:_master_model_container: 10
2024-09-11 10:44:58,141:INFO:_display_container: 2
2024-09-11 10:44:58,142:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=5062, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2024-09-11 10:44:58,142:INFO:create_model() successfully completed......................................
2024-09-11 10:44:58,217:INFO:SubProcess create_model() end ==================================
2024-09-11 10:44:58,218:INFO:Creating metrics dataframe
2024-09-11 10:44:58,222:INFO:Initializing Linear Discriminant Analysis
2024-09-11 10:44:58,222:INFO:Total runtime is 0.5342460672060648 minutes
2024-09-11 10:44:58,223:INFO:SubProcess create_model() called ==================================
2024-09-11 10:44:58,223:INFO:Initializing create_model()
2024-09-11 10:44:58,223:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000175A84C1D50>, estimator=lda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000175A88C4C90>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-11 10:44:58,223:INFO:Checking exceptions
2024-09-11 10:44:58,223:INFO:Importing libraries
2024-09-11 10:44:58,223:INFO:Copying training dataset
2024-09-11 10:44:58,230:INFO:Defining folds
2024-09-11 10:44:58,230:INFO:Declaring metric variables
2024-09-11 10:44:58,230:INFO:Importing untrained model
2024-09-11 10:44:58,231:INFO:Linear Discriminant Analysis Imported successfully
2024-09-11 10:44:58,231:INFO:Starting cross validation
2024-09-11 10:44:58,233:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-11 10:44:58,606:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:44:58,630:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:58,637:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:44:58,647:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:58,647:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:58,652:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:44:58,657:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:44:58,658:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:44:58,660:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 10:44:58,662:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:44:58,666:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:58,666:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:58,666:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:58,669:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:58,672:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:58,673:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:58,683:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:58,683:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:44:58,684:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:58,687:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:44:58,688:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:58,690:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:58,694:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:58,698:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:58,697:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:58,698:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 10:44:58,699:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:58,703:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:58,715:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:44:58,716:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:58,717:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:58,718:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:58,721:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:58,724:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:58,728:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:44:58,728:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:58,733:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:58,735:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:58,745:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:58,746:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:58,748:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 10:44:58,753:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:44:58,763:INFO:Calculating mean and std
2024-09-11 10:44:58,763:INFO:Creating metrics dataframe
2024-09-11 10:44:58,766:INFO:Uploading results into container
2024-09-11 10:44:58,767:INFO:Uploading model into container now
2024-09-11 10:44:58,768:INFO:_master_model_container: 11
2024-09-11 10:44:58,768:INFO:_display_container: 2
2024-09-11 10:44:58,768:INFO:LinearDiscriminantAnalysis(covariance_estimator=None, n_components=None,
                           priors=None, shrinkage=None, solver='svd',
                           store_covariance=False, tol=0.0001)
2024-09-11 10:44:58,769:INFO:create_model() successfully completed......................................
2024-09-11 10:44:58,838:INFO:SubProcess create_model() end ==================================
2024-09-11 10:44:58,838:INFO:Creating metrics dataframe
2024-09-11 10:44:58,842:INFO:Initializing Extra Trees Classifier
2024-09-11 10:44:58,842:INFO:Total runtime is 0.5445806304613748 minutes
2024-09-11 10:44:58,842:INFO:SubProcess create_model() called ==================================
2024-09-11 10:44:58,843:INFO:Initializing create_model()
2024-09-11 10:44:58,843:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000175A84C1D50>, estimator=et, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000175A88C4C90>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-11 10:44:58,843:INFO:Checking exceptions
2024-09-11 10:44:58,843:INFO:Importing libraries
2024-09-11 10:44:58,843:INFO:Copying training dataset
2024-09-11 10:44:58,850:INFO:Defining folds
2024-09-11 10:44:58,850:INFO:Declaring metric variables
2024-09-11 10:44:58,850:INFO:Importing untrained model
2024-09-11 10:44:58,851:INFO:Extra Trees Classifier Imported successfully
2024-09-11 10:44:58,852:INFO:Starting cross validation
2024-09-11 10:44:58,854:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-11 10:45:01,002:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:45:01,003:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:45:01,003:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:45:01,008:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:45:01,018:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:45:01,021:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:45:01,022:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:45:01,023:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:45:01,023:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:45:01,026:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:45:01,028:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 10:45:01,028:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:45:01,029:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:45:01,038:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:45:01,043:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:45:01,044:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:45:01,046:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 10:45:01,048:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:45:01,049:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:45:01,050:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:45:01,053:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:45:01,055:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:45:01,055:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:45:01,065:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:45:01,066:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 10:45:01,070:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:45:01,072:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:45:01,079:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:45:01,084:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:45:01,095:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:45:01,179:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:45:01,185:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:45:01,191:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:45:01,205:INFO:Calculating mean and std
2024-09-11 10:45:01,207:INFO:Creating metrics dataframe
2024-09-11 10:45:01,210:INFO:Uploading results into container
2024-09-11 10:45:01,211:INFO:Uploading model into container now
2024-09-11 10:45:01,211:INFO:_master_model_container: 12
2024-09-11 10:45:01,211:INFO:_display_container: 2
2024-09-11 10:45:01,212:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=5062, verbose=0,
                     warm_start=False)
2024-09-11 10:45:01,212:INFO:create_model() successfully completed......................................
2024-09-11 10:45:01,288:INFO:SubProcess create_model() end ==================================
2024-09-11 10:45:01,289:INFO:Creating metrics dataframe
2024-09-11 10:45:01,293:INFO:Initializing Light Gradient Boosting Machine
2024-09-11 10:45:01,293:INFO:Total runtime is 0.5854289968808492 minutes
2024-09-11 10:45:01,293:INFO:SubProcess create_model() called ==================================
2024-09-11 10:45:01,293:INFO:Initializing create_model()
2024-09-11 10:45:01,294:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000175A84C1D50>, estimator=lightgbm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000175A88C4C90>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-11 10:45:01,294:INFO:Checking exceptions
2024-09-11 10:45:01,294:INFO:Importing libraries
2024-09-11 10:45:01,294:INFO:Copying training dataset
2024-09-11 10:45:01,300:INFO:Defining folds
2024-09-11 10:45:01,300:INFO:Declaring metric variables
2024-09-11 10:45:01,301:INFO:Importing untrained model
2024-09-11 10:45:01,302:INFO:Light Gradient Boosting Machine Imported successfully
2024-09-11 10:45:01,302:INFO:Starting cross validation
2024-09-11 10:45:01,304:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-11 10:45:08,127:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:45:08,146:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:45:08,162:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:45:08,164:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:45:08,183:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:45:08,199:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:45:08,308:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:45:08,331:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:45:08,340:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 10:45:08,348:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:45:08,395:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:45:08,427:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:45:08,457:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:45:08,561:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:45:08,580:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:45:08,596:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:45:08,629:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:45:08,648:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:45:08,670:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:45:08,688:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:45:08,707:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:45:08,711:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:45:08,717:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 10:45:08,729:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:45:08,730:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:45:08,738:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 10:45:08,748:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:45:08,843:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:45:08,855:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:45:08,866:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:45:08,870:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:45:08,881:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:45:08,891:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:45:08,917:INFO:Calculating mean and std
2024-09-11 10:45:08,920:INFO:Creating metrics dataframe
2024-09-11 10:45:08,927:INFO:Uploading results into container
2024-09-11 10:45:08,930:INFO:Uploading model into container now
2024-09-11 10:45:08,932:INFO:_master_model_container: 13
2024-09-11 10:45:08,932:INFO:_display_container: 2
2024-09-11 10:45:08,935:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=5062, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-09-11 10:45:08,935:INFO:create_model() successfully completed......................................
2024-09-11 10:45:09,033:INFO:SubProcess create_model() end ==================================
2024-09-11 10:45:09,033:INFO:Creating metrics dataframe
2024-09-11 10:45:09,038:INFO:Initializing Dummy Classifier
2024-09-11 10:45:09,038:INFO:Total runtime is 0.7145124713579813 minutes
2024-09-11 10:45:09,038:INFO:SubProcess create_model() called ==================================
2024-09-11 10:45:09,039:INFO:Initializing create_model()
2024-09-11 10:45:09,039:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000175A84C1D50>, estimator=dummy, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000175A88C4C90>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-11 10:45:09,039:INFO:Checking exceptions
2024-09-11 10:45:09,039:INFO:Importing libraries
2024-09-11 10:45:09,039:INFO:Copying training dataset
2024-09-11 10:45:09,045:INFO:Defining folds
2024-09-11 10:45:09,045:INFO:Declaring metric variables
2024-09-11 10:45:09,045:INFO:Importing untrained model
2024-09-11 10:45:09,046:INFO:Dummy Classifier Imported successfully
2024-09-11 10:45:09,046:INFO:Starting cross validation
2024-09-11 10:45:09,049:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-11 10:45:09,325:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:45:09,328:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:45:09,329:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:45:09,337:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:45:09,341:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:45:09,341:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:45:09,343:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:45:09,345:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:45:09,348:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 10:45:09,350:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 10:45:09,353:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 10:45:09,354:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:45:09,358:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:45:09,358:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:45:09,360:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 10:45:09,360:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:45:09,360:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:45:09,363:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:45:09,364:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:45:09,366:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 10:45:09,369:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:45:09,372:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:45:09,372:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:45:09,376:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:45:09,377:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:45:09,382:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:45:09,382:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:45:09,386:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 10:45:09,388:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 10:45:09,389:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:45:09,391:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 10:45:09,391:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:45:09,392:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:45:09,393:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 10:45:09,393:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:45:09,393:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:45:09,397:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 10:45:09,398:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:45:09,399:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:45:09,416:INFO:Calculating mean and std
2024-09-11 10:45:09,417:INFO:Creating metrics dataframe
2024-09-11 10:45:09,420:INFO:Uploading results into container
2024-09-11 10:45:09,421:INFO:Uploading model into container now
2024-09-11 10:45:09,421:INFO:_master_model_container: 14
2024-09-11 10:45:09,422:INFO:_display_container: 2
2024-09-11 10:45:09,422:INFO:DummyClassifier(constant=None, random_state=5062, strategy='prior')
2024-09-11 10:45:09,422:INFO:create_model() successfully completed......................................
2024-09-11 10:45:09,479:INFO:SubProcess create_model() end ==================================
2024-09-11 10:45:09,479:INFO:Creating metrics dataframe
2024-09-11 10:45:09,484:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:339: FutureWarning: Styler.applymap has been deprecated. Use Styler.map instead.
  .applymap(highlight_cols, subset=["TT (Sec)"])

2024-09-11 10:45:09,488:INFO:Initializing create_model()
2024-09-11 10:45:09,488:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000175A84C1D50>, estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=5062, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-11 10:45:09,488:INFO:Checking exceptions
2024-09-11 10:45:09,489:INFO:Importing libraries
2024-09-11 10:45:09,490:INFO:Copying training dataset
2024-09-11 10:45:09,494:INFO:Defining folds
2024-09-11 10:45:09,495:INFO:Declaring metric variables
2024-09-11 10:45:09,495:INFO:Importing untrained model
2024-09-11 10:45:09,495:INFO:Declaring custom model
2024-09-11 10:45:09,496:INFO:Logistic Regression Imported successfully
2024-09-11 10:45:09,498:INFO:Cross validation set to False
2024-09-11 10:45:09,499:INFO:Fitting Model
2024-09-11 10:45:09,730:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-11 10:45:09,731:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=5062, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2024-09-11 10:45:09,731:INFO:create_model() successfully completed......................................
2024-09-11 10:45:09,814:INFO:_master_model_container: 14
2024-09-11 10:45:09,814:INFO:_display_container: 2
2024-09-11 10:45:09,815:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=5062, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2024-09-11 10:45:09,815:INFO:compare_models() successfully completed......................................
2024-09-11 10:45:09,816:INFO:Initializing evaluate_model()
2024-09-11 10:45:09,817:INFO:evaluate_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000175A84C1D50>, estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=5062, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False), fold=None, fit_kwargs=None, plot_kwargs=None, feature_name=None, groups=None)
2024-09-11 10:45:09,832:INFO:Initializing plot_model()
2024-09-11 10:45:09,832:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000175A84C1D50>, estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=5062, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False), plot=pipeline, scale=1, save=False, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), fit_kwargs={}, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=False, system=True, display=None, display_format=None)
2024-09-11 10:45:09,832:INFO:Checking exceptions
2024-09-11 10:45:09,835:INFO:Preloading libraries
2024-09-11 10:45:09,835:INFO:Copying training dataset
2024-09-11 10:45:09,835:INFO:Plot type: pipeline
2024-09-11 10:45:18,247:INFO:Visual Rendered Successfully
2024-09-11 10:45:18,323:INFO:plot_model() successfully completed......................................
2024-09-11 10:46:01,914:INFO:Initializing plot_model()
2024-09-11 10:46:01,915:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000175A84C1D50>, estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=5062, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False), plot=confusion_matrix, scale=1, save=False, fold=None, fit_kwargs=None, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=True, system=True, display=None, display_format=None)
2024-09-11 10:46:01,915:INFO:Checking exceptions
2024-09-11 10:46:01,916:INFO:Preloading libraries
2024-09-11 10:46:01,916:INFO:Copying training dataset
2024-09-11 10:46:01,916:INFO:Plot type: confusion_matrix
2024-09-11 10:46:02,164:INFO:Fitting Model
2024-09-11 10:46:02,166:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\base.py:493: UserWarning: X does not have valid feature names, but LogisticRegression was fitted with feature names
  warnings.warn(

2024-09-11 10:46:02,166:INFO:Scoring test/hold-out set
2024-09-11 10:46:02,216:INFO:Visual Rendered Successfully
2024-09-11 10:46:02,347:INFO:plot_model() successfully completed......................................
2024-09-11 10:47:17,064:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-11 10:47:17,065:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-11 10:47:17,065:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-11 10:47:17,065:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-11 10:47:18,851:INFO:PyCaret ClassificationExperiment
2024-09-11 10:47:18,851:INFO:Logging name: clf-default-name
2024-09-11 10:47:18,851:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2024-09-11 10:47:18,851:INFO:version 3.3.2
2024-09-11 10:47:18,851:INFO:Initializing setup()
2024-09-11 10:47:18,852:INFO:self.USI: eed1
2024-09-11 10:47:18,852:INFO:self._variable_keys: {'exp_name_log', 'fold_generator', 'target_param', 'n_jobs_param', 'gpu_n_jobs_param', 'seed', 'USI', '_available_plots', 'logging_param', 'y', 'exp_id', 'data', 'log_plots_param', 'gpu_param', 'pipeline', '_ml_usecase', 'memory', 'idx', 'y_train', 'X_train', 'fold_shuffle_param', 'X', 'html_param', 'y_test', 'X_test', 'fold_groups_param', 'fix_imbalance', 'is_multiclass'}
2024-09-11 10:47:18,852:INFO:Checking environment
2024-09-11 10:47:18,852:INFO:python_version: 3.11.8
2024-09-11 10:47:18,852:INFO:python_build: ('tags/v3.11.8:db85d51', 'Feb  6 2024 22:03:32')
2024-09-11 10:47:18,852:INFO:machine: AMD64
2024-09-11 10:47:18,852:INFO:platform: Windows-10-10.0.22631-SP0
2024-09-11 10:47:18,856:INFO:Memory: svmem(total=17096892416, available=6715338752, percent=60.7, used=10381553664, free=6715338752)
2024-09-11 10:47:18,856:INFO:Physical Core: 6
2024-09-11 10:47:18,856:INFO:Logical Core: 12
2024-09-11 10:47:18,856:INFO:Checking libraries
2024-09-11 10:47:18,856:INFO:System:
2024-09-11 10:47:18,856:INFO:    python: 3.11.8 (tags/v3.11.8:db85d51, Feb  6 2024, 22:03:32) [MSC v.1937 64 bit (AMD64)]
2024-09-11 10:47:18,856:INFO:executable: h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Scripts\python.exe
2024-09-11 10:47:18,856:INFO:   machine: Windows-10-10.0.22631-SP0
2024-09-11 10:47:18,856:INFO:PyCaret required dependencies:
2024-09-11 10:47:18,900:INFO:                 pip: 24.2
2024-09-11 10:47:18,901:INFO:          setuptools: 65.5.0
2024-09-11 10:47:18,901:INFO:             pycaret: 3.3.2
2024-09-11 10:47:18,901:INFO:             IPython: 8.27.0
2024-09-11 10:47:18,901:INFO:          ipywidgets: 8.1.5
2024-09-11 10:47:18,901:INFO:                tqdm: 4.66.5
2024-09-11 10:47:18,901:INFO:               numpy: 1.26.4
2024-09-11 10:47:18,901:INFO:              pandas: 2.1.4
2024-09-11 10:47:18,901:INFO:              jinja2: 3.1.4
2024-09-11 10:47:18,901:INFO:               scipy: 1.11.4
2024-09-11 10:47:18,901:INFO:              joblib: 1.3.2
2024-09-11 10:47:18,901:INFO:             sklearn: 1.4.2
2024-09-11 10:47:18,902:INFO:                pyod: 2.0.2
2024-09-11 10:47:18,902:INFO:            imblearn: 0.12.3
2024-09-11 10:47:18,902:INFO:   category_encoders: 2.6.3
2024-09-11 10:47:18,902:INFO:            lightgbm: 4.5.0
2024-09-11 10:47:18,902:INFO:               numba: 0.60.0
2024-09-11 10:47:18,902:INFO:            requests: 2.32.3
2024-09-11 10:47:18,902:INFO:          matplotlib: 3.7.5
2024-09-11 10:47:18,902:INFO:          scikitplot: 0.3.7
2024-09-11 10:47:18,902:INFO:         yellowbrick: 1.5
2024-09-11 10:47:18,902:INFO:              plotly: 5.24.0
2024-09-11 10:47:18,902:INFO:    plotly-resampler: Not installed
2024-09-11 10:47:18,902:INFO:             kaleido: 0.2.1
2024-09-11 10:47:18,903:INFO:           schemdraw: 0.15
2024-09-11 10:47:18,903:INFO:         statsmodels: 0.14.2
2024-09-11 10:47:18,903:INFO:              sktime: 0.26.0
2024-09-11 10:47:18,903:INFO:               tbats: 1.1.3
2024-09-11 10:47:18,903:INFO:            pmdarima: 2.0.4
2024-09-11 10:47:18,903:INFO:              psutil: 6.0.0
2024-09-11 10:47:18,903:INFO:          markupsafe: 2.1.5
2024-09-11 10:47:18,903:INFO:             pickle5: Not installed
2024-09-11 10:47:18,903:INFO:         cloudpickle: 3.0.0
2024-09-11 10:47:18,903:INFO:         deprecation: 2.1.0
2024-09-11 10:47:18,903:INFO:              xxhash: 3.5.0
2024-09-11 10:47:18,903:INFO:           wurlitzer: Not installed
2024-09-11 10:47:18,903:INFO:PyCaret optional dependencies:
2024-09-11 10:47:18,934:INFO:                shap: Not installed
2024-09-11 10:47:18,934:INFO:           interpret: Not installed
2024-09-11 10:47:18,934:INFO:                umap: Not installed
2024-09-11 10:47:18,934:INFO:     ydata_profiling: Not installed
2024-09-11 10:47:18,934:INFO:  explainerdashboard: Not installed
2024-09-11 10:47:18,934:INFO:             autoviz: Not installed
2024-09-11 10:47:18,934:INFO:           fairlearn: Not installed
2024-09-11 10:47:18,934:INFO:          deepchecks: Not installed
2024-09-11 10:47:18,934:INFO:             xgboost: Not installed
2024-09-11 10:47:18,935:INFO:            catboost: Not installed
2024-09-11 10:47:18,935:INFO:              kmodes: Not installed
2024-09-11 10:47:18,935:INFO:             mlxtend: Not installed
2024-09-11 10:47:18,935:INFO:       statsforecast: Not installed
2024-09-11 10:47:18,935:INFO:        tune_sklearn: Not installed
2024-09-11 10:47:18,935:INFO:                 ray: Not installed
2024-09-11 10:47:18,935:INFO:            hyperopt: Not installed
2024-09-11 10:47:18,935:INFO:              optuna: Not installed
2024-09-11 10:47:18,935:INFO:               skopt: Not installed
2024-09-11 10:47:18,935:INFO:              mlflow: Not installed
2024-09-11 10:47:18,935:INFO:              gradio: Not installed
2024-09-11 10:47:18,935:INFO:             fastapi: Not installed
2024-09-11 10:47:18,935:INFO:             uvicorn: Not installed
2024-09-11 10:47:18,936:INFO:              m2cgen: Not installed
2024-09-11 10:47:18,936:INFO:           evidently: Not installed
2024-09-11 10:47:18,936:INFO:               fugue: Not installed
2024-09-11 10:47:18,936:INFO:           streamlit: Not installed
2024-09-11 10:47:18,936:INFO:             prophet: Not installed
2024-09-11 10:47:18,936:INFO:None
2024-09-11 10:47:18,936:INFO:Set up data.
2024-09-11 10:47:18,948:INFO:Set up folding strategy.
2024-09-11 10:47:18,948:INFO:Set up train/test split.
2024-09-11 10:47:18,958:INFO:Set up index.
2024-09-11 10:47:18,959:INFO:Assigning column types.
2024-09-11 10:47:18,965:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2024-09-11 10:47:19,005:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-09-11 10:47:19,011:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-09-11 10:47:19,053:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-11 10:47:19,053:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-11 10:47:19,095:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-09-11 10:47:19,096:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-09-11 10:47:19,122:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-11 10:47:19,123:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-11 10:47:19,123:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2024-09-11 10:47:19,166:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-09-11 10:47:19,193:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-11 10:47:19,193:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-11 10:47:19,236:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-09-11 10:47:19,263:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-11 10:47:19,263:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-11 10:47:19,263:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2024-09-11 10:47:19,331:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-11 10:47:19,332:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-11 10:47:19,408:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-11 10:47:19,409:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-11 10:47:19,414:INFO:Preparing preprocessing pipeline...
2024-09-11 10:47:19,416:INFO:Set up label encoding.
2024-09-11 10:47:19,416:INFO:Set up simple imputation.
2024-09-11 10:47:19,419:INFO:Set up encoding of categorical features.
2024-09-11 10:47:19,550:INFO:Finished creating preprocessing pipeline.
2024-09-11 10:47:19,568:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\ipkov\AppData\Local\Temp\joblib),
         steps=[('label_encoding',
                 TransformerWrapperWithInverse(exclude=None, include=None,
                                               transformer=LabelEncoder())),
                ('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['MPG', 'Cylinders', 'Displacement',
                                             'Horsepower', 'Weight',
                                             'Acceleration', 'Model'],
                                    transformer=SimpleImp...
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='most_frequent'))),
                ('rest_encoding',
                 TransformerWrapper(exclude=None, include=['Car'],
                                    transformer=TargetEncoder(cols=['Car'],
                                                              drop_invariant=False,
                                                              handle_missing='return_nan',
                                                              handle_unknown='value',
                                                              hierarchy=None,
                                                              min_samples_leaf=20,
                                                              return_df=True,
                                                              smoothing=10,
                                                              verbose=0)))],
         verbose=False)
2024-09-11 10:47:19,568:INFO:Creating final display dataframe.
2024-09-11 10:47:20,018:INFO:Setup _display_container:                     Description                       Value
0                    Session id                        2857
1                        Target                      Origin
2                   Target type                  Multiclass
3                Target mapping  Europe: 0, Japan: 1, US: 2
4           Original data shape                    (406, 9)
5        Transformed data shape                    (406, 9)
6   Transformed train set shape                    (284, 9)
7    Transformed test set shape                    (122, 9)
8              Numeric features                           7
9          Categorical features                           1
10                   Preprocess                        True
11              Imputation type                      simple
12           Numeric imputation                        mean
13       Categorical imputation                        mode
14     Maximum one-hot encoding                          25
15              Encoding method                        None
16               Fold Generator             StratifiedKFold
17                  Fold Number                          10
18                     CPU Jobs                          -1
19                      Use GPU                       False
20               Log Experiment                       False
21              Experiment Name            clf-default-name
22                          USI                        eed1
2024-09-11 10:47:20,107:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-11 10:47:20,107:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-11 10:47:20,181:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-11 10:47:20,181:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-11 10:47:20,184:INFO:setup() successfully completed in 1.34s...............
2024-09-11 10:47:20,184:INFO:Initializing compare_models()
2024-09-11 10:47:20,185:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000020282FB7310>, include=None, exclude=None, fold=None, round=4, cross_validation=True, sort=Accuracy, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x0000020282FB7310>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'Accuracy', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>})
2024-09-11 10:47:20,185:INFO:Checking exceptions
2024-09-11 10:47:20,190:INFO:Preparing display monitor
2024-09-11 10:47:20,202:INFO:Initializing Logistic Regression
2024-09-11 10:47:20,202:INFO:Total runtime is 0.0 minutes
2024-09-11 10:47:20,203:INFO:SubProcess create_model() called ==================================
2024-09-11 10:47:20,204:INFO:Initializing create_model()
2024-09-11 10:47:20,204:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000020282FB7310>, estimator=lr, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000202831FA090>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-11 10:47:20,204:INFO:Checking exceptions
2024-09-11 10:47:20,204:INFO:Importing libraries
2024-09-11 10:47:20,205:INFO:Copying training dataset
2024-09-11 10:47:20,214:INFO:Defining folds
2024-09-11 10:47:20,214:INFO:Declaring metric variables
2024-09-11 10:47:20,215:INFO:Importing untrained model
2024-09-11 10:47:20,216:INFO:Logistic Regression Imported successfully
2024-09-11 10:47:20,216:INFO:Starting cross validation
2024-09-11 10:47:20,219:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-11 10:47:36,405:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-11 10:47:36,514:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:47:36,525:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:36,536:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-11 10:47:36,541:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:36,558:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:36,646:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:47:36,658:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:36,676:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:36,690:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:36,713:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-11 10:47:36,728:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-11 10:47:36,792:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:47:36,800:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:36,811:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:36,813:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:47:36,819:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:36,820:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:36,830:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:36,840:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:36,871:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-11 10:47:36,881:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-11 10:47:36,951:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:47:36,955:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:36,963:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:36,963:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:47:36,968:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:36,970:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:36,980:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:36,988:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:37,034:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-11 10:47:37,034:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-11 10:47:37,084:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:47:37,088:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:47:37,088:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:37,093:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:37,094:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:37,099:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:37,102:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-11 10:47:37,103:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:37,112:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:37,159:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:47:37,165:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:37,171:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:37,172:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-11 10:47:37,176:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:37,212:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:47:37,218:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:37,223:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:37,232:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:37,247:INFO:Calculating mean and std
2024-09-11 10:47:37,249:INFO:Creating metrics dataframe
2024-09-11 10:47:37,252:INFO:Uploading results into container
2024-09-11 10:47:37,253:INFO:Uploading model into container now
2024-09-11 10:47:37,253:INFO:_master_model_container: 1
2024-09-11 10:47:37,254:INFO:_display_container: 2
2024-09-11 10:47:37,254:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=2857, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2024-09-11 10:47:37,254:INFO:create_model() successfully completed......................................
2024-09-11 10:47:37,323:INFO:SubProcess create_model() end ==================================
2024-09-11 10:47:37,323:INFO:Creating metrics dataframe
2024-09-11 10:47:37,327:INFO:Initializing K Neighbors Classifier
2024-09-11 10:47:37,327:INFO:Total runtime is 0.28540966113408406 minutes
2024-09-11 10:47:37,327:INFO:SubProcess create_model() called ==================================
2024-09-11 10:47:37,328:INFO:Initializing create_model()
2024-09-11 10:47:37,328:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000020282FB7310>, estimator=knn, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000202831FA090>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-11 10:47:37,328:INFO:Checking exceptions
2024-09-11 10:47:37,328:INFO:Importing libraries
2024-09-11 10:47:37,328:INFO:Copying training dataset
2024-09-11 10:47:37,333:INFO:Defining folds
2024-09-11 10:47:37,333:INFO:Declaring metric variables
2024-09-11 10:47:37,333:INFO:Importing untrained model
2024-09-11 10:47:37,334:INFO:K Neighbors Classifier Imported successfully
2024-09-11 10:47:37,334:INFO:Starting cross validation
2024-09-11 10:47:37,336:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-11 10:47:37,962:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:37,964:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:37,973:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:37,977:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:37,978:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:37,981:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:37,984:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:37,991:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:37,994:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:37,997:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:38,001:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:38,003:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:38,005:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:38,010:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:38,018:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:38,018:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:38,024:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:38,024:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:38,029:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:38,037:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:38,040:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:38,044:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:38,055:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:38,069:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:42,561:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:42,564:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:42,571:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:42,571:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:42,577:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:42,577:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:42,594:INFO:Calculating mean and std
2024-09-11 10:47:42,596:INFO:Creating metrics dataframe
2024-09-11 10:47:42,599:INFO:Uploading results into container
2024-09-11 10:47:42,600:INFO:Uploading model into container now
2024-09-11 10:47:42,601:INFO:_master_model_container: 2
2024-09-11 10:47:42,601:INFO:_display_container: 2
2024-09-11 10:47:42,601:INFO:KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform')
2024-09-11 10:47:42,602:INFO:create_model() successfully completed......................................
2024-09-11 10:47:42,689:INFO:SubProcess create_model() end ==================================
2024-09-11 10:47:42,689:INFO:Creating metrics dataframe
2024-09-11 10:47:42,693:INFO:Initializing Naive Bayes
2024-09-11 10:47:42,694:INFO:Total runtime is 0.37484943866729736 minutes
2024-09-11 10:47:42,694:INFO:SubProcess create_model() called ==================================
2024-09-11 10:47:42,694:INFO:Initializing create_model()
2024-09-11 10:47:42,694:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000020282FB7310>, estimator=nb, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000202831FA090>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-11 10:47:42,694:INFO:Checking exceptions
2024-09-11 10:47:42,694:INFO:Importing libraries
2024-09-11 10:47:42,694:INFO:Copying training dataset
2024-09-11 10:47:42,700:INFO:Defining folds
2024-09-11 10:47:42,700:INFO:Declaring metric variables
2024-09-11 10:47:42,701:INFO:Importing untrained model
2024-09-11 10:47:42,701:INFO:Naive Bayes Imported successfully
2024-09-11 10:47:42,701:INFO:Starting cross validation
2024-09-11 10:47:42,704:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-11 10:47:43,030:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:43,032:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:43,040:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:43,041:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:43,045:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:43,049:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:43,050:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:43,057:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:43,058:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:43,061:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:43,062:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:43,065:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:43,065:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:43,066:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:43,071:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:43,075:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:43,076:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:43,077:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:43,079:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:43,085:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:43,085:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:43,086:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:43,089:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:43,098:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:43,099:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:43,101:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:43,101:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:43,102:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:43,106:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 10:47:43,109:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:43,123:INFO:Calculating mean and std
2024-09-11 10:47:43,124:INFO:Creating metrics dataframe
2024-09-11 10:47:43,127:INFO:Uploading results into container
2024-09-11 10:47:43,128:INFO:Uploading model into container now
2024-09-11 10:47:43,128:INFO:_master_model_container: 3
2024-09-11 10:47:43,128:INFO:_display_container: 2
2024-09-11 10:47:43,129:INFO:GaussianNB(priors=None, var_smoothing=1e-09)
2024-09-11 10:47:43,129:INFO:create_model() successfully completed......................................
2024-09-11 10:47:43,186:INFO:SubProcess create_model() end ==================================
2024-09-11 10:47:43,186:INFO:Creating metrics dataframe
2024-09-11 10:47:43,191:INFO:Initializing Decision Tree Classifier
2024-09-11 10:47:43,191:INFO:Total runtime is 0.38315677642822266 minutes
2024-09-11 10:47:43,191:INFO:SubProcess create_model() called ==================================
2024-09-11 10:47:43,191:INFO:Initializing create_model()
2024-09-11 10:47:43,192:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000020282FB7310>, estimator=dt, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000202831FA090>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-11 10:47:43,192:INFO:Checking exceptions
2024-09-11 10:47:43,192:INFO:Importing libraries
2024-09-11 10:47:43,192:INFO:Copying training dataset
2024-09-11 10:47:43,198:INFO:Defining folds
2024-09-11 10:47:43,198:INFO:Declaring metric variables
2024-09-11 10:47:43,199:INFO:Importing untrained model
2024-09-11 10:47:43,199:INFO:Decision Tree Classifier Imported successfully
2024-09-11 10:47:43,200:INFO:Starting cross validation
2024-09-11 10:47:43,202:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-11 10:47:43,490:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:43,491:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:43,501:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:43,505:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:43,506:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:43,516:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:43,516:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:43,516:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:43,517:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:43,520:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:43,521:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:43,521:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:43,527:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:43,528:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:43,530:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:43,531:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:43,532:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:43,532:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:43,539:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:43,541:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:43,544:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:43,546:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 10:47:43,547:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:43,548:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:43,548:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:43,549:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:43,552:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:43,558:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:43,561:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:43,578:INFO:Calculating mean and std
2024-09-11 10:47:43,579:INFO:Creating metrics dataframe
2024-09-11 10:47:43,582:INFO:Uploading results into container
2024-09-11 10:47:43,582:INFO:Uploading model into container now
2024-09-11 10:47:43,584:INFO:_master_model_container: 4
2024-09-11 10:47:43,584:INFO:_display_container: 2
2024-09-11 10:47:43,584:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=2857, splitter='best')
2024-09-11 10:47:43,585:INFO:create_model() successfully completed......................................
2024-09-11 10:47:43,646:INFO:SubProcess create_model() end ==================================
2024-09-11 10:47:43,646:INFO:Creating metrics dataframe
2024-09-11 10:47:43,650:INFO:Initializing SVM - Linear Kernel
2024-09-11 10:47:43,650:INFO:Total runtime is 0.39080746173858644 minutes
2024-09-11 10:47:43,651:INFO:SubProcess create_model() called ==================================
2024-09-11 10:47:43,651:INFO:Initializing create_model()
2024-09-11 10:47:43,651:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000020282FB7310>, estimator=svm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000202831FA090>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-11 10:47:43,651:INFO:Checking exceptions
2024-09-11 10:47:43,651:INFO:Importing libraries
2024-09-11 10:47:43,651:INFO:Copying training dataset
2024-09-11 10:47:43,657:INFO:Defining folds
2024-09-11 10:47:43,658:INFO:Declaring metric variables
2024-09-11 10:47:43,658:INFO:Importing untrained model
2024-09-11 10:47:43,658:INFO:SVM - Linear Kernel Imported successfully
2024-09-11 10:47:43,660:INFO:Starting cross validation
2024-09-11 10:47:43,662:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-11 10:47:44,112:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:47:44,127:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:44,128:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:47:44,135:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:44,144:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:44,145:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:47:44,152:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 10:47:44,154:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:44,156:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:47:44,161:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:44,162:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:44,162:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:47:44,165:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:44,172:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:47:44,174:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:44,176:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:44,178:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:44,182:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:44,183:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 10:47:44,186:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:44,190:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 10:47:44,192:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:47:44,194:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:44,196:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:44,197:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:47:44,201:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 10:47:44,201:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:44,202:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:44,205:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:44,206:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:47:44,209:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:44,214:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:44,215:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 10:47:44,216:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:44,228:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:44,235:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 10:47:44,241:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:44,248:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:44,248:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:44,249:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 10:47:44,256:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:44,257:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 10:47:44,276:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:44,277:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:44,310:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:47:44,317:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:44,321:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:44,324:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 10:47:44,330:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:44,354:INFO:Calculating mean and std
2024-09-11 10:47:44,355:INFO:Creating metrics dataframe
2024-09-11 10:47:44,358:INFO:Uploading results into container
2024-09-11 10:47:44,359:INFO:Uploading model into container now
2024-09-11 10:47:44,359:INFO:_master_model_container: 5
2024-09-11 10:47:44,359:INFO:_display_container: 2
2024-09-11 10:47:44,360:INFO:SGDClassifier(alpha=0.0001, average=False, class_weight=None,
              early_stopping=False, epsilon=0.1, eta0=0.001, fit_intercept=True,
              l1_ratio=0.15, learning_rate='optimal', loss='hinge',
              max_iter=1000, n_iter_no_change=5, n_jobs=-1, penalty='l2',
              power_t=0.5, random_state=2857, shuffle=True, tol=0.001,
              validation_fraction=0.1, verbose=0, warm_start=False)
2024-09-11 10:47:44,360:INFO:create_model() successfully completed......................................
2024-09-11 10:47:44,425:INFO:SubProcess create_model() end ==================================
2024-09-11 10:47:44,425:INFO:Creating metrics dataframe
2024-09-11 10:47:44,429:INFO:Initializing Ridge Classifier
2024-09-11 10:47:44,430:INFO:Total runtime is 0.40380450487136843 minutes
2024-09-11 10:47:44,430:INFO:SubProcess create_model() called ==================================
2024-09-11 10:47:44,430:INFO:Initializing create_model()
2024-09-11 10:47:44,431:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000020282FB7310>, estimator=ridge, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000202831FA090>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-11 10:47:44,431:INFO:Checking exceptions
2024-09-11 10:47:44,431:INFO:Importing libraries
2024-09-11 10:47:44,431:INFO:Copying training dataset
2024-09-11 10:47:44,437:INFO:Defining folds
2024-09-11 10:47:44,437:INFO:Declaring metric variables
2024-09-11 10:47:44,438:INFO:Importing untrained model
2024-09-11 10:47:44,438:INFO:Ridge Classifier Imported successfully
2024-09-11 10:47:44,439:INFO:Starting cross validation
2024-09-11 10:47:44,441:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-11 10:47:44,731:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:47:44,732:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:47:44,742:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:44,744:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:47:44,745:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:44,760:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:47:44,760:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:44,759:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:44,765:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:47:44,770:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:44,774:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:44,776:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:44,776:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:47:44,776:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:44,778:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:44,783:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:47:44,788:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:47:44,792:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:44,794:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:44,797:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:44,797:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:44,797:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:44,798:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:44,802:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:47:44,808:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:44,811:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:44,814:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:44,816:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:44,823:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:44,825:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:47:44,829:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:44,836:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:44,837:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:44,837:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:44,843:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:44,847:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:44,852:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:44,855:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:44,859:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 10:47:44,862:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:44,876:INFO:Calculating mean and std
2024-09-11 10:47:44,878:INFO:Creating metrics dataframe
2024-09-11 10:47:44,881:INFO:Uploading results into container
2024-09-11 10:47:44,881:INFO:Uploading model into container now
2024-09-11 10:47:44,882:INFO:_master_model_container: 6
2024-09-11 10:47:44,882:INFO:_display_container: 2
2024-09-11 10:47:44,883:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=2857, solver='auto',
                tol=0.0001)
2024-09-11 10:47:44,883:INFO:create_model() successfully completed......................................
2024-09-11 10:47:44,944:INFO:SubProcess create_model() end ==================================
2024-09-11 10:47:44,944:INFO:Creating metrics dataframe
2024-09-11 10:47:44,949:INFO:Initializing Random Forest Classifier
2024-09-11 10:47:44,949:INFO:Total runtime is 0.4124475280443827 minutes
2024-09-11 10:47:44,950:INFO:SubProcess create_model() called ==================================
2024-09-11 10:47:44,950:INFO:Initializing create_model()
2024-09-11 10:47:44,950:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000020282FB7310>, estimator=rf, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000202831FA090>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-11 10:47:44,950:INFO:Checking exceptions
2024-09-11 10:47:44,950:INFO:Importing libraries
2024-09-11 10:47:44,950:INFO:Copying training dataset
2024-09-11 10:47:44,955:INFO:Defining folds
2024-09-11 10:47:44,955:INFO:Declaring metric variables
2024-09-11 10:47:44,956:INFO:Importing untrained model
2024-09-11 10:47:44,956:INFO:Random Forest Classifier Imported successfully
2024-09-11 10:47:44,957:INFO:Starting cross validation
2024-09-11 10:47:44,959:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-11 10:47:46,663:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:46,672:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:46,682:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:46,689:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:46,690:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:46,698:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:46,704:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:46,706:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:46,707:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:46,709:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:46,709:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:46,718:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:46,724:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:46,728:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:46,736:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:46,741:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:46,751:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:46,755:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:46,759:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:46,770:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:46,777:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:46,801:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:46,802:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:46,825:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:46,827:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:46,835:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:46,842:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:46,849:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:46,854:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:46,858:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:46,882:INFO:Calculating mean and std
2024-09-11 10:47:46,884:INFO:Creating metrics dataframe
2024-09-11 10:47:46,888:INFO:Uploading results into container
2024-09-11 10:47:46,888:INFO:Uploading model into container now
2024-09-11 10:47:46,889:INFO:_master_model_container: 7
2024-09-11 10:47:46,889:INFO:_display_container: 2
2024-09-11 10:47:46,891:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=2857, verbose=0,
                       warm_start=False)
2024-09-11 10:47:46,891:INFO:create_model() successfully completed......................................
2024-09-11 10:47:46,949:INFO:SubProcess create_model() end ==================================
2024-09-11 10:47:46,949:INFO:Creating metrics dataframe
2024-09-11 10:47:46,954:INFO:Initializing Quadratic Discriminant Analysis
2024-09-11 10:47:46,954:INFO:Total runtime is 0.44586710929870604 minutes
2024-09-11 10:47:46,955:INFO:SubProcess create_model() called ==================================
2024-09-11 10:47:46,955:INFO:Initializing create_model()
2024-09-11 10:47:46,955:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000020282FB7310>, estimator=qda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000202831FA090>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-11 10:47:46,955:INFO:Checking exceptions
2024-09-11 10:47:46,955:INFO:Importing libraries
2024-09-11 10:47:46,955:INFO:Copying training dataset
2024-09-11 10:47:46,961:INFO:Defining folds
2024-09-11 10:47:46,961:INFO:Declaring metric variables
2024-09-11 10:47:46,961:INFO:Importing untrained model
2024-09-11 10:47:46,961:INFO:Quadratic Discriminant Analysis Imported successfully
2024-09-11 10:47:46,962:INFO:Starting cross validation
2024-09-11 10:47:46,964:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-11 10:47:47,285:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:47:47,286:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:47:47,295:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:47:47,296:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:47,297:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:47,302:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:47:47,307:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:47,311:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:47,313:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:47:47,314:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:47,314:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:47,324:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:47,326:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:47,330:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:47:47,330:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:47,331:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:47:47,331:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:47,334:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:47:47,334:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:47,336:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:47:47,340:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:47,341:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:47,342:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:47,342:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:47,342:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:47,345:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:47,347:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:47,355:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:47:47,357:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:47,360:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:47,362:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:47,364:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:47,368:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 10:47:47,372:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:47,372:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:47,372:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:47,373:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:47,373:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:47,379:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:47,397:INFO:Calculating mean and std
2024-09-11 10:47:47,398:INFO:Creating metrics dataframe
2024-09-11 10:47:47,401:INFO:Uploading results into container
2024-09-11 10:47:47,401:INFO:Uploading model into container now
2024-09-11 10:47:47,402:INFO:_master_model_container: 8
2024-09-11 10:47:47,402:INFO:_display_container: 2
2024-09-11 10:47:47,402:INFO:QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,
                              store_covariance=False, tol=0.0001)
2024-09-11 10:47:47,402:INFO:create_model() successfully completed......................................
2024-09-11 10:47:47,468:INFO:SubProcess create_model() end ==================================
2024-09-11 10:47:47,468:INFO:Creating metrics dataframe
2024-09-11 10:47:47,472:INFO:Initializing Ada Boost Classifier
2024-09-11 10:47:47,472:INFO:Total runtime is 0.4545074542363485 minutes
2024-09-11 10:47:47,472:INFO:SubProcess create_model() called ==================================
2024-09-11 10:47:47,472:INFO:Initializing create_model()
2024-09-11 10:47:47,474:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000020282FB7310>, estimator=ada, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000202831FA090>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-11 10:47:47,474:INFO:Checking exceptions
2024-09-11 10:47:47,474:INFO:Importing libraries
2024-09-11 10:47:47,474:INFO:Copying training dataset
2024-09-11 10:47:47,480:INFO:Defining folds
2024-09-11 10:47:47,480:INFO:Declaring metric variables
2024-09-11 10:47:47,480:INFO:Importing untrained model
2024-09-11 10:47:47,481:INFO:Ada Boost Classifier Imported successfully
2024-09-11 10:47:47,481:INFO:Starting cross validation
2024-09-11 10:47:47,483:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-11 10:47:47,635:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-11 10:47:47,642:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-11 10:47:47,651:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-11 10:47:47,652:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-11 10:47:47,667:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-11 10:47:47,669:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-11 10:47:47,675:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-11 10:47:47,689:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-11 10:47:47,690:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-11 10:47:47,696:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-11 10:47:48,274:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:47:48,284:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:48,285:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:47:48,295:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:48,297:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:47:48,304:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:48,304:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:47:48,307:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:48,313:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:48,313:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:47:48,313:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:48,313:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:47:48,314:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:47:48,320:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:48,323:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:48,324:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:48,325:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:48,326:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:48,330:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:48,332:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:48,338:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:47:48,340:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:48,340:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:48,340:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:47:48,342:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:48,343:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:48,349:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:48,350:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:48,351:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:48,353:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:47:48,357:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:48,357:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:48,359:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:48,360:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:48,361:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:48,366:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:48,371:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:48,372:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:48,376:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 10:47:48,376:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:48,378:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:48,394:INFO:Calculating mean and std
2024-09-11 10:47:48,395:INFO:Creating metrics dataframe
2024-09-11 10:47:48,398:INFO:Uploading results into container
2024-09-11 10:47:48,399:INFO:Uploading model into container now
2024-09-11 10:47:48,399:INFO:_master_model_container: 9
2024-09-11 10:47:48,399:INFO:_display_container: 2
2024-09-11 10:47:48,400:INFO:AdaBoostClassifier(algorithm='SAMME.R', estimator=None, learning_rate=1.0,
                   n_estimators=50, random_state=2857)
2024-09-11 10:47:48,400:INFO:create_model() successfully completed......................................
2024-09-11 10:47:48,461:INFO:SubProcess create_model() end ==================================
2024-09-11 10:47:48,461:INFO:Creating metrics dataframe
2024-09-11 10:47:48,465:INFO:Initializing Gradient Boosting Classifier
2024-09-11 10:47:48,465:INFO:Total runtime is 0.4710471034049988 minutes
2024-09-11 10:47:48,465:INFO:SubProcess create_model() called ==================================
2024-09-11 10:47:48,466:INFO:Initializing create_model()
2024-09-11 10:47:48,466:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000020282FB7310>, estimator=gbc, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000202831FA090>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-11 10:47:48,466:INFO:Checking exceptions
2024-09-11 10:47:48,466:INFO:Importing libraries
2024-09-11 10:47:48,466:INFO:Copying training dataset
2024-09-11 10:47:48,472:INFO:Defining folds
2024-09-11 10:47:48,472:INFO:Declaring metric variables
2024-09-11 10:47:48,472:INFO:Importing untrained model
2024-09-11 10:47:48,473:INFO:Gradient Boosting Classifier Imported successfully
2024-09-11 10:47:48,473:INFO:Starting cross validation
2024-09-11 10:47:48,475:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-11 10:47:49,902:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:47:49,907:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:47:49,908:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:47:49,911:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:49,916:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:49,918:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:49,924:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:47:49,930:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:49,932:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:47:49,932:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:49,934:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:49,935:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:49,935:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:47:49,938:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:47:49,941:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:49,945:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:49,946:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:49,948:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:49,949:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:49,951:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:47:49,951:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:49,951:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:49,958:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:49,961:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:49,961:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:49,966:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:47:49,967:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:49,968:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:49,974:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:49,974:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:49,976:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:47:49,976:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:49,976:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:49,980:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:49,984:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:49,987:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:49,987:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:49,991:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:49,992:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:49,992:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 10:47:49,996:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:50,009:INFO:Calculating mean and std
2024-09-11 10:47:50,010:INFO:Creating metrics dataframe
2024-09-11 10:47:50,013:INFO:Uploading results into container
2024-09-11 10:47:50,013:INFO:Uploading model into container now
2024-09-11 10:47:50,014:INFO:_master_model_container: 10
2024-09-11 10:47:50,014:INFO:_display_container: 2
2024-09-11 10:47:50,015:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=2857, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2024-09-11 10:47:50,015:INFO:create_model() successfully completed......................................
2024-09-11 10:47:50,073:INFO:SubProcess create_model() end ==================================
2024-09-11 10:47:50,073:INFO:Creating metrics dataframe
2024-09-11 10:47:50,076:INFO:Initializing Linear Discriminant Analysis
2024-09-11 10:47:50,076:INFO:Total runtime is 0.4979002833366394 minutes
2024-09-11 10:47:50,077:INFO:SubProcess create_model() called ==================================
2024-09-11 10:47:50,077:INFO:Initializing create_model()
2024-09-11 10:47:50,077:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000020282FB7310>, estimator=lda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000202831FA090>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-11 10:47:50,077:INFO:Checking exceptions
2024-09-11 10:47:50,077:INFO:Importing libraries
2024-09-11 10:47:50,077:INFO:Copying training dataset
2024-09-11 10:47:50,083:INFO:Defining folds
2024-09-11 10:47:50,083:INFO:Declaring metric variables
2024-09-11 10:47:50,083:INFO:Importing untrained model
2024-09-11 10:47:50,084:INFO:Linear Discriminant Analysis Imported successfully
2024-09-11 10:47:50,084:INFO:Starting cross validation
2024-09-11 10:47:50,086:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-11 10:47:50,379:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:47:50,380:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:47:50,382:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:47:50,384:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:47:50,386:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:50,386:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:47:50,390:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:50,392:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:50,396:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:50,396:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:47:50,399:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:50,401:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:50,401:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:47:50,405:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:50,407:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:50,407:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:47:50,410:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:50,411:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:50,412:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:50,414:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:50,417:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:50,418:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:50,418:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:47:50,423:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:50,424:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:50,425:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:47:50,428:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:50,428:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:50,429:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:50,430:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:50,434:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:50,439:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:50,444:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:50,444:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:50,449:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:50,452:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 10:47:50,453:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:50,454:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:50,470:INFO:Calculating mean and std
2024-09-11 10:47:50,471:INFO:Creating metrics dataframe
2024-09-11 10:47:50,474:INFO:Uploading results into container
2024-09-11 10:47:50,474:INFO:Uploading model into container now
2024-09-11 10:47:50,475:INFO:_master_model_container: 11
2024-09-11 10:47:50,475:INFO:_display_container: 2
2024-09-11 10:47:50,475:INFO:LinearDiscriminantAnalysis(covariance_estimator=None, n_components=None,
                           priors=None, shrinkage=None, solver='svd',
                           store_covariance=False, tol=0.0001)
2024-09-11 10:47:50,476:INFO:create_model() successfully completed......................................
2024-09-11 10:47:50,534:INFO:SubProcess create_model() end ==================================
2024-09-11 10:47:50,534:INFO:Creating metrics dataframe
2024-09-11 10:47:50,538:INFO:Initializing Extra Trees Classifier
2024-09-11 10:47:50,538:INFO:Total runtime is 0.5056029796600342 minutes
2024-09-11 10:47:50,538:INFO:SubProcess create_model() called ==================================
2024-09-11 10:47:50,538:INFO:Initializing create_model()
2024-09-11 10:47:50,538:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000020282FB7310>, estimator=et, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000202831FA090>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-11 10:47:50,538:INFO:Checking exceptions
2024-09-11 10:47:50,539:INFO:Importing libraries
2024-09-11 10:47:50,539:INFO:Copying training dataset
2024-09-11 10:47:50,544:INFO:Defining folds
2024-09-11 10:47:50,545:INFO:Declaring metric variables
2024-09-11 10:47:50,545:INFO:Importing untrained model
2024-09-11 10:47:50,546:INFO:Extra Trees Classifier Imported successfully
2024-09-11 10:47:50,546:INFO:Starting cross validation
2024-09-11 10:47:50,549:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-11 10:47:52,075:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:52,092:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:52,097:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:52,100:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:52,108:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:52,112:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:52,117:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:52,117:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:52,120:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:52,122:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:52,126:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:52,128:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:52,134:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:52,141:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:52,145:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:52,146:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:52,150:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:52,152:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:52,155:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:52,163:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:52,167:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:52,167:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:52,174:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:52,184:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:52,185:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:52,194:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:52,199:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:52,217:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:52,222:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:52,225:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 10:47:52,228:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:52,243:INFO:Calculating mean and std
2024-09-11 10:47:52,244:INFO:Creating metrics dataframe
2024-09-11 10:47:52,249:INFO:Uploading results into container
2024-09-11 10:47:52,250:INFO:Uploading model into container now
2024-09-11 10:47:52,250:INFO:_master_model_container: 12
2024-09-11 10:47:52,250:INFO:_display_container: 2
2024-09-11 10:47:52,251:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=2857, verbose=0,
                     warm_start=False)
2024-09-11 10:47:52,251:INFO:create_model() successfully completed......................................
2024-09-11 10:47:52,311:INFO:SubProcess create_model() end ==================================
2024-09-11 10:47:52,312:INFO:Creating metrics dataframe
2024-09-11 10:47:52,315:INFO:Initializing Light Gradient Boosting Machine
2024-09-11 10:47:52,315:INFO:Total runtime is 0.5352230827013652 minutes
2024-09-11 10:47:52,315:INFO:SubProcess create_model() called ==================================
2024-09-11 10:47:52,316:INFO:Initializing create_model()
2024-09-11 10:47:52,316:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000020282FB7310>, estimator=lightgbm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000202831FA090>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-11 10:47:52,316:INFO:Checking exceptions
2024-09-11 10:47:52,316:INFO:Importing libraries
2024-09-11 10:47:52,316:INFO:Copying training dataset
2024-09-11 10:47:52,321:INFO:Defining folds
2024-09-11 10:47:52,322:INFO:Declaring metric variables
2024-09-11 10:47:52,322:INFO:Importing untrained model
2024-09-11 10:47:52,322:INFO:Light Gradient Boosting Machine Imported successfully
2024-09-11 10:47:52,324:INFO:Starting cross validation
2024-09-11 10:47:52,326:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-11 10:47:56,141:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:56,145:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:56,155:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:56,156:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:56,159:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 10:47:56,169:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:56,174:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:56,244:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:56,256:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:56,262:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:56,278:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:56,282:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:56,300:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:56,441:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:56,482:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:56,501:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:56,638:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:56,669:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:56,682:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:56,870:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:56,881:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:56,893:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:56,894:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:56,896:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:56,904:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:56,907:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:56,915:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:56,918:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:56,919:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:56,928:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:56,939:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:56,965:INFO:Calculating mean and std
2024-09-11 10:47:56,968:INFO:Creating metrics dataframe
2024-09-11 10:47:56,973:INFO:Uploading results into container
2024-09-11 10:47:56,974:INFO:Uploading model into container now
2024-09-11 10:47:56,975:INFO:_master_model_container: 13
2024-09-11 10:47:56,976:INFO:_display_container: 2
2024-09-11 10:47:56,978:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=2857, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-09-11 10:47:56,979:INFO:create_model() successfully completed......................................
2024-09-11 10:47:57,054:INFO:SubProcess create_model() end ==================================
2024-09-11 10:47:57,054:INFO:Creating metrics dataframe
2024-09-11 10:47:57,059:INFO:Initializing Dummy Classifier
2024-09-11 10:47:57,059:INFO:Total runtime is 0.6142844875653586 minutes
2024-09-11 10:47:57,059:INFO:SubProcess create_model() called ==================================
2024-09-11 10:47:57,059:INFO:Initializing create_model()
2024-09-11 10:47:57,059:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000020282FB7310>, estimator=dummy, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000202831FA090>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-11 10:47:57,059:INFO:Checking exceptions
2024-09-11 10:47:57,060:INFO:Importing libraries
2024-09-11 10:47:57,060:INFO:Copying training dataset
2024-09-11 10:47:57,065:INFO:Defining folds
2024-09-11 10:47:57,065:INFO:Declaring metric variables
2024-09-11 10:47:57,066:INFO:Importing untrained model
2024-09-11 10:47:57,066:INFO:Dummy Classifier Imported successfully
2024-09-11 10:47:57,066:INFO:Starting cross validation
2024-09-11 10:47:57,070:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-11 10:47:57,349:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:57,351:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:57,351:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:57,357:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:57,365:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:57,367:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:57,370:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:57,372:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:57,372:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 10:47:57,373:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:57,375:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 10:47:57,375:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 10:47:57,379:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:57,380:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 10:47:57,382:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:57,384:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:57,384:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:57,384:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:57,386:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:57,386:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:57,388:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:57,391:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 10:47:57,392:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:57,392:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:57,395:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:57,396:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 10:47:57,401:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:57,402:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 10:47:57,404:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:57,404:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:57,406:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:57,408:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 10:47:57,410:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:57,410:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 10:47:57,410:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:57,412:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 10:47:57,414:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:57,417:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:57,419:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:47:57,436:INFO:Calculating mean and std
2024-09-11 10:47:57,437:INFO:Creating metrics dataframe
2024-09-11 10:47:57,440:INFO:Uploading results into container
2024-09-11 10:47:57,440:INFO:Uploading model into container now
2024-09-11 10:47:57,441:INFO:_master_model_container: 14
2024-09-11 10:47:57,441:INFO:_display_container: 2
2024-09-11 10:47:57,441:INFO:DummyClassifier(constant=None, random_state=2857, strategy='prior')
2024-09-11 10:47:57,441:INFO:create_model() successfully completed......................................
2024-09-11 10:47:57,502:INFO:SubProcess create_model() end ==================================
2024-09-11 10:47:57,502:INFO:Creating metrics dataframe
2024-09-11 10:47:57,508:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:339: FutureWarning: Styler.applymap has been deprecated. Use Styler.map instead.
  .applymap(highlight_cols, subset=["TT (Sec)"])

2024-09-11 10:47:57,511:INFO:Initializing create_model()
2024-09-11 10:47:57,511:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000020282FB7310>, estimator=ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=2857, verbose=0,
                     warm_start=False), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-11 10:47:57,511:INFO:Checking exceptions
2024-09-11 10:47:57,512:INFO:Importing libraries
2024-09-11 10:47:57,512:INFO:Copying training dataset
2024-09-11 10:47:57,518:INFO:Defining folds
2024-09-11 10:47:57,519:INFO:Declaring metric variables
2024-09-11 10:47:57,519:INFO:Importing untrained model
2024-09-11 10:47:57,519:INFO:Declaring custom model
2024-09-11 10:47:57,521:INFO:Extra Trees Classifier Imported successfully
2024-09-11 10:47:57,523:INFO:Cross validation set to False
2024-09-11 10:47:57,523:INFO:Fitting Model
2024-09-11 10:47:57,769:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=2857, verbose=0,
                     warm_start=False)
2024-09-11 10:47:57,770:INFO:create_model() successfully completed......................................
2024-09-11 10:47:57,858:INFO:_master_model_container: 14
2024-09-11 10:47:57,858:INFO:_display_container: 2
2024-09-11 10:47:57,859:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=2857, verbose=0,
                     warm_start=False)
2024-09-11 10:47:57,859:INFO:compare_models() successfully completed......................................
2024-09-11 10:47:57,860:INFO:Initializing plot_model()
2024-09-11 10:47:57,860:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000020282FB7310>, estimator=ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=2857, verbose=0,
                     warm_start=False), plot=confusion_matrix, scale=1, save=False, fold=None, fit_kwargs=None, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=True, system=True, display=None, display_format=None)
2024-09-11 10:47:57,860:INFO:Checking exceptions
2024-09-11 10:47:57,924:INFO:Preloading libraries
2024-09-11 10:47:57,945:INFO:Copying training dataset
2024-09-11 10:47:57,945:INFO:Plot type: confusion_matrix
2024-09-11 10:47:58,520:INFO:Fitting Model
2024-09-11 10:47:58,520:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\base.py:493: UserWarning: X does not have valid feature names, but ExtraTreesClassifier was fitted with feature names
  warnings.warn(

2024-09-11 10:47:58,521:INFO:Scoring test/hold-out set
2024-09-11 10:51:25,096:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-11 10:51:25,096:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-11 10:51:25,096:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-11 10:51:25,096:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-11 10:51:26,876:INFO:PyCaret ClassificationExperiment
2024-09-11 10:51:26,876:INFO:Logging name: clf-default-name
2024-09-11 10:51:26,876:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2024-09-11 10:51:26,876:INFO:version 3.3.2
2024-09-11 10:51:26,876:INFO:Initializing setup()
2024-09-11 10:51:26,877:INFO:self.USI: c1a5
2024-09-11 10:51:26,877:INFO:self._variable_keys: {'logging_param', 'n_jobs_param', '_available_plots', 'target_param', 'fold_shuffle_param', 'memory', 'pipeline', 'fold_generator', 'fold_groups_param', 'fix_imbalance', 'USI', 'data', 'y_test', 'seed', 'idx', 'y', 'html_param', 'y_train', 'exp_name_log', 'X_test', 'exp_id', 'X_train', 'is_multiclass', '_ml_usecase', 'X', 'gpu_param', 'gpu_n_jobs_param', 'log_plots_param'}
2024-09-11 10:51:26,877:INFO:Checking environment
2024-09-11 10:51:26,877:INFO:python_version: 3.11.8
2024-09-11 10:51:26,877:INFO:python_build: ('tags/v3.11.8:db85d51', 'Feb  6 2024 22:03:32')
2024-09-11 10:51:26,877:INFO:machine: AMD64
2024-09-11 10:51:26,877:INFO:platform: Windows-10-10.0.22631-SP0
2024-09-11 10:51:26,880:INFO:Memory: svmem(total=17096892416, available=6730080256, percent=60.6, used=10366812160, free=6730080256)
2024-09-11 10:51:26,881:INFO:Physical Core: 6
2024-09-11 10:51:26,881:INFO:Logical Core: 12
2024-09-11 10:51:26,881:INFO:Checking libraries
2024-09-11 10:51:26,881:INFO:System:
2024-09-11 10:51:26,881:INFO:    python: 3.11.8 (tags/v3.11.8:db85d51, Feb  6 2024, 22:03:32) [MSC v.1937 64 bit (AMD64)]
2024-09-11 10:51:26,881:INFO:executable: h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Scripts\python.exe
2024-09-11 10:51:26,881:INFO:   machine: Windows-10-10.0.22631-SP0
2024-09-11 10:51:26,881:INFO:PyCaret required dependencies:
2024-09-11 10:51:26,927:INFO:                 pip: 24.2
2024-09-11 10:51:26,927:INFO:          setuptools: 65.5.0
2024-09-11 10:51:26,927:INFO:             pycaret: 3.3.2
2024-09-11 10:51:26,927:INFO:             IPython: 8.27.0
2024-09-11 10:51:26,927:INFO:          ipywidgets: 8.1.5
2024-09-11 10:51:26,927:INFO:                tqdm: 4.66.5
2024-09-11 10:51:26,927:INFO:               numpy: 1.26.4
2024-09-11 10:51:26,927:INFO:              pandas: 2.1.4
2024-09-11 10:51:26,927:INFO:              jinja2: 3.1.4
2024-09-11 10:51:26,928:INFO:               scipy: 1.11.4
2024-09-11 10:51:26,928:INFO:              joblib: 1.3.2
2024-09-11 10:51:26,928:INFO:             sklearn: 1.4.2
2024-09-11 10:51:26,928:INFO:                pyod: 2.0.2
2024-09-11 10:51:26,928:INFO:            imblearn: 0.12.3
2024-09-11 10:51:26,928:INFO:   category_encoders: 2.6.3
2024-09-11 10:51:26,928:INFO:            lightgbm: 4.5.0
2024-09-11 10:51:26,928:INFO:               numba: 0.60.0
2024-09-11 10:51:26,928:INFO:            requests: 2.32.3
2024-09-11 10:51:26,928:INFO:          matplotlib: 3.7.5
2024-09-11 10:51:26,928:INFO:          scikitplot: 0.3.7
2024-09-11 10:51:26,929:INFO:         yellowbrick: 1.5
2024-09-11 10:51:26,929:INFO:              plotly: 5.24.0
2024-09-11 10:51:26,929:INFO:    plotly-resampler: Not installed
2024-09-11 10:51:26,929:INFO:             kaleido: 0.2.1
2024-09-11 10:51:26,929:INFO:           schemdraw: 0.15
2024-09-11 10:51:26,929:INFO:         statsmodels: 0.14.2
2024-09-11 10:51:26,929:INFO:              sktime: 0.26.0
2024-09-11 10:51:26,929:INFO:               tbats: 1.1.3
2024-09-11 10:51:26,929:INFO:            pmdarima: 2.0.4
2024-09-11 10:51:26,930:INFO:              psutil: 6.0.0
2024-09-11 10:51:26,930:INFO:          markupsafe: 2.1.5
2024-09-11 10:51:26,930:INFO:             pickle5: Not installed
2024-09-11 10:51:26,930:INFO:         cloudpickle: 3.0.0
2024-09-11 10:51:26,930:INFO:         deprecation: 2.1.0
2024-09-11 10:51:26,930:INFO:              xxhash: 3.5.0
2024-09-11 10:51:26,930:INFO:           wurlitzer: Not installed
2024-09-11 10:51:26,930:INFO:PyCaret optional dependencies:
2024-09-11 10:51:26,961:INFO:                shap: Not installed
2024-09-11 10:51:26,961:INFO:           interpret: Not installed
2024-09-11 10:51:26,961:INFO:                umap: Not installed
2024-09-11 10:51:26,961:INFO:     ydata_profiling: Not installed
2024-09-11 10:51:26,961:INFO:  explainerdashboard: Not installed
2024-09-11 10:51:26,962:INFO:             autoviz: Not installed
2024-09-11 10:51:26,962:INFO:           fairlearn: Not installed
2024-09-11 10:51:26,962:INFO:          deepchecks: Not installed
2024-09-11 10:51:26,962:INFO:             xgboost: Not installed
2024-09-11 10:51:26,962:INFO:            catboost: Not installed
2024-09-11 10:51:26,962:INFO:              kmodes: Not installed
2024-09-11 10:51:26,962:INFO:             mlxtend: Not installed
2024-09-11 10:51:26,962:INFO:       statsforecast: Not installed
2024-09-11 10:51:26,962:INFO:        tune_sklearn: Not installed
2024-09-11 10:51:26,962:INFO:                 ray: Not installed
2024-09-11 10:51:26,962:INFO:            hyperopt: Not installed
2024-09-11 10:51:26,962:INFO:              optuna: Not installed
2024-09-11 10:51:26,963:INFO:               skopt: Not installed
2024-09-11 10:51:26,963:INFO:              mlflow: Not installed
2024-09-11 10:51:26,963:INFO:              gradio: Not installed
2024-09-11 10:51:26,963:INFO:             fastapi: Not installed
2024-09-11 10:51:26,963:INFO:             uvicorn: Not installed
2024-09-11 10:51:26,963:INFO:              m2cgen: Not installed
2024-09-11 10:51:26,963:INFO:           evidently: Not installed
2024-09-11 10:51:26,963:INFO:               fugue: Not installed
2024-09-11 10:51:26,963:INFO:           streamlit: Not installed
2024-09-11 10:51:26,963:INFO:             prophet: Not installed
2024-09-11 10:51:26,963:INFO:None
2024-09-11 10:51:26,964:INFO:Set up data.
2024-09-11 10:51:26,976:INFO:Set up folding strategy.
2024-09-11 10:51:26,977:INFO:Set up train/test split.
2024-09-11 10:51:26,988:INFO:Set up index.
2024-09-11 10:51:26,988:INFO:Assigning column types.
2024-09-11 10:51:26,994:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2024-09-11 10:51:27,036:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-09-11 10:51:27,042:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-09-11 10:51:27,080:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-11 10:51:27,081:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-11 10:51:27,121:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-09-11 10:51:27,123:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-09-11 10:51:27,149:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-11 10:51:27,150:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-11 10:51:27,150:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2024-09-11 10:51:27,192:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-09-11 10:51:27,221:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-11 10:51:27,221:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-11 10:51:27,265:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-09-11 10:51:27,293:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-11 10:51:27,293:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-11 10:51:27,293:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2024-09-11 10:51:27,367:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-11 10:51:27,367:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-11 10:51:27,439:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-11 10:51:27,440:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-11 10:51:27,444:INFO:Preparing preprocessing pipeline...
2024-09-11 10:51:27,446:INFO:Set up label encoding.
2024-09-11 10:51:27,446:INFO:Set up simple imputation.
2024-09-11 10:51:27,451:INFO:Set up encoding of categorical features.
2024-09-11 10:51:27,592:INFO:Finished creating preprocessing pipeline.
2024-09-11 10:51:27,610:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\ipkov\AppData\Local\Temp\joblib),
         steps=[('label_encoding',
                 TransformerWrapperWithInverse(exclude=None, include=None,
                                               transformer=LabelEncoder())),
                ('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['MPG', 'Cylinders', 'Displacement',
                                             'Horsepower', 'Weight',
                                             'Acceleration', 'Model'],
                                    transformer=SimpleImp...
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='most_frequent'))),
                ('rest_encoding',
                 TransformerWrapper(exclude=None, include=['Car'],
                                    transformer=TargetEncoder(cols=['Car'],
                                                              drop_invariant=False,
                                                              handle_missing='return_nan',
                                                              handle_unknown='value',
                                                              hierarchy=None,
                                                              min_samples_leaf=20,
                                                              return_df=True,
                                                              smoothing=10,
                                                              verbose=0)))],
         verbose=False)
2024-09-11 10:51:27,610:INFO:Creating final display dataframe.
2024-09-11 10:51:28,041:INFO:Setup _display_container:                     Description                       Value
0                    Session id                         369
1                        Target                      Origin
2                   Target type                  Multiclass
3                Target mapping  Europe: 0, Japan: 1, US: 2
4           Original data shape                    (406, 9)
5        Transformed data shape                    (406, 9)
6   Transformed train set shape                    (284, 9)
7    Transformed test set shape                    (122, 9)
8              Numeric features                           7
9          Categorical features                           1
10                   Preprocess                        True
11              Imputation type                      simple
12           Numeric imputation                        mean
13       Categorical imputation                        mode
14     Maximum one-hot encoding                          25
15              Encoding method                        None
16               Fold Generator             StratifiedKFold
17                  Fold Number                          10
18                     CPU Jobs                          -1
19                      Use GPU                       False
20               Log Experiment                       False
21              Experiment Name            clf-default-name
22                          USI                        c1a5
2024-09-11 10:51:28,055:ERROR:Data Failed with exception:
 No module named 'ydata_profiling'
No output to show, continue with modeling.
2024-09-11 10:51:28,124:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-11 10:51:28,125:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-11 10:51:28,195:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-11 10:51:28,196:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-11 10:51:28,198:INFO:setup() successfully completed in 1.32s...............
2024-09-11 10:51:28,199:INFO:Initializing compare_models()
2024-09-11 10:51:28,199:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000026AE32C35D0>, include=None, exclude=None, fold=None, round=4, cross_validation=True, sort=Accuracy, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x0000026AE32C35D0>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'Accuracy', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>})
2024-09-11 10:51:28,199:INFO:Checking exceptions
2024-09-11 10:51:28,203:INFO:Preparing display monitor
2024-09-11 10:51:28,211:INFO:Initializing Logistic Regression
2024-09-11 10:51:28,211:INFO:Total runtime is 0.0 minutes
2024-09-11 10:51:28,211:INFO:SubProcess create_model() called ==================================
2024-09-11 10:51:28,212:INFO:Initializing create_model()
2024-09-11 10:51:28,212:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000026AE32C35D0>, estimator=lr, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000026AE37C7490>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-11 10:51:28,213:INFO:Checking exceptions
2024-09-11 10:51:28,213:INFO:Importing libraries
2024-09-11 10:51:28,213:INFO:Copying training dataset
2024-09-11 10:51:28,218:INFO:Defining folds
2024-09-11 10:51:28,219:INFO:Declaring metric variables
2024-09-11 10:51:28,219:INFO:Importing untrained model
2024-09-11 10:51:28,220:INFO:Logistic Regression Imported successfully
2024-09-11 10:51:28,220:INFO:Starting cross validation
2024-09-11 10:51:28,223:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-11 10:51:47,715:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-11 10:51:47,853:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:51:47,866:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:51:47,871:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-11 10:51:47,894:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:51:47,908:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:51:48,012:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-11 10:51:48,016:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:51:48,030:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:51:48,048:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:51:48,066:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:51:48,149:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:51:48,167:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:51:48,185:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-11 10:51:48,197:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:51:48,220:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:51:48,356:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:51:48,386:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:51:48,391:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-11 10:51:48,405:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:51:48,421:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:51:48,536:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:51:48,548:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:51:48,564:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:51:48,579:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:51:48,594:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-11 10:51:48,594:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-11 10:51:48,611:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-11 10:51:48,706:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:51:48,716:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:51:48,719:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:51:48,729:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:51:48,732:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:51:48,740:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:51:48,749:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:51:48,752:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:51:48,766:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:51:48,766:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:51:48,769:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:51:48,783:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:51:48,875:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-11 10:51:48,894:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-11 10:51:48,937:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:51:48,940:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:51:48,945:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:51:48,949:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:51:48,951:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:51:48,952:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:51:48,957:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:51:48,962:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:51:48,973:INFO:Calculating mean and std
2024-09-11 10:51:48,974:INFO:Creating metrics dataframe
2024-09-11 10:51:48,978:INFO:Uploading results into container
2024-09-11 10:51:48,978:INFO:Uploading model into container now
2024-09-11 10:51:48,979:INFO:_master_model_container: 1
2024-09-11 10:51:48,979:INFO:_display_container: 2
2024-09-11 10:51:48,980:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=369, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2024-09-11 10:51:48,980:INFO:create_model() successfully completed......................................
2024-09-11 10:51:49,069:INFO:SubProcess create_model() end ==================================
2024-09-11 10:51:49,070:INFO:Creating metrics dataframe
2024-09-11 10:51:49,074:INFO:Initializing K Neighbors Classifier
2024-09-11 10:51:49,074:INFO:Total runtime is 0.34770906368891397 minutes
2024-09-11 10:51:49,074:INFO:SubProcess create_model() called ==================================
2024-09-11 10:51:49,075:INFO:Initializing create_model()
2024-09-11 10:51:49,075:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000026AE32C35D0>, estimator=knn, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000026AE37C7490>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-11 10:51:49,075:INFO:Checking exceptions
2024-09-11 10:51:49,075:INFO:Importing libraries
2024-09-11 10:51:49,075:INFO:Copying training dataset
2024-09-11 10:51:49,081:INFO:Defining folds
2024-09-11 10:51:49,081:INFO:Declaring metric variables
2024-09-11 10:51:49,082:INFO:Importing untrained model
2024-09-11 10:51:49,082:INFO:K Neighbors Classifier Imported successfully
2024-09-11 10:51:49,083:INFO:Starting cross validation
2024-09-11 10:51:49,085:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-11 10:51:50,228:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:51:50,232:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:51:50,248:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:51:50,256:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:51:50,263:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:51:50,266:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:51:50,266:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:51:50,272:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:51:50,284:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:51:50,289:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:51:50,297:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:51:50,305:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:51:50,307:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:51:50,316:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:51:50,404:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:51:50,425:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:51:50,432:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:51:50,444:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:51:50,454:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:51:50,454:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:51:50,472:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:51:50,479:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:51:50,491:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:51:55,369:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:51:55,383:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:51:55,391:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:51:55,393:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:51:55,398:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:51:55,406:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:51:55,418:INFO:Calculating mean and std
2024-09-11 10:51:55,420:INFO:Creating metrics dataframe
2024-09-11 10:51:55,429:INFO:Uploading results into container
2024-09-11 10:51:55,431:INFO:Uploading model into container now
2024-09-11 10:51:55,433:INFO:_master_model_container: 2
2024-09-11 10:51:55,433:INFO:_display_container: 2
2024-09-11 10:51:55,434:INFO:KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform')
2024-09-11 10:51:55,435:INFO:create_model() successfully completed......................................
2024-09-11 10:51:55,646:INFO:SubProcess create_model() end ==================================
2024-09-11 10:51:55,647:INFO:Creating metrics dataframe
2024-09-11 10:51:55,651:INFO:Initializing Naive Bayes
2024-09-11 10:51:55,652:INFO:Total runtime is 0.4573485732078552 minutes
2024-09-11 10:51:55,652:INFO:SubProcess create_model() called ==================================
2024-09-11 10:51:55,652:INFO:Initializing create_model()
2024-09-11 10:51:55,652:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000026AE32C35D0>, estimator=nb, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000026AE37C7490>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-11 10:51:55,652:INFO:Checking exceptions
2024-09-11 10:51:55,653:INFO:Importing libraries
2024-09-11 10:51:55,653:INFO:Copying training dataset
2024-09-11 10:51:55,661:INFO:Defining folds
2024-09-11 10:51:55,661:INFO:Declaring metric variables
2024-09-11 10:51:55,662:INFO:Importing untrained model
2024-09-11 10:51:55,662:INFO:Naive Bayes Imported successfully
2024-09-11 10:51:55,662:INFO:Starting cross validation
2024-09-11 10:51:55,665:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-11 10:51:56,051:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:51:56,057:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:51:56,068:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:51:56,076:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:51:56,078:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:51:56,079:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:51:56,086:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 10:51:56,096:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:51:56,096:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:51:56,097:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:51:56,104:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:51:56,104:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:51:56,115:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:51:56,115:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:51:56,117:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:51:56,123:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:51:56,125:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:51:56,129:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:51:56,130:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:51:56,130:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:51:56,135:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:51:56,143:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:51:56,146:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:51:56,147:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:51:56,154:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:51:56,156:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:51:56,157:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:51:56,163:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:51:56,164:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:51:56,171:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:51:56,178:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:51:56,196:INFO:Calculating mean and std
2024-09-11 10:51:56,197:INFO:Creating metrics dataframe
2024-09-11 10:51:56,202:INFO:Uploading results into container
2024-09-11 10:51:56,203:INFO:Uploading model into container now
2024-09-11 10:51:56,204:INFO:_master_model_container: 3
2024-09-11 10:51:56,204:INFO:_display_container: 2
2024-09-11 10:51:56,204:INFO:GaussianNB(priors=None, var_smoothing=1e-09)
2024-09-11 10:51:56,204:INFO:create_model() successfully completed......................................
2024-09-11 10:51:56,274:INFO:SubProcess create_model() end ==================================
2024-09-11 10:51:56,274:INFO:Creating metrics dataframe
2024-09-11 10:51:56,278:INFO:Initializing Decision Tree Classifier
2024-09-11 10:51:56,278:INFO:Total runtime is 0.46778393189112344 minutes
2024-09-11 10:51:56,278:INFO:SubProcess create_model() called ==================================
2024-09-11 10:51:56,279:INFO:Initializing create_model()
2024-09-11 10:51:56,279:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000026AE32C35D0>, estimator=dt, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000026AE37C7490>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-11 10:51:56,279:INFO:Checking exceptions
2024-09-11 10:51:56,279:INFO:Importing libraries
2024-09-11 10:51:56,279:INFO:Copying training dataset
2024-09-11 10:51:56,285:INFO:Defining folds
2024-09-11 10:51:56,286:INFO:Declaring metric variables
2024-09-11 10:51:56,286:INFO:Importing untrained model
2024-09-11 10:51:56,286:INFO:Decision Tree Classifier Imported successfully
2024-09-11 10:51:56,287:INFO:Starting cross validation
2024-09-11 10:51:56,289:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-11 10:51:56,687:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:51:56,702:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:51:56,711:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:51:56,725:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:51:56,728:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:51:56,729:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:51:56,731:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:51:56,731:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:51:56,740:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 10:51:56,745:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:51:56,747:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:51:56,749:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:51:56,752:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:51:56,753:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:51:56,755:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:51:56,758:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:51:56,764:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:51:56,766:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:51:56,768:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:51:56,769:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:51:56,776:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:51:56,777:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:51:56,777:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:51:56,779:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:51:56,794:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:51:56,794:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:51:56,796:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:51:56,797:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:51:56,798:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:51:56,805:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:51:56,837:INFO:Calculating mean and std
2024-09-11 10:51:56,838:INFO:Creating metrics dataframe
2024-09-11 10:51:56,844:INFO:Uploading results into container
2024-09-11 10:51:56,846:INFO:Uploading model into container now
2024-09-11 10:51:56,848:INFO:_master_model_container: 4
2024-09-11 10:51:56,849:INFO:_display_container: 2
2024-09-11 10:51:56,851:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=369, splitter='best')
2024-09-11 10:51:56,852:INFO:create_model() successfully completed......................................
2024-09-11 10:51:56,958:INFO:SubProcess create_model() end ==================================
2024-09-11 10:51:56,958:INFO:Creating metrics dataframe
2024-09-11 10:51:56,964:INFO:Initializing SVM - Linear Kernel
2024-09-11 10:51:56,964:INFO:Total runtime is 0.47921324968338014 minutes
2024-09-11 10:51:56,965:INFO:SubProcess create_model() called ==================================
2024-09-11 10:51:56,966:INFO:Initializing create_model()
2024-09-11 10:51:56,966:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000026AE32C35D0>, estimator=svm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000026AE37C7490>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-11 10:51:56,966:INFO:Checking exceptions
2024-09-11 10:51:56,966:INFO:Importing libraries
2024-09-11 10:51:56,966:INFO:Copying training dataset
2024-09-11 10:51:56,975:INFO:Defining folds
2024-09-11 10:51:56,975:INFO:Declaring metric variables
2024-09-11 10:51:56,978:INFO:Importing untrained model
2024-09-11 10:51:56,980:INFO:SVM - Linear Kernel Imported successfully
2024-09-11 10:51:56,982:INFO:Starting cross validation
2024-09-11 10:51:56,985:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-11 10:51:57,460:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:51:57,467:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:51:57,473:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:51:57,476:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:51:57,493:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:51:57,494:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:51:57,501:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 10:51:57,511:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 10:51:57,513:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:51:57,515:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:51:57,522:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:51:57,523:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:51:57,532:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:51:57,534:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:51:57,545:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:51:57,545:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:51:57,549:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:51:57,558:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:51:57,563:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:51:57,564:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:51:57,567:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:51:57,567:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 10:51:57,568:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:51:57,575:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 10:51:57,575:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 10:51:57,575:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:51:57,577:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:51:57,579:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:51:57,585:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:51:57,587:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:51:57,591:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:51:57,592:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:51:57,595:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:51:57,601:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 10:51:57,608:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 10:51:57,610:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:51:57,610:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:51:57,617:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:51:57,623:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:51:57,625:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 10:51:57,628:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:51:57,632:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:51:57,645:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:51:57,652:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 10:51:57,658:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:51:57,660:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:51:57,664:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:51:57,668:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:51:57,672:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 10:51:57,674:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:51:57,686:INFO:Calculating mean and std
2024-09-11 10:51:57,688:INFO:Creating metrics dataframe
2024-09-11 10:51:57,692:INFO:Uploading results into container
2024-09-11 10:51:57,692:INFO:Uploading model into container now
2024-09-11 10:51:57,693:INFO:_master_model_container: 5
2024-09-11 10:51:57,693:INFO:_display_container: 2
2024-09-11 10:51:57,693:INFO:SGDClassifier(alpha=0.0001, average=False, class_weight=None,
              early_stopping=False, epsilon=0.1, eta0=0.001, fit_intercept=True,
              l1_ratio=0.15, learning_rate='optimal', loss='hinge',
              max_iter=1000, n_iter_no_change=5, n_jobs=-1, penalty='l2',
              power_t=0.5, random_state=369, shuffle=True, tol=0.001,
              validation_fraction=0.1, verbose=0, warm_start=False)
2024-09-11 10:51:57,693:INFO:create_model() successfully completed......................................
2024-09-11 10:51:57,759:INFO:SubProcess create_model() end ==================================
2024-09-11 10:51:57,759:INFO:Creating metrics dataframe
2024-09-11 10:51:57,763:INFO:Initializing Ridge Classifier
2024-09-11 10:51:57,763:INFO:Total runtime is 0.4925294518470764 minutes
2024-09-11 10:51:57,763:INFO:SubProcess create_model() called ==================================
2024-09-11 10:51:57,763:INFO:Initializing create_model()
2024-09-11 10:51:57,764:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000026AE32C35D0>, estimator=ridge, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000026AE37C7490>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-11 10:51:57,764:INFO:Checking exceptions
2024-09-11 10:51:57,764:INFO:Importing libraries
2024-09-11 10:51:57,764:INFO:Copying training dataset
2024-09-11 10:51:57,769:INFO:Defining folds
2024-09-11 10:51:57,769:INFO:Declaring metric variables
2024-09-11 10:51:57,769:INFO:Importing untrained model
2024-09-11 10:51:57,770:INFO:Ridge Classifier Imported successfully
2024-09-11 10:51:57,770:INFO:Starting cross validation
2024-09-11 10:51:57,773:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-11 10:51:58,086:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:51:58,087:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:51:58,097:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead2024-09-11 10:51:58,097:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

 File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:51:58,105:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:51:58,110:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:51:58,117:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:51:58,117:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:51:58,122:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:51:58,124:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:51:58,130:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:51:58,133:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:51:58,137:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:51:58,138:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:51:58,141:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:51:58,144:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:51:58,144:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:51:58,147:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:51:58,147:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:51:58,151:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:51:58,157:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:51:58,158:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:51:58,163:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:51:58,165:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:51:58,169:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:51:58,172:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:51:58,173:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:51:58,173:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:51:58,174:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:51:58,179:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:51:58,181:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:51:58,186:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:51:58,187:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:51:58,187:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:51:58,187:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:51:58,187:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:51:58,195:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:51:58,196:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:51:58,201:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:51:58,210:INFO:Calculating mean and std
2024-09-11 10:51:58,210:INFO:Creating metrics dataframe
2024-09-11 10:51:58,215:INFO:Uploading results into container
2024-09-11 10:51:58,216:INFO:Uploading model into container now
2024-09-11 10:51:58,217:INFO:_master_model_container: 6
2024-09-11 10:51:58,217:INFO:_display_container: 2
2024-09-11 10:51:58,218:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=369, solver='auto',
                tol=0.0001)
2024-09-11 10:51:58,218:INFO:create_model() successfully completed......................................
2024-09-11 10:51:58,289:INFO:SubProcess create_model() end ==================================
2024-09-11 10:51:58,289:INFO:Creating metrics dataframe
2024-09-11 10:51:58,295:INFO:Initializing Random Forest Classifier
2024-09-11 10:51:58,295:INFO:Total runtime is 0.5013931433359782 minutes
2024-09-11 10:51:58,296:INFO:SubProcess create_model() called ==================================
2024-09-11 10:51:58,296:INFO:Initializing create_model()
2024-09-11 10:51:58,296:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000026AE32C35D0>, estimator=rf, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000026AE37C7490>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-11 10:51:58,296:INFO:Checking exceptions
2024-09-11 10:51:58,296:INFO:Importing libraries
2024-09-11 10:51:58,296:INFO:Copying training dataset
2024-09-11 10:51:58,302:INFO:Defining folds
2024-09-11 10:51:58,302:INFO:Declaring metric variables
2024-09-11 10:51:58,303:INFO:Importing untrained model
2024-09-11 10:51:58,303:INFO:Random Forest Classifier Imported successfully
2024-09-11 10:51:58,304:INFO:Starting cross validation
2024-09-11 10:51:58,306:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-11 10:52:00,537:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:52:00,560:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:52:00,562:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:52:00,565:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:52:00,574:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:52:00,578:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:52:00,584:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:52:00,585:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:52:00,594:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:52:00,597:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:52:00,600:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:52:00,601:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:52:00,608:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 10:52:00,611:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:52:00,617:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:52:00,618:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:52:00,619:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:52:00,624:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:52:00,643:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:52:00,649:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:52:00,664:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:52:00,668:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:52:00,690:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:52:00,693:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:52:00,696:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:52:00,712:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:52:00,720:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:52:00,738:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:52:00,742:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:52:00,758:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:52:00,775:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:52:00,814:INFO:Calculating mean and std
2024-09-11 10:52:00,817:INFO:Creating metrics dataframe
2024-09-11 10:52:00,825:INFO:Uploading results into container
2024-09-11 10:52:00,826:INFO:Uploading model into container now
2024-09-11 10:52:00,828:INFO:_master_model_container: 7
2024-09-11 10:52:00,829:INFO:_display_container: 2
2024-09-11 10:52:00,831:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=369, verbose=0,
                       warm_start=False)
2024-09-11 10:52:00,831:INFO:create_model() successfully completed......................................
2024-09-11 10:52:00,924:INFO:SubProcess create_model() end ==================================
2024-09-11 10:52:00,924:INFO:Creating metrics dataframe
2024-09-11 10:52:00,929:INFO:Initializing Quadratic Discriminant Analysis
2024-09-11 10:52:00,929:INFO:Total runtime is 0.5452951073646545 minutes
2024-09-11 10:52:00,929:INFO:SubProcess create_model() called ==================================
2024-09-11 10:52:00,930:INFO:Initializing create_model()
2024-09-11 10:52:00,930:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000026AE32C35D0>, estimator=qda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000026AE37C7490>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-11 10:52:00,930:INFO:Checking exceptions
2024-09-11 10:52:00,930:INFO:Importing libraries
2024-09-11 10:52:00,930:INFO:Copying training dataset
2024-09-11 10:52:00,937:INFO:Defining folds
2024-09-11 10:52:00,937:INFO:Declaring metric variables
2024-09-11 10:52:00,938:INFO:Importing untrained model
2024-09-11 10:52:00,938:INFO:Quadratic Discriminant Analysis Imported successfully
2024-09-11 10:52:00,939:INFO:Starting cross validation
2024-09-11 10:52:00,941:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-11 10:52:01,241:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:52:01,251:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:52:01,251:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:52:01,257:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:52:01,261:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:52:01,261:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:52:01,265:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:52:01,268:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:52:01,268:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:52:01,268:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:52:01,274:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:52:01,277:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:52:01,284:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:52:01,284:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:52:01,285:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:52:01,286:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:52:01,286:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:52:01,286:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:52:01,291:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:52:01,293:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:52:01,294:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:52:01,295:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:52:01,297:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:52:01,298:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:52:01,301:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:52:01,303:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:52:01,307:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:52:01,307:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:52:01,308:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:52:01,308:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:52:01,312:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:52:01,312:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:52:01,321:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:52:01,321:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:52:01,321:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:52:01,321:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:52:01,322:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:52:01,330:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:52:01,330:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:52:01,343:INFO:Calculating mean and std
2024-09-11 10:52:01,344:INFO:Creating metrics dataframe
2024-09-11 10:52:01,347:INFO:Uploading results into container
2024-09-11 10:52:01,348:INFO:Uploading model into container now
2024-09-11 10:52:01,348:INFO:_master_model_container: 8
2024-09-11 10:52:01,349:INFO:_display_container: 2
2024-09-11 10:52:01,349:INFO:QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,
                              store_covariance=False, tol=0.0001)
2024-09-11 10:52:01,349:INFO:create_model() successfully completed......................................
2024-09-11 10:52:01,411:INFO:SubProcess create_model() end ==================================
2024-09-11 10:52:01,411:INFO:Creating metrics dataframe
2024-09-11 10:52:01,416:INFO:Initializing Ada Boost Classifier
2024-09-11 10:52:01,416:INFO:Total runtime is 0.5534072319666544 minutes
2024-09-11 10:52:01,416:INFO:SubProcess create_model() called ==================================
2024-09-11 10:52:01,417:INFO:Initializing create_model()
2024-09-11 10:52:01,417:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000026AE32C35D0>, estimator=ada, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000026AE37C7490>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-11 10:52:01,417:INFO:Checking exceptions
2024-09-11 10:52:01,417:INFO:Importing libraries
2024-09-11 10:52:01,417:INFO:Copying training dataset
2024-09-11 10:52:01,422:INFO:Defining folds
2024-09-11 10:52:01,422:INFO:Declaring metric variables
2024-09-11 10:52:01,422:INFO:Importing untrained model
2024-09-11 10:52:01,423:INFO:Ada Boost Classifier Imported successfully
2024-09-11 10:52:01,423:INFO:Starting cross validation
2024-09-11 10:52:01,425:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-11 10:52:01,593:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-11 10:52:01,598:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-11 10:52:01,603:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-11 10:52:01,607:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-11 10:52:01,623:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-11 10:52:01,624:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-11 10:52:01,627:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-11 10:52:01,630:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-11 10:52:01,641:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-11 10:52:01,648:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-11 10:52:02,299:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:52:02,311:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:52:02,319:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:52:02,319:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:52:02,319:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:52:02,329:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:52:02,329:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:52:02,329:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:52:02,330:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:52:02,331:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:52:02,339:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 10:52:02,345:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:52:02,345:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:52:02,347:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:52:02,347:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:52:02,349:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:52:02,350:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:52:02,351:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:52:02,356:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:52:02,358:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:52:02,364:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:52:02,364:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:52:02,366:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:52:02,366:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:52:02,371:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:52:02,375:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:52:02,375:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:52:02,375:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:52:02,376:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:52:02,386:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:52:02,386:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:52:02,391:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:52:02,394:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:52:02,395:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:52:02,396:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:52:02,398:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:52:02,407:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:52:02,408:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:52:02,408:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:52:02,408:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:52:02,414:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:52:02,430:INFO:Calculating mean and std
2024-09-11 10:52:02,431:INFO:Creating metrics dataframe
2024-09-11 10:52:02,434:INFO:Uploading results into container
2024-09-11 10:52:02,435:INFO:Uploading model into container now
2024-09-11 10:52:02,435:INFO:_master_model_container: 9
2024-09-11 10:52:02,435:INFO:_display_container: 2
2024-09-11 10:52:02,436:INFO:AdaBoostClassifier(algorithm='SAMME.R', estimator=None, learning_rate=1.0,
                   n_estimators=50, random_state=369)
2024-09-11 10:52:02,436:INFO:create_model() successfully completed......................................
2024-09-11 10:52:02,499:INFO:SubProcess create_model() end ==================================
2024-09-11 10:52:02,499:INFO:Creating metrics dataframe
2024-09-11 10:52:02,504:INFO:Initializing Gradient Boosting Classifier
2024-09-11 10:52:02,504:INFO:Total runtime is 0.5715412259101867 minutes
2024-09-11 10:52:02,504:INFO:SubProcess create_model() called ==================================
2024-09-11 10:52:02,504:INFO:Initializing create_model()
2024-09-11 10:52:02,504:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000026AE32C35D0>, estimator=gbc, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000026AE37C7490>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-11 10:52:02,505:INFO:Checking exceptions
2024-09-11 10:52:02,505:INFO:Importing libraries
2024-09-11 10:52:02,505:INFO:Copying training dataset
2024-09-11 10:52:02,511:INFO:Defining folds
2024-09-11 10:52:02,511:INFO:Declaring metric variables
2024-09-11 10:52:02,511:INFO:Importing untrained model
2024-09-11 10:52:02,512:INFO:Gradient Boosting Classifier Imported successfully
2024-09-11 10:52:02,512:INFO:Starting cross validation
2024-09-11 10:52:02,514:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-11 10:52:03,965:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:52:03,967:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:52:03,974:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:52:03,977:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:52:03,983:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:52:03,984:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:52:03,993:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:52:03,993:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:52:03,994:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:52:03,993:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:52:03,995:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:52:04,002:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 10:52:04,002:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:52:04,004:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:52:04,010:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:52:04,010:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:52:04,010:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:52:04,011:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:52:04,012:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:52:04,019:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:52:04,020:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:52:04,022:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:52:04,023:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:52:04,026:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:52:04,028:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:52:04,029:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:52:04,034:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:52:04,036:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:52:04,037:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:52:04,041:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:52:04,046:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:52:04,047:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:52:04,052:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:52:04,053:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:52:04,056:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:52:04,062:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:52:04,063:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:52:04,068:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:52:04,081:INFO:Calculating mean and std
2024-09-11 10:52:04,081:INFO:Creating metrics dataframe
2024-09-11 10:52:04,085:INFO:Uploading results into container
2024-09-11 10:52:04,086:INFO:Uploading model into container now
2024-09-11 10:52:04,086:INFO:_master_model_container: 10
2024-09-11 10:52:04,086:INFO:_display_container: 2
2024-09-11 10:52:04,087:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=369, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2024-09-11 10:52:04,087:INFO:create_model() successfully completed......................................
2024-09-11 10:52:04,147:INFO:SubProcess create_model() end ==================================
2024-09-11 10:52:04,147:INFO:Creating metrics dataframe
2024-09-11 10:52:04,151:INFO:Initializing Linear Discriminant Analysis
2024-09-11 10:52:04,151:INFO:Total runtime is 0.5989925424257914 minutes
2024-09-11 10:52:04,152:INFO:SubProcess create_model() called ==================================
2024-09-11 10:52:04,152:INFO:Initializing create_model()
2024-09-11 10:52:04,152:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000026AE32C35D0>, estimator=lda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000026AE37C7490>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-11 10:52:04,152:INFO:Checking exceptions
2024-09-11 10:52:04,152:INFO:Importing libraries
2024-09-11 10:52:04,152:INFO:Copying training dataset
2024-09-11 10:52:04,157:INFO:Defining folds
2024-09-11 10:52:04,157:INFO:Declaring metric variables
2024-09-11 10:52:04,158:INFO:Importing untrained model
2024-09-11 10:52:04,158:INFO:Linear Discriminant Analysis Imported successfully
2024-09-11 10:52:04,158:INFO:Starting cross validation
2024-09-11 10:52:04,161:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-11 10:52:04,451:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:52:04,455:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:52:04,459:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:52:04,464:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:52:04,468:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:52:04,469:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:52:04,470:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:52:04,475:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:52:04,476:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:52:04,479:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:52:04,479:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:52:04,481:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:52:04,482:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:52:04,482:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 10:52:04,487:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:52:04,487:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:52:04,491:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:52:04,492:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:52:04,494:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:52:04,494:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:52:04,495:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:52:04,496:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:52:04,497:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:52:04,498:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:52:04,499:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 10:52:04,501:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:52:04,504:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:52:04,504:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:52:04,508:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:52:04,510:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:52:04,511:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:52:04,513:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:52:04,514:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:52:04,518:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:52:04,520:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:52:04,521:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:52:04,524:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:52:04,527:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:52:04,531:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:52:04,532:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:52:04,537:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:52:04,555:INFO:Calculating mean and std
2024-09-11 10:52:04,556:INFO:Creating metrics dataframe
2024-09-11 10:52:04,559:INFO:Uploading results into container
2024-09-11 10:52:04,559:INFO:Uploading model into container now
2024-09-11 10:52:04,560:INFO:_master_model_container: 11
2024-09-11 10:52:04,560:INFO:_display_container: 2
2024-09-11 10:52:04,560:INFO:LinearDiscriminantAnalysis(covariance_estimator=None, n_components=None,
                           priors=None, shrinkage=None, solver='svd',
                           store_covariance=False, tol=0.0001)
2024-09-11 10:52:04,560:INFO:create_model() successfully completed......................................
2024-09-11 10:52:04,629:INFO:SubProcess create_model() end ==================================
2024-09-11 10:52:04,629:INFO:Creating metrics dataframe
2024-09-11 10:52:04,634:INFO:Initializing Extra Trees Classifier
2024-09-11 10:52:04,634:INFO:Total runtime is 0.6070432424545288 minutes
2024-09-11 10:52:04,635:INFO:SubProcess create_model() called ==================================
2024-09-11 10:52:04,635:INFO:Initializing create_model()
2024-09-11 10:52:04,635:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000026AE32C35D0>, estimator=et, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000026AE37C7490>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-11 10:52:04,635:INFO:Checking exceptions
2024-09-11 10:52:04,635:INFO:Importing libraries
2024-09-11 10:52:04,635:INFO:Copying training dataset
2024-09-11 10:52:04,641:INFO:Defining folds
2024-09-11 10:52:04,641:INFO:Declaring metric variables
2024-09-11 10:52:04,641:INFO:Importing untrained model
2024-09-11 10:52:04,642:INFO:Extra Trees Classifier Imported successfully
2024-09-11 10:52:04,643:INFO:Starting cross validation
2024-09-11 10:52:04,644:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-11 10:52:06,293:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:52:06,304:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:52:06,309:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:52:06,313:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:52:06,317:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:52:06,319:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:52:06,321:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:52:06,326:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:52:06,328:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 10:52:06,338:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:52:06,339:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:52:06,342:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:52:06,349:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:52:06,351:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:52:06,351:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:52:06,358:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:52:06,361:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:52:06,361:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:52:06,367:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:52:06,371:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:52:06,379:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:52:06,398:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:52:06,400:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:52:06,402:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:52:06,489:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:52:06,494:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:52:06,499:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:52:06,515:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:52:06,519:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:52:06,524:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:52:06,538:INFO:Calculating mean and std
2024-09-11 10:52:06,540:INFO:Creating metrics dataframe
2024-09-11 10:52:06,542:INFO:Uploading results into container
2024-09-11 10:52:06,543:INFO:Uploading model into container now
2024-09-11 10:52:06,543:INFO:_master_model_container: 12
2024-09-11 10:52:06,543:INFO:_display_container: 2
2024-09-11 10:52:06,544:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=369, verbose=0,
                     warm_start=False)
2024-09-11 10:52:06,544:INFO:create_model() successfully completed......................................
2024-09-11 10:52:06,608:INFO:SubProcess create_model() end ==================================
2024-09-11 10:52:06,608:INFO:Creating metrics dataframe
2024-09-11 10:52:06,612:INFO:Initializing Light Gradient Boosting Machine
2024-09-11 10:52:06,612:INFO:Total runtime is 0.6400117953618367 minutes
2024-09-11 10:52:06,613:INFO:SubProcess create_model() called ==================================
2024-09-11 10:52:06,613:INFO:Initializing create_model()
2024-09-11 10:52:06,613:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000026AE32C35D0>, estimator=lightgbm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000026AE37C7490>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-11 10:52:06,613:INFO:Checking exceptions
2024-09-11 10:52:06,613:INFO:Importing libraries
2024-09-11 10:52:06,613:INFO:Copying training dataset
2024-09-11 10:52:06,619:INFO:Defining folds
2024-09-11 10:52:06,619:INFO:Declaring metric variables
2024-09-11 10:52:06,619:INFO:Importing untrained model
2024-09-11 10:52:06,620:INFO:Light Gradient Boosting Machine Imported successfully
2024-09-11 10:52:06,620:INFO:Starting cross validation
2024-09-11 10:52:06,623:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-11 10:52:11,027:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:52:11,047:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:52:11,058:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 10:52:11,064:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:52:11,130:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:52:11,146:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:52:11,156:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:52:11,163:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:52:11,163:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:52:11,169:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:52:11,175:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:52:11,188:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:52:11,196:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:52:11,291:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:52:11,328:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:52:11,342:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:52:11,492:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:52:11,510:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:52:11,530:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:52:11,628:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:52:11,672:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:52:11,696:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:52:12,041:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:52:12,053:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:52:12,067:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:52:12,089:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:52:12,094:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:52:12,098:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:52:12,106:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:52:12,111:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:52:12,116:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:52:12,136:INFO:Calculating mean and std
2024-09-11 10:52:12,138:INFO:Creating metrics dataframe
2024-09-11 10:52:12,145:INFO:Uploading results into container
2024-09-11 10:52:12,146:INFO:Uploading model into container now
2024-09-11 10:52:12,147:INFO:_master_model_container: 13
2024-09-11 10:52:12,147:INFO:_display_container: 2
2024-09-11 10:52:12,150:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=369, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-09-11 10:52:12,150:INFO:create_model() successfully completed......................................
2024-09-11 10:52:12,248:INFO:SubProcess create_model() end ==================================
2024-09-11 10:52:12,248:INFO:Creating metrics dataframe
2024-09-11 10:52:12,254:INFO:Initializing Dummy Classifier
2024-09-11 10:52:12,254:INFO:Total runtime is 0.7340392510096232 minutes
2024-09-11 10:52:12,254:INFO:SubProcess create_model() called ==================================
2024-09-11 10:52:12,255:INFO:Initializing create_model()
2024-09-11 10:52:12,255:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000026AE32C35D0>, estimator=dummy, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000026AE37C7490>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-11 10:52:12,255:INFO:Checking exceptions
2024-09-11 10:52:12,255:INFO:Importing libraries
2024-09-11 10:52:12,255:INFO:Copying training dataset
2024-09-11 10:52:12,261:INFO:Defining folds
2024-09-11 10:52:12,261:INFO:Declaring metric variables
2024-09-11 10:52:12,261:INFO:Importing untrained model
2024-09-11 10:52:12,262:INFO:Dummy Classifier Imported successfully
2024-09-11 10:52:12,262:INFO:Starting cross validation
2024-09-11 10:52:12,264:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-11 10:52:12,595:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:52:12,607:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:52:12,611:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:52:12,619:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 10:52:12,629:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:52:12,633:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:52:12,639:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 10:52:12,641:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:52:12,650:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:52:12,655:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:52:12,663:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:52:12,664:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:52:12,664:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 10:52:12,675:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:52:12,675:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:52:12,682:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:52:12,687:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:52:12,688:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:52:12,688:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:52:12,691:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 10:52:12,691:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:52:12,692:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:52:12,694:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 10:52:12,695:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:52:12,700:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:52:12,699:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 10:52:12,703:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:52:12,704:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:52:12,704:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:52:12,706:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:52:12,709:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:52:12,710:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 10:52:12,712:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:52:12,712:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 10:52:12,712:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 10:52:12,715:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 10:52:12,717:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:52:12,720:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:52:12,721:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:52:12,725:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 10:52:12,743:INFO:Calculating mean and std
2024-09-11 10:52:12,744:INFO:Creating metrics dataframe
2024-09-11 10:52:12,747:INFO:Uploading results into container
2024-09-11 10:52:12,748:INFO:Uploading model into container now
2024-09-11 10:52:12,748:INFO:_master_model_container: 14
2024-09-11 10:52:12,749:INFO:_display_container: 2
2024-09-11 10:52:12,749:INFO:DummyClassifier(constant=None, random_state=369, strategy='prior')
2024-09-11 10:52:12,749:INFO:create_model() successfully completed......................................
2024-09-11 10:52:12,842:INFO:SubProcess create_model() end ==================================
2024-09-11 10:52:12,842:INFO:Creating metrics dataframe
2024-09-11 10:52:12,849:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:339: FutureWarning: Styler.applymap has been deprecated. Use Styler.map instead.
  .applymap(highlight_cols, subset=["TT (Sec)"])

2024-09-11 10:52:12,854:INFO:Initializing create_model()
2024-09-11 10:52:12,854:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000026AE32C35D0>, estimator=ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=369, verbose=0,
                     warm_start=False), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-11 10:52:12,854:INFO:Checking exceptions
2024-09-11 10:52:12,855:INFO:Importing libraries
2024-09-11 10:52:12,855:INFO:Copying training dataset
2024-09-11 10:52:12,860:INFO:Defining folds
2024-09-11 10:52:12,861:INFO:Declaring metric variables
2024-09-11 10:52:12,861:INFO:Importing untrained model
2024-09-11 10:52:12,861:INFO:Declaring custom model
2024-09-11 10:52:12,862:INFO:Extra Trees Classifier Imported successfully
2024-09-11 10:52:12,864:INFO:Cross validation set to False
2024-09-11 10:52:12,865:INFO:Fitting Model
2024-09-11 10:52:13,127:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=369, verbose=0,
                     warm_start=False)
2024-09-11 10:52:13,127:INFO:create_model() successfully completed......................................
2024-09-11 10:52:13,215:INFO:_master_model_container: 14
2024-09-11 10:52:13,215:INFO:_display_container: 2
2024-09-11 10:52:13,216:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=369, verbose=0,
                     warm_start=False)
2024-09-11 10:52:13,217:INFO:compare_models() successfully completed......................................
2024-09-11 10:52:13,218:INFO:Initializing plot_model()
2024-09-11 10:52:13,218:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000026AE32C35D0>, estimator=ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=369, verbose=0,
                     warm_start=False), plot=confusion_matrix, scale=1, save=False, fold=None, fit_kwargs=None, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=True, system=True, display=None, display_format=None)
2024-09-11 10:52:13,218:INFO:Checking exceptions
2024-09-11 10:52:13,277:INFO:Preloading libraries
2024-09-11 10:52:13,300:INFO:Copying training dataset
2024-09-11 10:52:13,300:INFO:Plot type: confusion_matrix
2024-09-11 10:52:13,875:INFO:Fitting Model
2024-09-11 10:52:13,876:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\base.py:493: UserWarning: X does not have valid feature names, but ExtraTreesClassifier was fitted with feature names
  warnings.warn(

2024-09-11 10:52:13,877:INFO:Scoring test/hold-out set
2024-09-11 11:10:50,842:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-11 11:10:50,843:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-11 11:10:50,843:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-11 11:10:50,843:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-11 11:10:52,739:INFO:PyCaret ClassificationExperiment
2024-09-11 11:10:52,740:INFO:Logging name: clf-default-name
2024-09-11 11:10:52,740:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2024-09-11 11:10:52,740:INFO:version 3.3.2
2024-09-11 11:10:52,740:INFO:Initializing setup()
2024-09-11 11:10:52,740:INFO:self.USI: c22c
2024-09-11 11:10:52,741:INFO:self._variable_keys: {'data', 'X', 'exp_id', 'fold_generator', 'is_multiclass', 'log_plots_param', 'html_param', 'logging_param', 'seed', 'exp_name_log', 'target_param', 'y_test', 'fix_imbalance', 'gpu_n_jobs_param', 'fold_shuffle_param', 'memory', 'y_train', 'y', 'n_jobs_param', 'gpu_param', 'X_train', '_available_plots', 'fold_groups_param', 'pipeline', 'USI', '_ml_usecase', 'idx', 'X_test'}
2024-09-11 11:10:52,741:INFO:Checking environment
2024-09-11 11:10:52,741:INFO:python_version: 3.11.8
2024-09-11 11:10:52,741:INFO:python_build: ('tags/v3.11.8:db85d51', 'Feb  6 2024 22:03:32')
2024-09-11 11:10:52,741:INFO:machine: AMD64
2024-09-11 11:10:52,741:INFO:platform: Windows-10-10.0.22631-SP0
2024-09-11 11:10:52,751:INFO:Memory: svmem(total=17096892416, available=6090465280, percent=64.4, used=11006427136, free=6090465280)
2024-09-11 11:10:52,752:INFO:Physical Core: 6
2024-09-11 11:10:52,752:INFO:Logical Core: 12
2024-09-11 11:10:52,752:INFO:Checking libraries
2024-09-11 11:10:52,752:INFO:System:
2024-09-11 11:10:52,752:INFO:    python: 3.11.8 (tags/v3.11.8:db85d51, Feb  6 2024, 22:03:32) [MSC v.1937 64 bit (AMD64)]
2024-09-11 11:10:52,752:INFO:executable: h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Scripts\python.exe
2024-09-11 11:10:52,752:INFO:   machine: Windows-10-10.0.22631-SP0
2024-09-11 11:10:52,752:INFO:PyCaret required dependencies:
2024-09-11 11:10:52,808:INFO:                 pip: 24.2
2024-09-11 11:10:52,808:INFO:          setuptools: 65.5.0
2024-09-11 11:10:52,808:INFO:             pycaret: 3.3.2
2024-09-11 11:10:52,808:INFO:             IPython: 8.27.0
2024-09-11 11:10:52,808:INFO:          ipywidgets: 8.1.5
2024-09-11 11:10:52,808:INFO:                tqdm: 4.66.5
2024-09-11 11:10:52,808:INFO:               numpy: 1.26.4
2024-09-11 11:10:52,808:INFO:              pandas: 2.1.4
2024-09-11 11:10:52,808:INFO:              jinja2: 3.1.4
2024-09-11 11:10:52,808:INFO:               scipy: 1.11.4
2024-09-11 11:10:52,809:INFO:              joblib: 1.3.2
2024-09-11 11:10:52,809:INFO:             sklearn: 1.4.2
2024-09-11 11:10:52,809:INFO:                pyod: 2.0.2
2024-09-11 11:10:52,809:INFO:            imblearn: 0.12.3
2024-09-11 11:10:52,809:INFO:   category_encoders: 2.6.3
2024-09-11 11:10:52,809:INFO:            lightgbm: 4.5.0
2024-09-11 11:10:52,809:INFO:               numba: 0.60.0
2024-09-11 11:10:52,809:INFO:            requests: 2.32.3
2024-09-11 11:10:52,809:INFO:          matplotlib: 3.7.5
2024-09-11 11:10:52,809:INFO:          scikitplot: 0.3.7
2024-09-11 11:10:52,809:INFO:         yellowbrick: 1.5
2024-09-11 11:10:52,809:INFO:              plotly: 5.24.0
2024-09-11 11:10:52,810:INFO:    plotly-resampler: Not installed
2024-09-11 11:10:52,810:INFO:             kaleido: 0.2.1
2024-09-11 11:10:52,810:INFO:           schemdraw: 0.15
2024-09-11 11:10:52,810:INFO:         statsmodels: 0.14.2
2024-09-11 11:10:52,810:INFO:              sktime: 0.26.0
2024-09-11 11:10:52,811:INFO:               tbats: 1.1.3
2024-09-11 11:10:52,812:INFO:            pmdarima: 2.0.4
2024-09-11 11:10:52,812:INFO:              psutil: 6.0.0
2024-09-11 11:10:52,812:INFO:          markupsafe: 2.1.5
2024-09-11 11:10:52,812:INFO:             pickle5: Not installed
2024-09-11 11:10:52,812:INFO:         cloudpickle: 3.0.0
2024-09-11 11:10:52,812:INFO:         deprecation: 2.1.0
2024-09-11 11:10:52,812:INFO:              xxhash: 3.5.0
2024-09-11 11:10:52,813:INFO:           wurlitzer: Not installed
2024-09-11 11:10:52,813:INFO:PyCaret optional dependencies:
2024-09-11 11:10:52,847:INFO:                shap: Not installed
2024-09-11 11:10:52,847:INFO:           interpret: Not installed
2024-09-11 11:10:52,847:INFO:                umap: Not installed
2024-09-11 11:10:52,847:INFO:     ydata_profiling: Not installed
2024-09-11 11:10:52,847:INFO:  explainerdashboard: Not installed
2024-09-11 11:10:52,848:INFO:             autoviz: Not installed
2024-09-11 11:10:52,848:INFO:           fairlearn: Not installed
2024-09-11 11:10:52,848:INFO:          deepchecks: Not installed
2024-09-11 11:10:52,848:INFO:             xgboost: Not installed
2024-09-11 11:10:52,848:INFO:            catboost: Not installed
2024-09-11 11:10:52,848:INFO:              kmodes: Not installed
2024-09-11 11:10:52,848:INFO:             mlxtend: Not installed
2024-09-11 11:10:52,848:INFO:       statsforecast: Not installed
2024-09-11 11:10:52,848:INFO:        tune_sklearn: Not installed
2024-09-11 11:10:52,848:INFO:                 ray: Not installed
2024-09-11 11:10:52,848:INFO:            hyperopt: Not installed
2024-09-11 11:10:52,848:INFO:              optuna: Not installed
2024-09-11 11:10:52,848:INFO:               skopt: Not installed
2024-09-11 11:10:52,848:INFO:              mlflow: Not installed
2024-09-11 11:10:52,848:INFO:              gradio: Not installed
2024-09-11 11:10:52,848:INFO:             fastapi: Not installed
2024-09-11 11:10:52,848:INFO:             uvicorn: Not installed
2024-09-11 11:10:52,848:INFO:              m2cgen: Not installed
2024-09-11 11:10:52,848:INFO:           evidently: Not installed
2024-09-11 11:10:52,849:INFO:               fugue: Not installed
2024-09-11 11:10:52,849:INFO:           streamlit: Not installed
2024-09-11 11:10:52,849:INFO:             prophet: Not installed
2024-09-11 11:10:52,849:INFO:None
2024-09-11 11:10:52,849:INFO:Set up data.
2024-09-11 11:10:52,863:INFO:Set up folding strategy.
2024-09-11 11:10:52,863:INFO:Set up train/test split.
2024-09-11 11:10:52,875:INFO:Set up index.
2024-09-11 11:10:52,875:INFO:Assigning column types.
2024-09-11 11:10:52,880:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2024-09-11 11:10:52,923:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-09-11 11:10:52,928:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-09-11 11:10:52,968:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-11 11:10:52,968:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-11 11:10:53,010:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-09-11 11:10:53,011:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-09-11 11:10:53,036:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-11 11:10:53,038:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-11 11:10:53,038:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2024-09-11 11:10:53,082:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-09-11 11:10:53,109:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-11 11:10:53,110:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-11 11:10:53,152:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-09-11 11:10:53,180:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-11 11:10:53,180:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-11 11:10:53,181:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2024-09-11 11:10:53,253:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-11 11:10:53,253:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-11 11:10:53,325:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-11 11:10:53,325:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-11 11:10:53,329:INFO:Preparing preprocessing pipeline...
2024-09-11 11:10:53,331:INFO:Set up label encoding.
2024-09-11 11:10:53,331:INFO:Set up simple imputation.
2024-09-11 11:10:53,334:INFO:Set up encoding of categorical features.
2024-09-11 11:10:53,462:INFO:Finished creating preprocessing pipeline.
2024-09-11 11:10:53,478:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\ipkov\AppData\Local\Temp\joblib),
         steps=[('label_encoding',
                 TransformerWrapperWithInverse(exclude=None, include=None,
                                               transformer=LabelEncoder())),
                ('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['MPG', 'Cylinders', 'Displacement',
                                             'Horsepower', 'Weight',
                                             'Acceleration', 'Model'],
                                    transformer=SimpleImp...
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='most_frequent'))),
                ('rest_encoding',
                 TransformerWrapper(exclude=None, include=['Car'],
                                    transformer=TargetEncoder(cols=['Car'],
                                                              drop_invariant=False,
                                                              handle_missing='return_nan',
                                                              handle_unknown='value',
                                                              hierarchy=None,
                                                              min_samples_leaf=20,
                                                              return_df=True,
                                                              smoothing=10,
                                                              verbose=0)))],
         verbose=False)
2024-09-11 11:10:53,479:INFO:Creating final display dataframe.
2024-09-11 11:10:53,895:INFO:Setup _display_container:                     Description                       Value
0                    Session id                        3218
1                        Target                      Origin
2                   Target type                  Multiclass
3                Target mapping  Europe: 0, Japan: 1, US: 2
4           Original data shape                    (406, 9)
5        Transformed data shape                    (406, 9)
6   Transformed train set shape                    (284, 9)
7    Transformed test set shape                    (122, 9)
8              Numeric features                           7
9          Categorical features                           1
10                   Preprocess                        True
11              Imputation type                      simple
12           Numeric imputation                        mean
13       Categorical imputation                        mode
14     Maximum one-hot encoding                          25
15              Encoding method                        None
16               Fold Generator             StratifiedKFold
17                  Fold Number                          10
18                     CPU Jobs                          -1
19                      Use GPU                       False
20               Log Experiment                       False
21              Experiment Name            clf-default-name
22                          USI                        c22c
2024-09-11 11:10:53,979:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-11 11:10:53,980:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-11 11:10:54,052:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-11 11:10:54,053:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-11 11:10:54,055:INFO:setup() successfully completed in 1.32s...............
2024-09-11 11:10:54,055:INFO:Initializing compare_models()
2024-09-11 11:10:54,056:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002B573BE74D0>, include=None, exclude=None, fold=None, round=4, cross_validation=True, sort=Accuracy, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x000002B573BE74D0>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'Accuracy', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>})
2024-09-11 11:10:54,056:INFO:Checking exceptions
2024-09-11 11:10:54,060:INFO:Preparing display monitor
2024-09-11 11:10:54,068:INFO:Initializing Logistic Regression
2024-09-11 11:10:54,068:INFO:Total runtime is 0.0 minutes
2024-09-11 11:10:54,069:INFO:SubProcess create_model() called ==================================
2024-09-11 11:10:54,069:INFO:Initializing create_model()
2024-09-11 11:10:54,070:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002B573BE74D0>, estimator=lr, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002B573DC7490>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-11 11:10:54,070:INFO:Checking exceptions
2024-09-11 11:10:54,070:INFO:Importing libraries
2024-09-11 11:10:54,070:INFO:Copying training dataset
2024-09-11 11:10:54,076:INFO:Defining folds
2024-09-11 11:10:54,076:INFO:Declaring metric variables
2024-09-11 11:10:54,076:INFO:Importing untrained model
2024-09-11 11:10:54,078:INFO:Logistic Regression Imported successfully
2024-09-11 11:10:54,078:INFO:Starting cross validation
2024-09-11 11:10:54,081:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-11 11:11:11,975:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-11 11:11:12,009:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-11 11:11:12,016:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-11 11:11:12,027:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-11 11:11:12,087:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 11:11:12,096:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:12,110:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:12,125:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 11:11:12,126:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:12,135:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:12,136:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 11:11:12,145:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:12,148:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:12,157:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:12,159:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:12,159:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 11:11:12,170:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:12,171:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:12,175:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-11 11:11:12,180:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-11 11:11:12,184:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:12,193:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:12,215:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-11 11:11:12,264:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 11:11:12,267:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 11:11:12,274:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:12,276:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:12,289:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:12,291:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:12,304:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:12,305:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:12,311:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 11:11:12,311:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-11 11:11:12,322:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:12,335:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:12,344:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:12,363:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-11 11:11:12,365:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-11 11:11:12,390:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 11:11:12,396:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:12,405:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:12,412:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:12,419:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 11:11:12,425:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:12,435:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 11:11:12,439:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:12,443:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:12,445:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:12,453:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:12,461:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:12,482:INFO:Calculating mean and std
2024-09-11 11:11:12,484:INFO:Creating metrics dataframe
2024-09-11 11:11:12,489:INFO:Uploading results into container
2024-09-11 11:11:12,490:INFO:Uploading model into container now
2024-09-11 11:11:12,491:INFO:_master_model_container: 1
2024-09-11 11:11:12,491:INFO:_display_container: 2
2024-09-11 11:11:12,492:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=3218, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2024-09-11 11:11:12,492:INFO:create_model() successfully completed......................................
2024-09-11 11:11:12,576:INFO:SubProcess create_model() end ==================================
2024-09-11 11:11:12,576:INFO:Creating metrics dataframe
2024-09-11 11:11:12,581:INFO:Initializing K Neighbors Classifier
2024-09-11 11:11:12,581:INFO:Total runtime is 0.30853637456893923 minutes
2024-09-11 11:11:12,582:INFO:SubProcess create_model() called ==================================
2024-09-11 11:11:12,582:INFO:Initializing create_model()
2024-09-11 11:11:12,582:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002B573BE74D0>, estimator=knn, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002B573DC7490>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-11 11:11:12,582:INFO:Checking exceptions
2024-09-11 11:11:12,582:INFO:Importing libraries
2024-09-11 11:11:12,582:INFO:Copying training dataset
2024-09-11 11:11:12,588:INFO:Defining folds
2024-09-11 11:11:12,588:INFO:Declaring metric variables
2024-09-11 11:11:12,588:INFO:Importing untrained model
2024-09-11 11:11:12,589:INFO:K Neighbors Classifier Imported successfully
2024-09-11 11:11:12,589:INFO:Starting cross validation
2024-09-11 11:11:12,592:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-11 11:11:13,385:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:13,386:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:13,395:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:13,395:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:13,406:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:13,412:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:13,416:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:13,418:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:13,419:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:13,419:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:13,421:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:13,431:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:13,436:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:13,436:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:13,438:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:13,438:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:13,440:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:13,442:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:13,449:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:13,455:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:13,461:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:13,462:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:13,472:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:13,477:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:17,609:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:17,612:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:17,615:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:17,618:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:17,621:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:17,622:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:17,634:INFO:Calculating mean and std
2024-09-11 11:11:17,636:INFO:Creating metrics dataframe
2024-09-11 11:11:17,639:INFO:Uploading results into container
2024-09-11 11:11:17,640:INFO:Uploading model into container now
2024-09-11 11:11:17,640:INFO:_master_model_container: 2
2024-09-11 11:11:17,641:INFO:_display_container: 2
2024-09-11 11:11:17,641:INFO:KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform')
2024-09-11 11:11:17,641:INFO:create_model() successfully completed......................................
2024-09-11 11:11:17,735:INFO:SubProcess create_model() end ==================================
2024-09-11 11:11:17,735:INFO:Creating metrics dataframe
2024-09-11 11:11:17,740:INFO:Initializing Naive Bayes
2024-09-11 11:11:17,740:INFO:Total runtime is 0.3945258378982544 minutes
2024-09-11 11:11:17,740:INFO:SubProcess create_model() called ==================================
2024-09-11 11:11:17,740:INFO:Initializing create_model()
2024-09-11 11:11:17,740:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002B573BE74D0>, estimator=nb, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002B573DC7490>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-11 11:11:17,741:INFO:Checking exceptions
2024-09-11 11:11:17,741:INFO:Importing libraries
2024-09-11 11:11:17,741:INFO:Copying training dataset
2024-09-11 11:11:17,746:INFO:Defining folds
2024-09-11 11:11:17,746:INFO:Declaring metric variables
2024-09-11 11:11:17,746:INFO:Importing untrained model
2024-09-11 11:11:17,748:INFO:Naive Bayes Imported successfully
2024-09-11 11:11:17,748:INFO:Starting cross validation
2024-09-11 11:11:17,750:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-11 11:11:18,089:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:18,091:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:18,105:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:18,106:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:18,108:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:18,109:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:18,119:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:18,125:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:18,126:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:18,126:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:18,128:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:18,131:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:18,134:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:18,137:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:18,141:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:18,141:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:18,144:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:18,144:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 11:11:18,144:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:18,147:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:18,148:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:18,153:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:18,155:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:18,155:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 11:11:18,159:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:18,159:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:18,159:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:18,161:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:18,162:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:18,162:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 11:11:18,165:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 11:11:18,167:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:18,168:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:18,169:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:18,184:INFO:Calculating mean and std
2024-09-11 11:11:18,185:INFO:Creating metrics dataframe
2024-09-11 11:11:18,188:INFO:Uploading results into container
2024-09-11 11:11:18,188:INFO:Uploading model into container now
2024-09-11 11:11:18,189:INFO:_master_model_container: 3
2024-09-11 11:11:18,189:INFO:_display_container: 2
2024-09-11 11:11:18,189:INFO:GaussianNB(priors=None, var_smoothing=1e-09)
2024-09-11 11:11:18,189:INFO:create_model() successfully completed......................................
2024-09-11 11:11:18,251:INFO:SubProcess create_model() end ==================================
2024-09-11 11:11:18,251:INFO:Creating metrics dataframe
2024-09-11 11:11:18,255:INFO:Initializing Decision Tree Classifier
2024-09-11 11:11:18,256:INFO:Total runtime is 0.4031206687291463 minutes
2024-09-11 11:11:18,256:INFO:SubProcess create_model() called ==================================
2024-09-11 11:11:18,256:INFO:Initializing create_model()
2024-09-11 11:11:18,256:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002B573BE74D0>, estimator=dt, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002B573DC7490>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-11 11:11:18,256:INFO:Checking exceptions
2024-09-11 11:11:18,256:INFO:Importing libraries
2024-09-11 11:11:18,257:INFO:Copying training dataset
2024-09-11 11:11:18,262:INFO:Defining folds
2024-09-11 11:11:18,262:INFO:Declaring metric variables
2024-09-11 11:11:18,262:INFO:Importing untrained model
2024-09-11 11:11:18,263:INFO:Decision Tree Classifier Imported successfully
2024-09-11 11:11:18,263:INFO:Starting cross validation
2024-09-11 11:11:18,265:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-11 11:11:18,587:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:18,602:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:18,605:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:18,606:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:18,608:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:18,609:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:18,614:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:18,619:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:18,622:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:18,623:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:18,624:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:18,627:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:18,627:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:18,631:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:18,632:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:18,632:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 11:11:18,634:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:18,636:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:18,640:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 11:11:18,643:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:18,643:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:18,644:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:18,644:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:18,648:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:18,650:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 11:11:18,651:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:18,652:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:18,652:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:18,658:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:18,664:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:18,668:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:18,668:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:18,671:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 11:11:18,673:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:18,681:INFO:Calculating mean and std
2024-09-11 11:11:18,682:INFO:Creating metrics dataframe
2024-09-11 11:11:18,685:INFO:Uploading results into container
2024-09-11 11:11:18,685:INFO:Uploading model into container now
2024-09-11 11:11:18,686:INFO:_master_model_container: 4
2024-09-11 11:11:18,686:INFO:_display_container: 2
2024-09-11 11:11:18,687:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=3218, splitter='best')
2024-09-11 11:11:18,687:INFO:create_model() successfully completed......................................
2024-09-11 11:11:18,746:INFO:SubProcess create_model() end ==================================
2024-09-11 11:11:18,746:INFO:Creating metrics dataframe
2024-09-11 11:11:18,751:INFO:Initializing SVM - Linear Kernel
2024-09-11 11:11:18,751:INFO:Total runtime is 0.4113736351331075 minutes
2024-09-11 11:11:18,752:INFO:SubProcess create_model() called ==================================
2024-09-11 11:11:18,752:INFO:Initializing create_model()
2024-09-11 11:11:18,752:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002B573BE74D0>, estimator=svm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002B573DC7490>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-11 11:11:18,752:INFO:Checking exceptions
2024-09-11 11:11:18,752:INFO:Importing libraries
2024-09-11 11:11:18,753:INFO:Copying training dataset
2024-09-11 11:11:18,758:INFO:Defining folds
2024-09-11 11:11:18,758:INFO:Declaring metric variables
2024-09-11 11:11:18,759:INFO:Importing untrained model
2024-09-11 11:11:18,759:INFO:SVM - Linear Kernel Imported successfully
2024-09-11 11:11:18,760:INFO:Starting cross validation
2024-09-11 11:11:18,762:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-11 11:11:19,208:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 11:11:19,209:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 11:11:19,217:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:19,221:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:19,227:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 11:11:19,233:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:19,237:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:19,244:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:19,245:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 11:11:19,255:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 11:11:19,255:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:19,256:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 11:11:19,259:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:19,264:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 11:11:19,266:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 11:11:19,266:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:19,273:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:19,275:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:19,278:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:19,278:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 11:11:19,280:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 11:11:19,284:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:19,289:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:19,292:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:19,295:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:19,296:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 11:11:19,302:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 11:11:19,303:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 11:11:19,310:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 11:11:19,312:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:19,313:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:19,314:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:19,321:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:19,323:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 11:11:19,323:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 11:11:19,328:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:19,333:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:19,342:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:19,344:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:19,377:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 11:11:19,382:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:19,389:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 11:11:19,394:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:19,405:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 11:11:19,408:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:19,418:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:19,425:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:19,428:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 11:11:19,432:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:19,446:INFO:Calculating mean and std
2024-09-11 11:11:19,450:INFO:Creating metrics dataframe
2024-09-11 11:11:19,456:INFO:Uploading results into container
2024-09-11 11:11:19,456:INFO:Uploading model into container now
2024-09-11 11:11:19,457:INFO:_master_model_container: 5
2024-09-11 11:11:19,457:INFO:_display_container: 2
2024-09-11 11:11:19,459:INFO:SGDClassifier(alpha=0.0001, average=False, class_weight=None,
              early_stopping=False, epsilon=0.1, eta0=0.001, fit_intercept=True,
              l1_ratio=0.15, learning_rate='optimal', loss='hinge',
              max_iter=1000, n_iter_no_change=5, n_jobs=-1, penalty='l2',
              power_t=0.5, random_state=3218, shuffle=True, tol=0.001,
              validation_fraction=0.1, verbose=0, warm_start=False)
2024-09-11 11:11:19,460:INFO:create_model() successfully completed......................................
2024-09-11 11:11:19,528:INFO:SubProcess create_model() end ==================================
2024-09-11 11:11:19,528:INFO:Creating metrics dataframe
2024-09-11 11:11:19,532:INFO:Initializing Ridge Classifier
2024-09-11 11:11:19,532:INFO:Total runtime is 0.4243914524714152 minutes
2024-09-11 11:11:19,532:INFO:SubProcess create_model() called ==================================
2024-09-11 11:11:19,532:INFO:Initializing create_model()
2024-09-11 11:11:19,533:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002B573BE74D0>, estimator=ridge, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002B573DC7490>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-11 11:11:19,533:INFO:Checking exceptions
2024-09-11 11:11:19,533:INFO:Importing libraries
2024-09-11 11:11:19,533:INFO:Copying training dataset
2024-09-11 11:11:19,538:INFO:Defining folds
2024-09-11 11:11:19,538:INFO:Declaring metric variables
2024-09-11 11:11:19,538:INFO:Importing untrained model
2024-09-11 11:11:19,539:INFO:Ridge Classifier Imported successfully
2024-09-11 11:11:19,539:INFO:Starting cross validation
2024-09-11 11:11:19,541:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-11 11:11:19,855:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 11:11:19,861:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 11:11:19,865:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:19,866:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 11:11:19,870:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 11:11:19,873:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:19,876:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:19,878:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:19,883:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:19,889:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 11:11:19,891:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:19,892:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:19,894:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 11:11:19,896:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:19,899:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 11:11:19,899:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 11:11:19,899:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:19,901:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:19,904:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:19,907:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:19,912:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:19,916:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 11:11:19,916:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 11:11:19,917:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:19,922:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:19,925:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:19,926:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 11:11:19,926:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:19,926:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:19,935:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:19,936:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:19,939:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:19,939:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:19,940:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:19,940:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:19,943:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 11:11:19,945:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:19,946:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:19,964:INFO:Calculating mean and std
2024-09-11 11:11:19,965:INFO:Creating metrics dataframe
2024-09-11 11:11:19,967:INFO:Uploading results into container
2024-09-11 11:11:19,968:INFO:Uploading model into container now
2024-09-11 11:11:19,969:INFO:_master_model_container: 6
2024-09-11 11:11:19,969:INFO:_display_container: 2
2024-09-11 11:11:19,969:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=3218, solver='auto',
                tol=0.0001)
2024-09-11 11:11:19,969:INFO:create_model() successfully completed......................................
2024-09-11 11:11:20,028:INFO:SubProcess create_model() end ==================================
2024-09-11 11:11:20,028:INFO:Creating metrics dataframe
2024-09-11 11:11:20,032:INFO:Initializing Random Forest Classifier
2024-09-11 11:11:20,032:INFO:Total runtime is 0.4327316323916117 minutes
2024-09-11 11:11:20,033:INFO:SubProcess create_model() called ==================================
2024-09-11 11:11:20,033:INFO:Initializing create_model()
2024-09-11 11:11:20,033:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002B573BE74D0>, estimator=rf, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002B573DC7490>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-11 11:11:20,033:INFO:Checking exceptions
2024-09-11 11:11:20,033:INFO:Importing libraries
2024-09-11 11:11:20,033:INFO:Copying training dataset
2024-09-11 11:11:20,039:INFO:Defining folds
2024-09-11 11:11:20,039:INFO:Declaring metric variables
2024-09-11 11:11:20,039:INFO:Importing untrained model
2024-09-11 11:11:20,040:INFO:Random Forest Classifier Imported successfully
2024-09-11 11:11:20,041:INFO:Starting cross validation
2024-09-11 11:11:20,043:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-11 11:11:22,041:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:22,041:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:22,062:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:22,066:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:22,067:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:22,072:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:22,081:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:22,082:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:22,084:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:22,086:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:22,093:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:22,096:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 11:11:22,108:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:22,112:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:22,113:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:22,118:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:22,118:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:22,128:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:22,129:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:22,131:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:22,137:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:22,146:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:22,148:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:22,149:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:22,150:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:22,168:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:22,175:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:22,179:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 11:11:22,186:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:22,237:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:22,246:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:22,254:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:22,269:INFO:Calculating mean and std
2024-09-11 11:11:22,270:INFO:Creating metrics dataframe
2024-09-11 11:11:22,273:INFO:Uploading results into container
2024-09-11 11:11:22,273:INFO:Uploading model into container now
2024-09-11 11:11:22,274:INFO:_master_model_container: 7
2024-09-11 11:11:22,274:INFO:_display_container: 2
2024-09-11 11:11:22,275:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=3218, verbose=0,
                       warm_start=False)
2024-09-11 11:11:22,275:INFO:create_model() successfully completed......................................
2024-09-11 11:11:22,335:INFO:SubProcess create_model() end ==================================
2024-09-11 11:11:22,336:INFO:Creating metrics dataframe
2024-09-11 11:11:22,339:INFO:Initializing Quadratic Discriminant Analysis
2024-09-11 11:11:22,340:INFO:Total runtime is 0.47119590441385906 minutes
2024-09-11 11:11:22,340:INFO:SubProcess create_model() called ==================================
2024-09-11 11:11:22,340:INFO:Initializing create_model()
2024-09-11 11:11:22,340:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002B573BE74D0>, estimator=qda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002B573DC7490>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-11 11:11:22,341:INFO:Checking exceptions
2024-09-11 11:11:22,341:INFO:Importing libraries
2024-09-11 11:11:22,341:INFO:Copying training dataset
2024-09-11 11:11:22,348:INFO:Defining folds
2024-09-11 11:11:22,349:INFO:Declaring metric variables
2024-09-11 11:11:22,349:INFO:Importing untrained model
2024-09-11 11:11:22,350:INFO:Quadratic Discriminant Analysis Imported successfully
2024-09-11 11:11:22,350:INFO:Starting cross validation
2024-09-11 11:11:22,352:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-11 11:11:22,671:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 11:11:22,678:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 11:11:22,680:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:22,686:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:22,690:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 11:11:22,691:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 11:11:22,691:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 11:11:22,697:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:22,699:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:22,699:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:22,700:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:22,700:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:22,702:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 11:11:22,705:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:22,707:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 11:11:22,711:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:22,712:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:22,713:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:22,716:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:22,716:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:22,716:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:22,716:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 11:11:22,720:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 11:11:22,721:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 11:11:22,726:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:22,727:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:22,729:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:22,730:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:22,731:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:22,732:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:22,735:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:22,744:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:22,748:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:22,749:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:22,752:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 11:11:22,762:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:22,762:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:22,783:INFO:Calculating mean and std
2024-09-11 11:11:22,784:INFO:Creating metrics dataframe
2024-09-11 11:11:22,787:INFO:Uploading results into container
2024-09-11 11:11:22,787:INFO:Uploading model into container now
2024-09-11 11:11:22,788:INFO:_master_model_container: 8
2024-09-11 11:11:22,788:INFO:_display_container: 2
2024-09-11 11:11:22,788:INFO:QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,
                              store_covariance=False, tol=0.0001)
2024-09-11 11:11:22,788:INFO:create_model() successfully completed......................................
2024-09-11 11:11:22,853:INFO:SubProcess create_model() end ==================================
2024-09-11 11:11:22,854:INFO:Creating metrics dataframe
2024-09-11 11:11:22,858:INFO:Initializing Ada Boost Classifier
2024-09-11 11:11:22,858:INFO:Total runtime is 0.47981969912846884 minutes
2024-09-11 11:11:22,858:INFO:SubProcess create_model() called ==================================
2024-09-11 11:11:22,858:INFO:Initializing create_model()
2024-09-11 11:11:22,858:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002B573BE74D0>, estimator=ada, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002B573DC7490>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-11 11:11:22,858:INFO:Checking exceptions
2024-09-11 11:11:22,858:INFO:Importing libraries
2024-09-11 11:11:22,858:INFO:Copying training dataset
2024-09-11 11:11:22,863:INFO:Defining folds
2024-09-11 11:11:22,863:INFO:Declaring metric variables
2024-09-11 11:11:22,864:INFO:Importing untrained model
2024-09-11 11:11:22,864:INFO:Ada Boost Classifier Imported successfully
2024-09-11 11:11:22,866:INFO:Starting cross validation
2024-09-11 11:11:22,867:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-11 11:11:23,048:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-11 11:11:23,049:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-11 11:11:23,071:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-11 11:11:23,083:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-11 11:11:23,098:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-11 11:11:23,100:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-11 11:11:23,101:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-11 11:11:23,101:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-11 11:11:23,126:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-11 11:11:23,132:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-11 11:11:23,789:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 11:11:23,798:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:23,810:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 11:11:23,816:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:23,816:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:23,832:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:23,834:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:23,834:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 11:11:23,840:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 11:11:23,842:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 11:11:23,843:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:23,845:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 11:11:23,849:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:23,850:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:23,852:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 11:11:23,852:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 11:11:23,853:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:23,853:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:23,856:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 11:11:23,861:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:23,861:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:23,861:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:23,863:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:23,865:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:23,867:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 11:11:23,870:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:23,871:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 11:11:23,872:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:23,877:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 11:11:23,879:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:23,880:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 11:11:23,881:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:23,881:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:23,882:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:23,882:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:23,884:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:23,887:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 11:11:23,888:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:23,888:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:23,896:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:23,896:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:23,898:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:23,902:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:23,910:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:23,923:INFO:Calculating mean and std
2024-09-11 11:11:23,924:INFO:Creating metrics dataframe
2024-09-11 11:11:23,926:INFO:Uploading results into container
2024-09-11 11:11:23,927:INFO:Uploading model into container now
2024-09-11 11:11:23,928:INFO:_master_model_container: 9
2024-09-11 11:11:23,928:INFO:_display_container: 2
2024-09-11 11:11:23,928:INFO:AdaBoostClassifier(algorithm='SAMME.R', estimator=None, learning_rate=1.0,
                   n_estimators=50, random_state=3218)
2024-09-11 11:11:23,928:INFO:create_model() successfully completed......................................
2024-09-11 11:11:23,986:INFO:SubProcess create_model() end ==================================
2024-09-11 11:11:23,986:INFO:Creating metrics dataframe
2024-09-11 11:11:23,990:INFO:Initializing Gradient Boosting Classifier
2024-09-11 11:11:23,990:INFO:Total runtime is 0.4987023154894511 minutes
2024-09-11 11:11:23,990:INFO:SubProcess create_model() called ==================================
2024-09-11 11:11:23,992:INFO:Initializing create_model()
2024-09-11 11:11:23,992:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002B573BE74D0>, estimator=gbc, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002B573DC7490>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-11 11:11:23,992:INFO:Checking exceptions
2024-09-11 11:11:23,992:INFO:Importing libraries
2024-09-11 11:11:23,992:INFO:Copying training dataset
2024-09-11 11:11:23,998:INFO:Defining folds
2024-09-11 11:11:23,998:INFO:Declaring metric variables
2024-09-11 11:11:23,998:INFO:Importing untrained model
2024-09-11 11:11:23,999:INFO:Gradient Boosting Classifier Imported successfully
2024-09-11 11:11:24,000:INFO:Starting cross validation
2024-09-11 11:11:24,002:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-11 11:11:25,625:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 11:11:25,643:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:25,646:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 11:11:25,658:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:25,664:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:25,672:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 11:11:25,677:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:25,683:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:25,684:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:25,685:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 11:11:25,695:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:25,696:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:25,700:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 11:11:25,702:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:25,704:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 11:11:25,710:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 11:11:25,712:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:25,714:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 11:11:25,715:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:25,715:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:25,720:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:25,722:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 11:11:25,725:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:25,730:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 11:11:25,731:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:25,734:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:25,736:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:25,740:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:25,741:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:25,742:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 11:11:25,751:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:25,753:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:25,753:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:25,758:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:25,762:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:25,767:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 11:11:25,767:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 11:11:25,770:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:25,775:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:25,778:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:25,780:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 11:11:25,791:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:25,796:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:25,812:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:25,833:INFO:Calculating mean and std
2024-09-11 11:11:25,836:INFO:Creating metrics dataframe
2024-09-11 11:11:25,847:INFO:Uploading results into container
2024-09-11 11:11:25,849:INFO:Uploading model into container now
2024-09-11 11:11:25,852:INFO:_master_model_container: 10
2024-09-11 11:11:25,852:INFO:_display_container: 2
2024-09-11 11:11:25,856:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=3218, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2024-09-11 11:11:25,857:INFO:create_model() successfully completed......................................
2024-09-11 11:11:25,960:INFO:SubProcess create_model() end ==================================
2024-09-11 11:11:25,960:INFO:Creating metrics dataframe
2024-09-11 11:11:25,964:INFO:Initializing Linear Discriminant Analysis
2024-09-11 11:11:25,965:INFO:Total runtime is 0.5316059986750284 minutes
2024-09-11 11:11:25,965:INFO:SubProcess create_model() called ==================================
2024-09-11 11:11:25,965:INFO:Initializing create_model()
2024-09-11 11:11:25,965:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002B573BE74D0>, estimator=lda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002B573DC7490>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-11 11:11:25,965:INFO:Checking exceptions
2024-09-11 11:11:25,966:INFO:Importing libraries
2024-09-11 11:11:25,966:INFO:Copying training dataset
2024-09-11 11:11:25,971:INFO:Defining folds
2024-09-11 11:11:25,971:INFO:Declaring metric variables
2024-09-11 11:11:25,972:INFO:Importing untrained model
2024-09-11 11:11:25,972:INFO:Linear Discriminant Analysis Imported successfully
2024-09-11 11:11:25,973:INFO:Starting cross validation
2024-09-11 11:11:25,975:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-11 11:11:26,295:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 11:11:26,304:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:26,305:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 11:11:26,319:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 11:11:26,320:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:26,322:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 11:11:26,323:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:26,326:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 11:11:26,328:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:26,332:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:26,336:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:26,342:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:26,343:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 11:11:26,346:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:26,349:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:26,351:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:26,351:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 11:11:26,352:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:26,356:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 11:11:26,358:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:26,360:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 11:11:26,361:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:26,367:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:26,367:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:26,368:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 11:11:26,368:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:26,368:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:26,370:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:26,372:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:26,379:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:26,381:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 11:11:26,383:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 11:11:26,385:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 11:11:26,385:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:26,389:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:26,393:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:26,394:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:26,396:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:26,401:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:26,402:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:26,410:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:26,415:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 11:11:26,420:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:26,429:INFO:Calculating mean and std
2024-09-11 11:11:26,430:INFO:Creating metrics dataframe
2024-09-11 11:11:26,433:INFO:Uploading results into container
2024-09-11 11:11:26,433:INFO:Uploading model into container now
2024-09-11 11:11:26,434:INFO:_master_model_container: 11
2024-09-11 11:11:26,434:INFO:_display_container: 2
2024-09-11 11:11:26,435:INFO:LinearDiscriminantAnalysis(covariance_estimator=None, n_components=None,
                           priors=None, shrinkage=None, solver='svd',
                           store_covariance=False, tol=0.0001)
2024-09-11 11:11:26,435:INFO:create_model() successfully completed......................................
2024-09-11 11:11:26,499:INFO:SubProcess create_model() end ==================================
2024-09-11 11:11:26,499:INFO:Creating metrics dataframe
2024-09-11 11:11:26,504:INFO:Initializing Extra Trees Classifier
2024-09-11 11:11:26,504:INFO:Total runtime is 0.5405993461608887 minutes
2024-09-11 11:11:26,504:INFO:SubProcess create_model() called ==================================
2024-09-11 11:11:26,505:INFO:Initializing create_model()
2024-09-11 11:11:26,505:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002B573BE74D0>, estimator=et, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002B573DC7490>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-11 11:11:26,505:INFO:Checking exceptions
2024-09-11 11:11:26,505:INFO:Importing libraries
2024-09-11 11:11:26,505:INFO:Copying training dataset
2024-09-11 11:11:26,511:INFO:Defining folds
2024-09-11 11:11:26,512:INFO:Declaring metric variables
2024-09-11 11:11:26,512:INFO:Importing untrained model
2024-09-11 11:11:26,513:INFO:Extra Trees Classifier Imported successfully
2024-09-11 11:11:26,513:INFO:Starting cross validation
2024-09-11 11:11:26,515:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-11 11:11:28,172:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:28,198:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:28,216:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:28,221:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:28,225:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:28,239:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:28,246:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:28,249:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:28,253:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 11:11:28,258:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:28,265:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:28,266:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 11:11:28,275:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:28,276:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:28,277:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:28,299:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:28,308:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 11:11:28,322:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:28,345:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:28,360:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:28,368:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:28,374:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:28,379:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:28,388:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:28,403:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:28,411:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:28,417:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:28,417:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:28,419:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:28,423:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:28,424:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:28,425:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 11:11:28,430:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:28,432:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:28,448:INFO:Calculating mean and std
2024-09-11 11:11:28,449:INFO:Creating metrics dataframe
2024-09-11 11:11:28,453:INFO:Uploading results into container
2024-09-11 11:11:28,453:INFO:Uploading model into container now
2024-09-11 11:11:28,454:INFO:_master_model_container: 12
2024-09-11 11:11:28,454:INFO:_display_container: 2
2024-09-11 11:11:28,455:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=3218, verbose=0,
                     warm_start=False)
2024-09-11 11:11:28,455:INFO:create_model() successfully completed......................................
2024-09-11 11:11:28,524:INFO:SubProcess create_model() end ==================================
2024-09-11 11:11:28,524:INFO:Creating metrics dataframe
2024-09-11 11:11:28,530:INFO:Initializing Light Gradient Boosting Machine
2024-09-11 11:11:28,530:INFO:Total runtime is 0.5743575930595398 minutes
2024-09-11 11:11:28,530:INFO:SubProcess create_model() called ==================================
2024-09-11 11:11:28,530:INFO:Initializing create_model()
2024-09-11 11:11:28,531:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002B573BE74D0>, estimator=lightgbm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002B573DC7490>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-11 11:11:28,531:INFO:Checking exceptions
2024-09-11 11:11:28,531:INFO:Importing libraries
2024-09-11 11:11:28,531:INFO:Copying training dataset
2024-09-11 11:11:28,537:INFO:Defining folds
2024-09-11 11:11:28,537:INFO:Declaring metric variables
2024-09-11 11:11:28,538:INFO:Importing untrained model
2024-09-11 11:11:28,539:INFO:Light Gradient Boosting Machine Imported successfully
2024-09-11 11:11:28,539:INFO:Starting cross validation
2024-09-11 11:11:28,541:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-11 11:11:33,737:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:33,763:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:33,777:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:33,786:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:33,791:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 11:11:33,800:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:33,848:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:33,863:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:33,887:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:33,898:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:33,914:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:33,942:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:33,986:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:34,001:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:34,020:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:34,221:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:34,240:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:34,258:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:34,271:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:34,286:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:34,300:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:34,385:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:34,409:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:34,416:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 11:11:34,434:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:34,499:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:34,512:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:34,516:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 11:11:34,523:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:34,531:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:34,545:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:34,549:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 11:11:34,556:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:34,582:INFO:Calculating mean and std
2024-09-11 11:11:34,585:INFO:Creating metrics dataframe
2024-09-11 11:11:34,598:INFO:Uploading results into container
2024-09-11 11:11:34,600:INFO:Uploading model into container now
2024-09-11 11:11:34,601:INFO:_master_model_container: 13
2024-09-11 11:11:34,601:INFO:_display_container: 2
2024-09-11 11:11:34,606:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=3218, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-09-11 11:11:34,607:INFO:create_model() successfully completed......................................
2024-09-11 11:11:34,680:INFO:SubProcess create_model() end ==================================
2024-09-11 11:11:34,680:INFO:Creating metrics dataframe
2024-09-11 11:11:34,684:INFO:Initializing Dummy Classifier
2024-09-11 11:11:34,684:INFO:Total runtime is 0.676923127969106 minutes
2024-09-11 11:11:34,685:INFO:SubProcess create_model() called ==================================
2024-09-11 11:11:34,685:INFO:Initializing create_model()
2024-09-11 11:11:34,685:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002B573BE74D0>, estimator=dummy, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002B573DC7490>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-11 11:11:34,685:INFO:Checking exceptions
2024-09-11 11:11:34,685:INFO:Importing libraries
2024-09-11 11:11:34,685:INFO:Copying training dataset
2024-09-11 11:11:34,691:INFO:Defining folds
2024-09-11 11:11:34,691:INFO:Declaring metric variables
2024-09-11 11:11:34,692:INFO:Importing untrained model
2024-09-11 11:11:34,692:INFO:Dummy Classifier Imported successfully
2024-09-11 11:11:34,692:INFO:Starting cross validation
2024-09-11 11:11:34,694:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-11 11:11:34,983:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:34,997:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:35,002:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:35,005:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 11:11:35,010:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:35,014:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:35,015:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:35,015:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:35,018:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:35,025:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:35,026:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:35,028:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:35,034:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 11:11:35,039:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:35,041:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 11:11:35,043:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:35,046:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:35,046:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:35,046:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:35,046:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:35,049:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 11:11:35,053:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 11:11:35,054:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:35,056:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 11:11:35,056:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 11:11:35,056:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:35,061:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:35,062:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:35,065:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:35,065:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:35,065:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:35,071:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 11:11:35,072:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:35,072:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:35,079:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 11:11:35,080:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:35,083:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:35,084:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:35,086:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 11:11:35,092:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:11:35,103:INFO:Calculating mean and std
2024-09-11 11:11:35,104:INFO:Creating metrics dataframe
2024-09-11 11:11:35,107:INFO:Uploading results into container
2024-09-11 11:11:35,108:INFO:Uploading model into container now
2024-09-11 11:11:35,108:INFO:_master_model_container: 14
2024-09-11 11:11:35,108:INFO:_display_container: 2
2024-09-11 11:11:35,109:INFO:DummyClassifier(constant=None, random_state=3218, strategy='prior')
2024-09-11 11:11:35,109:INFO:create_model() successfully completed......................................
2024-09-11 11:11:35,168:INFO:SubProcess create_model() end ==================================
2024-09-11 11:11:35,169:INFO:Creating metrics dataframe
2024-09-11 11:11:35,174:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:339: FutureWarning: Styler.applymap has been deprecated. Use Styler.map instead.
  .applymap(highlight_cols, subset=["TT (Sec)"])

2024-09-11 11:11:35,177:INFO:Initializing create_model()
2024-09-11 11:11:35,177:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002B573BE74D0>, estimator=ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=3218, verbose=0,
                     warm_start=False), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-11 11:11:35,177:INFO:Checking exceptions
2024-09-11 11:11:35,179:INFO:Importing libraries
2024-09-11 11:11:35,179:INFO:Copying training dataset
2024-09-11 11:11:35,186:INFO:Defining folds
2024-09-11 11:11:35,186:INFO:Declaring metric variables
2024-09-11 11:11:35,187:INFO:Importing untrained model
2024-09-11 11:11:35,187:INFO:Declaring custom model
2024-09-11 11:11:35,189:INFO:Extra Trees Classifier Imported successfully
2024-09-11 11:11:35,191:INFO:Cross validation set to False
2024-09-11 11:11:35,191:INFO:Fitting Model
2024-09-11 11:11:35,446:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=3218, verbose=0,
                     warm_start=False)
2024-09-11 11:11:35,448:INFO:create_model() successfully completed......................................
2024-09-11 11:11:35,540:INFO:_master_model_container: 14
2024-09-11 11:11:35,540:INFO:_display_container: 2
2024-09-11 11:11:35,542:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=3218, verbose=0,
                     warm_start=False)
2024-09-11 11:11:35,542:INFO:compare_models() successfully completed......................................
2024-09-11 11:11:35,543:INFO:Initializing plot_model()
2024-09-11 11:11:35,543:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002B573BE74D0>, estimator=ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=3218, verbose=0,
                     warm_start=False), plot=confusion_matrix, scale=1, save=False, fold=None, fit_kwargs=None, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=True, system=True, display=None, display_format=None)
2024-09-11 11:11:35,543:INFO:Checking exceptions
2024-09-11 11:11:35,601:INFO:Preloading libraries
2024-09-11 11:11:35,625:INFO:Copying training dataset
2024-09-11 11:11:35,626:INFO:Plot type: confusion_matrix
2024-09-11 11:11:36,226:INFO:Fitting Model
2024-09-11 11:11:36,227:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\base.py:493: UserWarning: X does not have valid feature names, but ExtraTreesClassifier was fitted with feature names
  warnings.warn(

2024-09-11 11:11:36,228:INFO:Scoring test/hold-out set
2024-09-11 11:11:59,665:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-11 11:11:59,666:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-11 11:11:59,666:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-11 11:11:59,666:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-11 11:12:01,410:INFO:PyCaret ClassificationExperiment
2024-09-11 11:12:01,411:INFO:Logging name: clf-default-name
2024-09-11 11:12:01,411:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2024-09-11 11:12:01,411:INFO:version 3.3.2
2024-09-11 11:12:01,411:INFO:Initializing setup()
2024-09-11 11:12:01,411:INFO:self.USI: 81e5
2024-09-11 11:12:01,411:INFO:self._variable_keys: {'n_jobs_param', 'log_plots_param', 'pipeline', 'gpu_n_jobs_param', 'fold_groups_param', 'fold_shuffle_param', 'is_multiclass', 'y', 'memory', 'X_test', 'exp_name_log', 'seed', 'USI', 'target_param', 'data', 'X_train', 'y_test', '_available_plots', 'X', 'idx', 'fix_imbalance', 'gpu_param', 'logging_param', 'fold_generator', 'html_param', 'y_train', '_ml_usecase', 'exp_id'}
2024-09-11 11:12:01,411:INFO:Checking environment
2024-09-11 11:12:01,411:INFO:python_version: 3.11.8
2024-09-11 11:12:01,412:INFO:python_build: ('tags/v3.11.8:db85d51', 'Feb  6 2024 22:03:32')
2024-09-11 11:12:01,412:INFO:machine: AMD64
2024-09-11 11:12:01,412:INFO:platform: Windows-10-10.0.22631-SP0
2024-09-11 11:12:01,415:INFO:Memory: svmem(total=17096892416, available=6184730624, percent=63.8, used=10912161792, free=6184730624)
2024-09-11 11:12:01,415:INFO:Physical Core: 6
2024-09-11 11:12:01,415:INFO:Logical Core: 12
2024-09-11 11:12:01,415:INFO:Checking libraries
2024-09-11 11:12:01,415:INFO:System:
2024-09-11 11:12:01,415:INFO:    python: 3.11.8 (tags/v3.11.8:db85d51, Feb  6 2024, 22:03:32) [MSC v.1937 64 bit (AMD64)]
2024-09-11 11:12:01,415:INFO:executable: h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Scripts\python.exe
2024-09-11 11:12:01,415:INFO:   machine: Windows-10-10.0.22631-SP0
2024-09-11 11:12:01,415:INFO:PyCaret required dependencies:
2024-09-11 11:12:01,458:INFO:                 pip: 24.2
2024-09-11 11:12:01,458:INFO:          setuptools: 65.5.0
2024-09-11 11:12:01,458:INFO:             pycaret: 3.3.2
2024-09-11 11:12:01,459:INFO:             IPython: 8.27.0
2024-09-11 11:12:01,459:INFO:          ipywidgets: 8.1.5
2024-09-11 11:12:01,459:INFO:                tqdm: 4.66.5
2024-09-11 11:12:01,459:INFO:               numpy: 1.26.4
2024-09-11 11:12:01,459:INFO:              pandas: 2.1.4
2024-09-11 11:12:01,459:INFO:              jinja2: 3.1.4
2024-09-11 11:12:01,459:INFO:               scipy: 1.11.4
2024-09-11 11:12:01,459:INFO:              joblib: 1.3.2
2024-09-11 11:12:01,459:INFO:             sklearn: 1.4.2
2024-09-11 11:12:01,459:INFO:                pyod: 2.0.2
2024-09-11 11:12:01,459:INFO:            imblearn: 0.12.3
2024-09-11 11:12:01,459:INFO:   category_encoders: 2.6.3
2024-09-11 11:12:01,460:INFO:            lightgbm: 4.5.0
2024-09-11 11:12:01,460:INFO:               numba: 0.60.0
2024-09-11 11:12:01,460:INFO:            requests: 2.32.3
2024-09-11 11:12:01,460:INFO:          matplotlib: 3.7.5
2024-09-11 11:12:01,460:INFO:          scikitplot: 0.3.7
2024-09-11 11:12:01,460:INFO:         yellowbrick: 1.5
2024-09-11 11:12:01,460:INFO:              plotly: 5.24.0
2024-09-11 11:12:01,460:INFO:    plotly-resampler: Not installed
2024-09-11 11:12:01,460:INFO:             kaleido: 0.2.1
2024-09-11 11:12:01,460:INFO:           schemdraw: 0.15
2024-09-11 11:12:01,460:INFO:         statsmodels: 0.14.2
2024-09-11 11:12:01,460:INFO:              sktime: 0.26.0
2024-09-11 11:12:01,460:INFO:               tbats: 1.1.3
2024-09-11 11:12:01,461:INFO:            pmdarima: 2.0.4
2024-09-11 11:12:01,461:INFO:              psutil: 6.0.0
2024-09-11 11:12:01,461:INFO:          markupsafe: 2.1.5
2024-09-11 11:12:01,461:INFO:             pickle5: Not installed
2024-09-11 11:12:01,461:INFO:         cloudpickle: 3.0.0
2024-09-11 11:12:01,461:INFO:         deprecation: 2.1.0
2024-09-11 11:12:01,461:INFO:              xxhash: 3.5.0
2024-09-11 11:12:01,461:INFO:           wurlitzer: Not installed
2024-09-11 11:12:01,461:INFO:PyCaret optional dependencies:
2024-09-11 11:12:01,492:INFO:                shap: Not installed
2024-09-11 11:12:01,492:INFO:           interpret: Not installed
2024-09-11 11:12:01,492:INFO:                umap: Not installed
2024-09-11 11:12:01,492:INFO:     ydata_profiling: Not installed
2024-09-11 11:12:01,492:INFO:  explainerdashboard: Not installed
2024-09-11 11:12:01,493:INFO:             autoviz: Not installed
2024-09-11 11:12:01,493:INFO:           fairlearn: Not installed
2024-09-11 11:12:01,493:INFO:          deepchecks: Not installed
2024-09-11 11:12:01,493:INFO:             xgboost: Not installed
2024-09-11 11:12:01,493:INFO:            catboost: Not installed
2024-09-11 11:12:01,493:INFO:              kmodes: Not installed
2024-09-11 11:12:01,493:INFO:             mlxtend: Not installed
2024-09-11 11:12:01,493:INFO:       statsforecast: Not installed
2024-09-11 11:12:01,493:INFO:        tune_sklearn: Not installed
2024-09-11 11:12:01,493:INFO:                 ray: Not installed
2024-09-11 11:12:01,493:INFO:            hyperopt: Not installed
2024-09-11 11:12:01,493:INFO:              optuna: Not installed
2024-09-11 11:12:01,494:INFO:               skopt: Not installed
2024-09-11 11:12:01,494:INFO:              mlflow: Not installed
2024-09-11 11:12:01,494:INFO:              gradio: Not installed
2024-09-11 11:12:01,494:INFO:             fastapi: Not installed
2024-09-11 11:12:01,494:INFO:             uvicorn: Not installed
2024-09-11 11:12:01,494:INFO:              m2cgen: Not installed
2024-09-11 11:12:01,494:INFO:           evidently: Not installed
2024-09-11 11:12:01,494:INFO:               fugue: Not installed
2024-09-11 11:12:01,494:INFO:           streamlit: Not installed
2024-09-11 11:12:01,494:INFO:             prophet: Not installed
2024-09-11 11:12:01,494:INFO:None
2024-09-11 11:12:01,494:INFO:Set up data.
2024-09-11 11:12:01,506:INFO:Set up folding strategy.
2024-09-11 11:12:01,506:INFO:Set up train/test split.
2024-09-11 11:12:01,518:INFO:Set up index.
2024-09-11 11:12:01,518:INFO:Assigning column types.
2024-09-11 11:12:01,523:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2024-09-11 11:12:01,566:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-09-11 11:12:01,572:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-09-11 11:12:01,612:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-11 11:12:01,612:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-11 11:12:01,655:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-09-11 11:12:01,656:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-09-11 11:12:01,683:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-11 11:12:01,684:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-11 11:12:01,684:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2024-09-11 11:12:01,728:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-09-11 11:12:01,754:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-11 11:12:01,755:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-11 11:12:01,798:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-09-11 11:12:01,840:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-11 11:12:01,840:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-11 11:12:01,841:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2024-09-11 11:12:01,920:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-11 11:12:01,920:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-11 11:12:01,990:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-11 11:12:01,990:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-11 11:12:01,994:INFO:Preparing preprocessing pipeline...
2024-09-11 11:12:01,995:INFO:Set up label encoding.
2024-09-11 11:12:01,995:INFO:Set up simple imputation.
2024-09-11 11:12:01,998:INFO:Set up encoding of categorical features.
2024-09-11 11:12:02,129:INFO:Finished creating preprocessing pipeline.
2024-09-11 11:12:02,145:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\ipkov\AppData\Local\Temp\joblib),
         steps=[('label_encoding',
                 TransformerWrapperWithInverse(exclude=None, include=None,
                                               transformer=LabelEncoder())),
                ('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['MPG', 'Cylinders', 'Displacement',
                                             'Horsepower', 'Weight',
                                             'Acceleration', 'Model'],
                                    transformer=SimpleImp...
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='most_frequent'))),
                ('rest_encoding',
                 TransformerWrapper(exclude=None, include=['Car'],
                                    transformer=TargetEncoder(cols=['Car'],
                                                              drop_invariant=False,
                                                              handle_missing='return_nan',
                                                              handle_unknown='value',
                                                              hierarchy=None,
                                                              min_samples_leaf=20,
                                                              return_df=True,
                                                              smoothing=10,
                                                              verbose=0)))],
         verbose=False)
2024-09-11 11:12:02,145:INFO:Creating final display dataframe.
2024-09-11 11:12:02,552:INFO:Setup _display_container:                     Description                       Value
0                    Session id                        2210
1                        Target                      Origin
2                   Target type                  Multiclass
3                Target mapping  Europe: 0, Japan: 1, US: 2
4           Original data shape                    (406, 9)
5        Transformed data shape                    (406, 9)
6   Transformed train set shape                    (284, 9)
7    Transformed test set shape                    (122, 9)
8              Numeric features                           7
9          Categorical features                           1
10                   Preprocess                        True
11              Imputation type                      simple
12           Numeric imputation                        mean
13       Categorical imputation                        mode
14     Maximum one-hot encoding                          25
15              Encoding method                        None
16               Fold Generator             StratifiedKFold
17                  Fold Number                          10
18                     CPU Jobs                          -1
19                      Use GPU                       False
20               Log Experiment                       False
21              Experiment Name            clf-default-name
22                          USI                        81e5
2024-09-11 11:12:02,633:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-11 11:12:02,634:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-11 11:12:02,705:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-11 11:12:02,706:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-11 11:12:02,708:INFO:setup() successfully completed in 1.3s...............
2024-09-11 11:12:02,709:INFO:Initializing compare_models()
2024-09-11 11:12:02,709:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000208E05773D0>, include=None, exclude=None, fold=None, round=4, cross_validation=True, sort=Accuracy, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x00000208E05773D0>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'Accuracy', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>})
2024-09-11 11:12:02,709:INFO:Checking exceptions
2024-09-11 11:12:02,714:INFO:Preparing display monitor
2024-09-11 11:12:02,720:INFO:Initializing Logistic Regression
2024-09-11 11:12:02,720:INFO:Total runtime is 0.0 minutes
2024-09-11 11:12:02,721:INFO:SubProcess create_model() called ==================================
2024-09-11 11:12:02,721:INFO:Initializing create_model()
2024-09-11 11:12:02,721:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000208E05773D0>, estimator=lr, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000208E0A77490>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-11 11:12:02,721:INFO:Checking exceptions
2024-09-11 11:12:02,721:INFO:Importing libraries
2024-09-11 11:12:02,721:INFO:Copying training dataset
2024-09-11 11:12:02,728:INFO:Defining folds
2024-09-11 11:12:02,728:INFO:Declaring metric variables
2024-09-11 11:12:02,728:INFO:Importing untrained model
2024-09-11 11:12:02,729:INFO:Logistic Regression Imported successfully
2024-09-11 11:12:02,729:INFO:Starting cross validation
2024-09-11 11:12:02,732:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-11 11:12:20,387:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-11 11:12:20,457:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-11 11:12:20,519:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 11:12:20,525:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:20,535:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:20,549:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:20,570:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 11:12:20,587:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:20,605:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:20,618:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:20,627:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-11 11:12:20,632:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-11 11:12:20,680:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-11 11:12:20,722:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 11:12:20,726:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 11:12:20,732:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:20,738:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:20,749:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:20,755:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:20,759:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:20,763:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:20,769:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 11:12:20,778:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:20,793:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:20,796:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-11 11:12:20,804:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:20,823:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-11 11:12:20,867:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-11 11:12:20,868:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 11:12:20,876:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:20,878:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-11 11:12:20,885:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:20,887:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 11:12:20,892:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:20,894:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:20,897:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-11 11:12:20,897:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:20,903:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:20,920:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 11:12:20,925:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:20,927:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 11:12:20,935:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:20,935:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:20,944:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:20,944:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:20,944:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 11:12:20,949:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:20,951:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:20,959:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:20,964:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:20,983:INFO:Calculating mean and std
2024-09-11 11:12:20,985:INFO:Creating metrics dataframe
2024-09-11 11:12:20,991:INFO:Uploading results into container
2024-09-11 11:12:20,993:INFO:Uploading model into container now
2024-09-11 11:12:20,995:INFO:_master_model_container: 1
2024-09-11 11:12:20,995:INFO:_display_container: 2
2024-09-11 11:12:20,996:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=2210, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2024-09-11 11:12:20,996:INFO:create_model() successfully completed......................................
2024-09-11 11:12:21,101:INFO:SubProcess create_model() end ==================================
2024-09-11 11:12:21,101:INFO:Creating metrics dataframe
2024-09-11 11:12:21,105:INFO:Initializing K Neighbors Classifier
2024-09-11 11:12:21,105:INFO:Total runtime is 0.30640552441279095 minutes
2024-09-11 11:12:21,105:INFO:SubProcess create_model() called ==================================
2024-09-11 11:12:21,106:INFO:Initializing create_model()
2024-09-11 11:12:21,106:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000208E05773D0>, estimator=knn, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000208E0A77490>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-11 11:12:21,106:INFO:Checking exceptions
2024-09-11 11:12:21,106:INFO:Importing libraries
2024-09-11 11:12:21,106:INFO:Copying training dataset
2024-09-11 11:12:21,112:INFO:Defining folds
2024-09-11 11:12:21,112:INFO:Declaring metric variables
2024-09-11 11:12:21,112:INFO:Importing untrained model
2024-09-11 11:12:21,113:INFO:K Neighbors Classifier Imported successfully
2024-09-11 11:12:21,113:INFO:Starting cross validation
2024-09-11 11:12:21,115:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-11 11:12:21,835:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:21,841:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:21,848:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:21,860:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:21,866:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:21,866:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:21,870:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:21,872:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:21,879:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:21,885:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:21,886:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:21,888:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:21,890:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:21,895:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:21,903:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:21,903:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:21,904:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:21,913:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:21,915:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:21,919:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:21,924:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:21,924:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:21,945:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:21,950:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:25,962:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:25,963:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:25,971:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:25,971:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:25,986:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:25,986:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:26,014:INFO:Calculating mean and std
2024-09-11 11:12:26,016:INFO:Creating metrics dataframe
2024-09-11 11:12:26,022:INFO:Uploading results into container
2024-09-11 11:12:26,023:INFO:Uploading model into container now
2024-09-11 11:12:26,024:INFO:_master_model_container: 2
2024-09-11 11:12:26,024:INFO:_display_container: 2
2024-09-11 11:12:26,025:INFO:KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform')
2024-09-11 11:12:26,025:INFO:create_model() successfully completed......................................
2024-09-11 11:12:26,122:INFO:SubProcess create_model() end ==================================
2024-09-11 11:12:26,122:INFO:Creating metrics dataframe
2024-09-11 11:12:26,126:INFO:Initializing Naive Bayes
2024-09-11 11:12:26,126:INFO:Total runtime is 0.39009138345718386 minutes
2024-09-11 11:12:26,126:INFO:SubProcess create_model() called ==================================
2024-09-11 11:12:26,127:INFO:Initializing create_model()
2024-09-11 11:12:26,127:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000208E05773D0>, estimator=nb, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000208E0A77490>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-11 11:12:26,127:INFO:Checking exceptions
2024-09-11 11:12:26,127:INFO:Importing libraries
2024-09-11 11:12:26,127:INFO:Copying training dataset
2024-09-11 11:12:26,133:INFO:Defining folds
2024-09-11 11:12:26,133:INFO:Declaring metric variables
2024-09-11 11:12:26,133:INFO:Importing untrained model
2024-09-11 11:12:26,134:INFO:Naive Bayes Imported successfully
2024-09-11 11:12:26,134:INFO:Starting cross validation
2024-09-11 11:12:26,137:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-11 11:12:26,474:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:26,476:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:26,490:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:26,490:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:26,492:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:26,497:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:26,497:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 11:12:26,498:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:26,499:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:26,500:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:26,506:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:26,506:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:26,506:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:26,510:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:26,511:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:26,513:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:26,514:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:26,515:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:26,518:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:26,519:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:26,521:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:26,521:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 11:12:26,525:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 11:12:26,527:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:26,528:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:26,530:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:26,531:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:26,531:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:26,532:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:26,533:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:26,539:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:26,539:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:26,542:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:26,562:INFO:Calculating mean and std
2024-09-11 11:12:26,563:INFO:Creating metrics dataframe
2024-09-11 11:12:26,566:INFO:Uploading results into container
2024-09-11 11:12:26,566:INFO:Uploading model into container now
2024-09-11 11:12:26,567:INFO:_master_model_container: 3
2024-09-11 11:12:26,567:INFO:_display_container: 2
2024-09-11 11:12:26,568:INFO:GaussianNB(priors=None, var_smoothing=1e-09)
2024-09-11 11:12:26,568:INFO:create_model() successfully completed......................................
2024-09-11 11:12:26,626:INFO:SubProcess create_model() end ==================================
2024-09-11 11:12:26,626:INFO:Creating metrics dataframe
2024-09-11 11:12:26,631:INFO:Initializing Decision Tree Classifier
2024-09-11 11:12:26,631:INFO:Total runtime is 0.3985036889712016 minutes
2024-09-11 11:12:26,631:INFO:SubProcess create_model() called ==================================
2024-09-11 11:12:26,632:INFO:Initializing create_model()
2024-09-11 11:12:26,632:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000208E05773D0>, estimator=dt, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000208E0A77490>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-11 11:12:26,632:INFO:Checking exceptions
2024-09-11 11:12:26,632:INFO:Importing libraries
2024-09-11 11:12:26,632:INFO:Copying training dataset
2024-09-11 11:12:26,637:INFO:Defining folds
2024-09-11 11:12:26,637:INFO:Declaring metric variables
2024-09-11 11:12:26,637:INFO:Importing untrained model
2024-09-11 11:12:26,638:INFO:Decision Tree Classifier Imported successfully
2024-09-11 11:12:26,639:INFO:Starting cross validation
2024-09-11 11:12:26,641:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-11 11:12:26,939:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:26,944:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:26,945:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:26,951:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:26,955:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:26,956:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:26,957:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:26,957:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:26,961:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:26,962:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 11:12:26,966:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:26,971:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:26,972:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:26,973:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:26,973:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:26,975:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:26,978:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:26,979:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:26,979:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 11:12:26,981:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:26,983:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:26,987:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:26,989:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:26,992:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:26,995:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:26,996:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:26,997:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:26,999:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 11:12:27,002:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:27,004:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:27,004:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:27,008:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:27,012:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:27,023:INFO:Calculating mean and std
2024-09-11 11:12:27,024:INFO:Creating metrics dataframe
2024-09-11 11:12:27,028:INFO:Uploading results into container
2024-09-11 11:12:27,028:INFO:Uploading model into container now
2024-09-11 11:12:27,029:INFO:_master_model_container: 4
2024-09-11 11:12:27,029:INFO:_display_container: 2
2024-09-11 11:12:27,030:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=2210, splitter='best')
2024-09-11 11:12:27,030:INFO:create_model() successfully completed......................................
2024-09-11 11:12:27,093:INFO:SubProcess create_model() end ==================================
2024-09-11 11:12:27,093:INFO:Creating metrics dataframe
2024-09-11 11:12:27,097:INFO:Initializing SVM - Linear Kernel
2024-09-11 11:12:27,097:INFO:Total runtime is 0.4062819004058838 minutes
2024-09-11 11:12:27,097:INFO:SubProcess create_model() called ==================================
2024-09-11 11:12:27,097:INFO:Initializing create_model()
2024-09-11 11:12:27,097:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000208E05773D0>, estimator=svm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000208E0A77490>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-11 11:12:27,098:INFO:Checking exceptions
2024-09-11 11:12:27,098:INFO:Importing libraries
2024-09-11 11:12:27,098:INFO:Copying training dataset
2024-09-11 11:12:27,103:INFO:Defining folds
2024-09-11 11:12:27,104:INFO:Declaring metric variables
2024-09-11 11:12:27,104:INFO:Importing untrained model
2024-09-11 11:12:27,105:INFO:SVM - Linear Kernel Imported successfully
2024-09-11 11:12:27,105:INFO:Starting cross validation
2024-09-11 11:12:27,108:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-11 11:12:27,526:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 11:12:27,534:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:27,546:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 11:12:27,552:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 11:12:27,555:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:27,555:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:27,556:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 11:12:27,559:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 11:12:27,562:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 11:12:27,563:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:27,565:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:27,572:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:27,574:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:27,573:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:27,578:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 11:12:27,583:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 11:12:27,584:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 11:12:27,585:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:27,589:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 11:12:27,593:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:27,594:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:27,594:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:27,600:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:27,601:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 11:12:27,601:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:27,602:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 11:12:27,602:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 11:12:27,604:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:27,611:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:27,612:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:27,615:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:27,616:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:27,622:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:27,624:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:27,624:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 11:12:27,629:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 11:12:27,631:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

at the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 11:12:27,634:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:27,634:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:27,640:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:27,641:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:27,641:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:27,645:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:27,647:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:27,648:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 11:12:27,649:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 11:12:27,653:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:27,654:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:27,672:INFO:Calculating mean and std
2024-09-11 11:12:27,673:INFO:Creating metrics dataframe
2024-09-11 11:12:27,676:INFO:Uploading results into container
2024-09-11 11:12:27,676:INFO:Uploading model into container now
2024-09-11 11:12:27,677:INFO:_master_model_container: 5
2024-09-11 11:12:27,677:INFO:_display_container: 2
2024-09-11 11:12:27,678:INFO:SGDClassifier(alpha=0.0001, average=False, class_weight=None,
              early_stopping=False, epsilon=0.1, eta0=0.001, fit_intercept=True,
              l1_ratio=0.15, learning_rate='optimal', loss='hinge',
              max_iter=1000, n_iter_no_change=5, n_jobs=-1, penalty='l2',
              power_t=0.5, random_state=2210, shuffle=True, tol=0.001,
              validation_fraction=0.1, verbose=0, warm_start=False)
2024-09-11 11:12:27,678:INFO:create_model() successfully completed......................................
2024-09-11 11:12:27,741:INFO:SubProcess create_model() end ==================================
2024-09-11 11:12:27,741:INFO:Creating metrics dataframe
2024-09-11 11:12:27,745:INFO:Initializing Ridge Classifier
2024-09-11 11:12:27,745:INFO:Total runtime is 0.41708057721455893 minutes
2024-09-11 11:12:27,745:INFO:SubProcess create_model() called ==================================
2024-09-11 11:12:27,746:INFO:Initializing create_model()
2024-09-11 11:12:27,746:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000208E05773D0>, estimator=ridge, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000208E0A77490>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-11 11:12:27,746:INFO:Checking exceptions
2024-09-11 11:12:27,746:INFO:Importing libraries
2024-09-11 11:12:27,746:INFO:Copying training dataset
2024-09-11 11:12:27,753:INFO:Defining folds
2024-09-11 11:12:27,753:INFO:Declaring metric variables
2024-09-11 11:12:27,753:INFO:Importing untrained model
2024-09-11 11:12:27,754:INFO:Ridge Classifier Imported successfully
2024-09-11 11:12:27,754:INFO:Starting cross validation
2024-09-11 11:12:27,756:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-11 11:12:28,056:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 11:12:28,059:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 11:12:28,064:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 11:12:28,065:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:28,068:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:28,072:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 11:12:28,073:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:28,075:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 11:12:28,077:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 11:12:28,081:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:28,083:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:28,084:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:28,084:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:28,086:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:28,089:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:28,096:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 11:12:28,096:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:28,097:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:28,097:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:28,097:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 11:12:28,099:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:28,102:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:28,102:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 11:12:28,103:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 11:12:28,105:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:28,106:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:28,107:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:28,107:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 11:12:28,111:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:28,112:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:28,113:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:28,118:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:28,119:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:28,122:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:28,122:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:28,127:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:28,128:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:28,131:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:28,134:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:28,136:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:28,137:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:28,153:INFO:Calculating mean and std
2024-09-11 11:12:28,154:INFO:Creating metrics dataframe
2024-09-11 11:12:28,156:INFO:Uploading results into container
2024-09-11 11:12:28,157:INFO:Uploading model into container now
2024-09-11 11:12:28,158:INFO:_master_model_container: 6
2024-09-11 11:12:28,158:INFO:_display_container: 2
2024-09-11 11:12:28,159:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=2210, solver='auto',
                tol=0.0001)
2024-09-11 11:12:28,159:INFO:create_model() successfully completed......................................
2024-09-11 11:12:28,221:INFO:SubProcess create_model() end ==================================
2024-09-11 11:12:28,222:INFO:Creating metrics dataframe
2024-09-11 11:12:28,226:INFO:Initializing Random Forest Classifier
2024-09-11 11:12:28,226:INFO:Total runtime is 0.42509532372156783 minutes
2024-09-11 11:12:28,226:INFO:SubProcess create_model() called ==================================
2024-09-11 11:12:28,226:INFO:Initializing create_model()
2024-09-11 11:12:28,227:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000208E05773D0>, estimator=rf, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000208E0A77490>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-11 11:12:28,227:INFO:Checking exceptions
2024-09-11 11:12:28,227:INFO:Importing libraries
2024-09-11 11:12:28,227:INFO:Copying training dataset
2024-09-11 11:12:28,233:INFO:Defining folds
2024-09-11 11:12:28,233:INFO:Declaring metric variables
2024-09-11 11:12:28,233:INFO:Importing untrained model
2024-09-11 11:12:28,234:INFO:Random Forest Classifier Imported successfully
2024-09-11 11:12:28,234:INFO:Starting cross validation
2024-09-11 11:12:28,236:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-11 11:12:30,196:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:30,207:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:30,213:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:30,213:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:30,216:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:30,222:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:30,223:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:30,228:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:30,229:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:30,231:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:30,234:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:30,236:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:30,240:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 11:12:30,245:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:30,245:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:30,246:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:30,253:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:30,254:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:30,256:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 11:12:30,259:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:30,259:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:30,260:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:30,262:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:30,265:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:30,270:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:30,273:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 11:12:30,276:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:30,285:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:30,286:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:30,294:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:30,365:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:30,369:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:30,375:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:30,392:INFO:Calculating mean and std
2024-09-11 11:12:30,393:INFO:Creating metrics dataframe
2024-09-11 11:12:30,396:INFO:Uploading results into container
2024-09-11 11:12:30,396:INFO:Uploading model into container now
2024-09-11 11:12:30,397:INFO:_master_model_container: 7
2024-09-11 11:12:30,398:INFO:_display_container: 2
2024-09-11 11:12:30,398:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=2210, verbose=0,
                       warm_start=False)
2024-09-11 11:12:30,399:INFO:create_model() successfully completed......................................
2024-09-11 11:12:30,464:INFO:SubProcess create_model() end ==================================
2024-09-11 11:12:30,464:INFO:Creating metrics dataframe
2024-09-11 11:12:30,468:INFO:Initializing Quadratic Discriminant Analysis
2024-09-11 11:12:30,468:INFO:Total runtime is 0.46245530843734745 minutes
2024-09-11 11:12:30,469:INFO:SubProcess create_model() called ==================================
2024-09-11 11:12:30,469:INFO:Initializing create_model()
2024-09-11 11:12:30,469:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000208E05773D0>, estimator=qda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000208E0A77490>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-11 11:12:30,469:INFO:Checking exceptions
2024-09-11 11:12:30,469:INFO:Importing libraries
2024-09-11 11:12:30,470:INFO:Copying training dataset
2024-09-11 11:12:30,475:INFO:Defining folds
2024-09-11 11:12:30,475:INFO:Declaring metric variables
2024-09-11 11:12:30,475:INFO:Importing untrained model
2024-09-11 11:12:30,476:INFO:Quadratic Discriminant Analysis Imported successfully
2024-09-11 11:12:30,476:INFO:Starting cross validation
2024-09-11 11:12:30,478:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-11 11:12:30,765:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 11:12:30,767:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 11:12:30,768:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 11:12:30,773:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:30,775:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:30,776:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:30,779:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 11:12:30,785:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:30,786:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 11:12:30,787:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 11:12:30,788:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:30,791:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:30,792:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:30,795:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:30,796:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:30,797:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 11:12:30,798:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:30,798:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 11:12:30,802:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 11:12:30,804:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:30,805:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:30,805:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:30,809:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:30,809:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:30,809:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 11:12:30,812:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:30,814:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:30,818:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:30,822:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 11:12:30,823:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 11:12:30,826:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:30,826:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:30,831:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:30,831:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:30,832:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:30,833:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:30,835:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:30,841:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:30,841:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 11:12:30,843:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:30,846:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:30,847:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:30,855:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:30,864:INFO:Calculating mean and std
2024-09-11 11:12:30,865:INFO:Creating metrics dataframe
2024-09-11 11:12:30,867:INFO:Uploading results into container
2024-09-11 11:12:30,868:INFO:Uploading model into container now
2024-09-11 11:12:30,868:INFO:_master_model_container: 8
2024-09-11 11:12:30,869:INFO:_display_container: 2
2024-09-11 11:12:30,869:INFO:QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,
                              store_covariance=False, tol=0.0001)
2024-09-11 11:12:30,869:INFO:create_model() successfully completed......................................
2024-09-11 11:12:30,933:INFO:SubProcess create_model() end ==================================
2024-09-11 11:12:30,934:INFO:Creating metrics dataframe
2024-09-11 11:12:30,937:INFO:Initializing Ada Boost Classifier
2024-09-11 11:12:30,938:INFO:Total runtime is 0.4702943404515585 minutes
2024-09-11 11:12:30,938:INFO:SubProcess create_model() called ==================================
2024-09-11 11:12:30,938:INFO:Initializing create_model()
2024-09-11 11:12:30,938:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000208E05773D0>, estimator=ada, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000208E0A77490>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-11 11:12:30,939:INFO:Checking exceptions
2024-09-11 11:12:30,939:INFO:Importing libraries
2024-09-11 11:12:30,939:INFO:Copying training dataset
2024-09-11 11:12:30,945:INFO:Defining folds
2024-09-11 11:12:30,946:INFO:Declaring metric variables
2024-09-11 11:12:30,946:INFO:Importing untrained model
2024-09-11 11:12:30,946:INFO:Ada Boost Classifier Imported successfully
2024-09-11 11:12:30,946:INFO:Starting cross validation
2024-09-11 11:12:30,950:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-11 11:12:31,122:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-11 11:12:31,131:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-11 11:12:31,133:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-11 11:12:31,135:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-11 11:12:31,141:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-11 11:12:31,150:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-11 11:12:31,159:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-11 11:12:31,167:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-11 11:12:31,167:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-11 11:12:31,171:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-11 11:12:31,828:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 11:12:31,832:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 11:12:31,838:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:31,841:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:31,844:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 11:12:31,857:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:31,861:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:31,861:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:31,863:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 11:12:31,865:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 11:12:31,869:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 11:12:31,873:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 11:12:31,876:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:31,876:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:31,876:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 11:12:31,877:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 11:12:31,878:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:31,879:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:31,879:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:31,883:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:31,885:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:31,886:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:31,887:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 11:12:31,893:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:31,893:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:31,894:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:31,895:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:31,898:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:31,899:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 11:12:31,901:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:31,902:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:31,905:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 11:12:31,905:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:31,907:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:31,908:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 11:12:31,910:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:31,910:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:31,913:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:31,913:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:31,916:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:31,917:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:31,924:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:31,926:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:31,936:INFO:Calculating mean and std
2024-09-11 11:12:31,937:INFO:Creating metrics dataframe
2024-09-11 11:12:31,940:INFO:Uploading results into container
2024-09-11 11:12:31,941:INFO:Uploading model into container now
2024-09-11 11:12:31,941:INFO:_master_model_container: 9
2024-09-11 11:12:31,941:INFO:_display_container: 2
2024-09-11 11:12:31,941:INFO:AdaBoostClassifier(algorithm='SAMME.R', estimator=None, learning_rate=1.0,
                   n_estimators=50, random_state=2210)
2024-09-11 11:12:31,941:INFO:create_model() successfully completed......................................
2024-09-11 11:12:32,003:INFO:SubProcess create_model() end ==================================
2024-09-11 11:12:32,003:INFO:Creating metrics dataframe
2024-09-11 11:12:32,008:INFO:Initializing Gradient Boosting Classifier
2024-09-11 11:12:32,008:INFO:Total runtime is 0.4881306131680807 minutes
2024-09-11 11:12:32,008:INFO:SubProcess create_model() called ==================================
2024-09-11 11:12:32,009:INFO:Initializing create_model()
2024-09-11 11:12:32,009:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000208E05773D0>, estimator=gbc, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000208E0A77490>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-11 11:12:32,009:INFO:Checking exceptions
2024-09-11 11:12:32,009:INFO:Importing libraries
2024-09-11 11:12:32,010:INFO:Copying training dataset
2024-09-11 11:12:32,015:INFO:Defining folds
2024-09-11 11:12:32,015:INFO:Declaring metric variables
2024-09-11 11:12:32,015:INFO:Importing untrained model
2024-09-11 11:12:32,016:INFO:Gradient Boosting Classifier Imported successfully
2024-09-11 11:12:32,016:INFO:Starting cross validation
2024-09-11 11:12:32,020:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-11 11:12:33,542:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 11:12:33,552:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:33,556:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 11:12:33,565:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 11:12:33,567:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:33,570:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 11:12:33,571:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 11:12:33,571:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:33,573:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 11:12:33,578:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:33,579:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 11:12:33,580:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:33,582:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:33,582:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:33,583:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:33,588:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 11:12:33,590:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 11:12:33,590:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:33,590:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:33,597:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:33,597:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:33,597:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:33,598:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:33,599:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:33,599:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:33,600:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:33,604:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 11:12:33,604:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 11:12:33,606:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:33,614:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:33,616:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:33,616:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:33,616:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:33,616:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:33,617:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:33,620:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:33,620:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 11:12:33,628:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:33,628:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 11:12:33,630:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:33,635:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:33,643:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:33,647:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:33,658:INFO:Calculating mean and std
2024-09-11 11:12:33,659:INFO:Creating metrics dataframe
2024-09-11 11:12:33,662:INFO:Uploading results into container
2024-09-11 11:12:33,662:INFO:Uploading model into container now
2024-09-11 11:12:33,663:INFO:_master_model_container: 10
2024-09-11 11:12:33,663:INFO:_display_container: 2
2024-09-11 11:12:33,664:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=2210, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2024-09-11 11:12:33,664:INFO:create_model() successfully completed......................................
2024-09-11 11:12:33,724:INFO:SubProcess create_model() end ==================================
2024-09-11 11:12:33,724:INFO:Creating metrics dataframe
2024-09-11 11:12:33,729:INFO:Initializing Linear Discriminant Analysis
2024-09-11 11:12:33,729:INFO:Total runtime is 0.5168085416158041 minutes
2024-09-11 11:12:33,730:INFO:SubProcess create_model() called ==================================
2024-09-11 11:12:33,730:INFO:Initializing create_model()
2024-09-11 11:12:33,730:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000208E05773D0>, estimator=lda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000208E0A77490>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-11 11:12:33,730:INFO:Checking exceptions
2024-09-11 11:12:33,730:INFO:Importing libraries
2024-09-11 11:12:33,730:INFO:Copying training dataset
2024-09-11 11:12:33,736:INFO:Defining folds
2024-09-11 11:12:33,736:INFO:Declaring metric variables
2024-09-11 11:12:33,736:INFO:Importing untrained model
2024-09-11 11:12:33,738:INFO:Linear Discriminant Analysis Imported successfully
2024-09-11 11:12:33,738:INFO:Starting cross validation
2024-09-11 11:12:33,740:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-11 11:12:34,016:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 11:12:34,021:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 11:12:34,026:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:34,029:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 11:12:34,032:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:34,042:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:34,044:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 11:12:34,045:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 11:12:34,045:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:34,046:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 11:12:34,050:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:34,054:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 11:12:34,055:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 11:12:34,055:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:34,056:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:34,058:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:34,062:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 11:12:34,064:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:34,066:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:34,066:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:34,066:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:34,068:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 11:12:34,072:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:34,072:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:34,076:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 11:12:34,076:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:34,077:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:34,077:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:34,080:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:34,082:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:34,083:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 11:12:34,085:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:34,087:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:34,089:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:34,091:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 11:12:34,094:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:34,094:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:34,097:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:34,102:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:34,102:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:34,103:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:34,108:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:34,112:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:34,127:INFO:Calculating mean and std
2024-09-11 11:12:34,128:INFO:Creating metrics dataframe
2024-09-11 11:12:34,131:INFO:Uploading results into container
2024-09-11 11:12:34,131:INFO:Uploading model into container now
2024-09-11 11:12:34,132:INFO:_master_model_container: 11
2024-09-11 11:12:34,132:INFO:_display_container: 2
2024-09-11 11:12:34,133:INFO:LinearDiscriminantAnalysis(covariance_estimator=None, n_components=None,
                           priors=None, shrinkage=None, solver='svd',
                           store_covariance=False, tol=0.0001)
2024-09-11 11:12:34,133:INFO:create_model() successfully completed......................................
2024-09-11 11:12:34,196:INFO:SubProcess create_model() end ==================================
2024-09-11 11:12:34,196:INFO:Creating metrics dataframe
2024-09-11 11:12:34,200:INFO:Initializing Extra Trees Classifier
2024-09-11 11:12:34,200:INFO:Total runtime is 0.5246632297833762 minutes
2024-09-11 11:12:34,200:INFO:SubProcess create_model() called ==================================
2024-09-11 11:12:34,201:INFO:Initializing create_model()
2024-09-11 11:12:34,201:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000208E05773D0>, estimator=et, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000208E0A77490>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-11 11:12:34,201:INFO:Checking exceptions
2024-09-11 11:12:34,201:INFO:Importing libraries
2024-09-11 11:12:34,201:INFO:Copying training dataset
2024-09-11 11:12:34,208:INFO:Defining folds
2024-09-11 11:12:34,208:INFO:Declaring metric variables
2024-09-11 11:12:34,209:INFO:Importing untrained model
2024-09-11 11:12:34,209:INFO:Extra Trees Classifier Imported successfully
2024-09-11 11:12:34,210:INFO:Starting cross validation
2024-09-11 11:12:34,212:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-11 11:12:35,875:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:35,886:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:35,900:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:35,901:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:35,905:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:35,911:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:35,912:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:35,915:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:35,917:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:35,924:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:35,924:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:35,927:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:35,929:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:35,933:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:35,935:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 11:12:35,938:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:35,944:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:35,945:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:35,948:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:35,956:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 11:12:35,956:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:35,957:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:35,969:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:35,975:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:35,990:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:35,996:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 11:12:36,004:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:36,016:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:36,021:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:36,027:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:36,038:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:36,043:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:36,048:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:36,062:INFO:Calculating mean and std
2024-09-11 11:12:36,063:INFO:Creating metrics dataframe
2024-09-11 11:12:36,065:INFO:Uploading results into container
2024-09-11 11:12:36,065:INFO:Uploading model into container now
2024-09-11 11:12:36,067:INFO:_master_model_container: 12
2024-09-11 11:12:36,067:INFO:_display_container: 2
2024-09-11 11:12:36,068:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=2210, verbose=0,
                     warm_start=False)
2024-09-11 11:12:36,068:INFO:create_model() successfully completed......................................
2024-09-11 11:12:36,137:INFO:SubProcess create_model() end ==================================
2024-09-11 11:12:36,138:INFO:Creating metrics dataframe
2024-09-11 11:12:36,142:INFO:Initializing Light Gradient Boosting Machine
2024-09-11 11:12:36,142:INFO:Total runtime is 0.5570207635561627 minutes
2024-09-11 11:12:36,142:INFO:SubProcess create_model() called ==================================
2024-09-11 11:12:36,142:INFO:Initializing create_model()
2024-09-11 11:12:36,143:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000208E05773D0>, estimator=lightgbm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000208E0A77490>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-11 11:12:36,143:INFO:Checking exceptions
2024-09-11 11:12:36,143:INFO:Importing libraries
2024-09-11 11:12:36,143:INFO:Copying training dataset
2024-09-11 11:12:36,149:INFO:Defining folds
2024-09-11 11:12:36,149:INFO:Declaring metric variables
2024-09-11 11:12:36,149:INFO:Importing untrained model
2024-09-11 11:12:36,150:INFO:Light Gradient Boosting Machine Imported successfully
2024-09-11 11:12:36,151:INFO:Starting cross validation
2024-09-11 11:12:36,153:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-11 11:12:40,242:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:40,261:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:40,266:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 11:12:40,278:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:40,328:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:40,343:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:40,362:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:41,078:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:41,081:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:41,099:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:41,102:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:41,104:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 11:12:41,111:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:41,111:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:41,119:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:41,124:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:41,131:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:41,156:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:41,183:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:41,196:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:41,302:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:41,305:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:41,314:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:41,317:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:41,319:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 11:12:41,325:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:41,328:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:41,338:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:41,352:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:41,367:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:41,377:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:41,386:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:41,409:INFO:Calculating mean and std
2024-09-11 11:12:41,411:INFO:Creating metrics dataframe
2024-09-11 11:12:41,420:INFO:Uploading results into container
2024-09-11 11:12:41,422:INFO:Uploading model into container now
2024-09-11 11:12:41,424:INFO:_master_model_container: 13
2024-09-11 11:12:41,425:INFO:_display_container: 2
2024-09-11 11:12:41,429:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=2210, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-09-11 11:12:41,429:INFO:create_model() successfully completed......................................
2024-09-11 11:12:41,503:INFO:SubProcess create_model() end ==================================
2024-09-11 11:12:41,503:INFO:Creating metrics dataframe
2024-09-11 11:12:41,507:INFO:Initializing Dummy Classifier
2024-09-11 11:12:41,507:INFO:Total runtime is 0.6464436451594037 minutes
2024-09-11 11:12:41,508:INFO:SubProcess create_model() called ==================================
2024-09-11 11:12:41,508:INFO:Initializing create_model()
2024-09-11 11:12:41,508:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000208E05773D0>, estimator=dummy, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000208E0A77490>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-11 11:12:41,508:INFO:Checking exceptions
2024-09-11 11:12:41,508:INFO:Importing libraries
2024-09-11 11:12:41,508:INFO:Copying training dataset
2024-09-11 11:12:41,515:INFO:Defining folds
2024-09-11 11:12:41,515:INFO:Declaring metric variables
2024-09-11 11:12:41,515:INFO:Importing untrained model
2024-09-11 11:12:41,516:INFO:Dummy Classifier Imported successfully
2024-09-11 11:12:41,516:INFO:Starting cross validation
2024-09-11 11:12:41,519:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-11 11:12:41,787:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:41,797:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:41,801:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:41,804:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:41,809:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 11:12:41,810:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:41,810:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:41,813:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:41,814:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:41,817:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:41,820:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 11:12:41,821:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:41,825:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:41,826:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:41,827:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:41,829:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 11:12:41,830:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:41,831:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:41,832:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 11:12:41,833:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 11:12:41,836:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:41,838:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:41,839:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 11:12:41,839:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:41,841:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:41,842:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:41,843:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:41,845:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:41,846:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:41,846:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:41,852:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

fier, f"{metric.capitalize()} is", len(result))

2024-09-11 11:12:41,853:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 11:12:41,856:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 11:12:41,856:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:41,857:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:41,860:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:41,860:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:41,862:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 11:12:41,866:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:41,882:INFO:Calculating mean and std
2024-09-11 11:12:41,883:INFO:Creating metrics dataframe
2024-09-11 11:12:41,885:INFO:Uploading results into container
2024-09-11 11:12:41,887:INFO:Uploading model into container now
2024-09-11 11:12:41,888:INFO:_master_model_container: 14
2024-09-11 11:12:41,888:INFO:_display_container: 2
2024-09-11 11:12:41,888:INFO:DummyClassifier(constant=None, random_state=2210, strategy='prior')
2024-09-11 11:12:41,888:INFO:create_model() successfully completed......................................
2024-09-11 11:12:41,950:INFO:SubProcess create_model() end ==================================
2024-09-11 11:12:41,950:INFO:Creating metrics dataframe
2024-09-11 11:12:41,956:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:339: FutureWarning: Styler.applymap has been deprecated. Use Styler.map instead.
  .applymap(highlight_cols, subset=["TT (Sec)"])

2024-09-11 11:12:41,959:INFO:Initializing create_model()
2024-09-11 11:12:41,959:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000208E05773D0>, estimator=ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=2210, verbose=0,
                     warm_start=False), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-11 11:12:41,959:INFO:Checking exceptions
2024-09-11 11:12:41,960:INFO:Importing libraries
2024-09-11 11:12:41,960:INFO:Copying training dataset
2024-09-11 11:12:41,966:INFO:Defining folds
2024-09-11 11:12:41,966:INFO:Declaring metric variables
2024-09-11 11:12:41,966:INFO:Importing untrained model
2024-09-11 11:12:41,966:INFO:Declaring custom model
2024-09-11 11:12:41,967:INFO:Extra Trees Classifier Imported successfully
2024-09-11 11:12:41,970:INFO:Cross validation set to False
2024-09-11 11:12:41,970:INFO:Fitting Model
2024-09-11 11:12:42,217:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=2210, verbose=0,
                     warm_start=False)
2024-09-11 11:12:42,217:INFO:create_model() successfully completed......................................
2024-09-11 11:12:42,304:INFO:_master_model_container: 14
2024-09-11 11:12:42,304:INFO:_display_container: 2
2024-09-11 11:12:42,305:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=2210, verbose=0,
                     warm_start=False)
2024-09-11 11:12:42,305:INFO:compare_models() successfully completed......................................
2024-09-11 11:12:42,306:INFO:Initializing predict_model()
2024-09-11 11:12:42,306:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000208E05773D0>, estimator=ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=2210, verbose=0,
                     warm_start=False), probability_threshold=None, encoded_labels=False, raw_score=True, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x00000208A8E77420>)
2024-09-11 11:12:42,306:INFO:Checking exceptions
2024-09-11 11:12:42,306:INFO:Preloading libraries
2024-09-11 11:12:42,306:INFO:Set up data.
2024-09-11 11:12:42,314:INFO:Set up index.
2024-09-11 11:12:42,565:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:42,571:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:42,576:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:12:42,685:INFO:Initializing save_model()
2024-09-11 11:12:42,686:INFO:save_model(model=ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=2210, verbose=0,
                     warm_start=False), model_name=my_best_pipeline, prep_pipe_=Pipeline(memory=FastMemory(location=C:\Users\ipkov\AppData\Local\Temp\joblib),
         steps=[('label_encoding',
                 TransformerWrapperWithInverse(exclude=None, include=None,
                                               transformer=LabelEncoder())),
                ('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['MPG', 'Cylinders', 'Displacement',
                                             'Horsepower', 'Weight',
                                             'Acceleration', 'Model'],
                                    transformer=SimpleImp...
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='most_frequent'))),
                ('rest_encoding',
                 TransformerWrapper(exclude=None, include=['Car'],
                                    transformer=TargetEncoder(cols=['Car'],
                                                              drop_invariant=False,
                                                              handle_missing='return_nan',
                                                              handle_unknown='value',
                                                              hierarchy=None,
                                                              min_samples_leaf=20,
                                                              return_df=True,
                                                              smoothing=10,
                                                              verbose=0)))],
         verbose=False), verbose=True, use_case=MLUsecase.CLASSIFICATION, kwargs={})
2024-09-11 11:12:42,686:INFO:Adding model into prep_pipe
2024-09-11 11:12:42,800:INFO:my_best_pipeline.pkl saved in current working directory
2024-09-11 11:12:42,819:INFO:Pipeline(memory=Memory(location=None),
         steps=[('label_encoding',
                 TransformerWrapperWithInverse(exclude=None, include=None,
                                               transformer=LabelEncoder())),
                ('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['MPG', 'Cylinders', 'Displacement',
                                             'Horsepower', 'Weight',
                                             'Acceleration', 'Model'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_...
                 ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0,
                                      class_weight=None, criterion='gini',
                                      max_depth=None, max_features='sqrt',
                                      max_leaf_nodes=None, max_samples=None,
                                      min_impurity_decrease=0.0,
                                      min_samples_leaf=1, min_samples_split=2,
                                      min_weight_fraction_leaf=0.0,
                                      monotonic_cst=None, n_estimators=100,
                                      n_jobs=-1, oob_score=False,
                                      random_state=2210, verbose=0,
                                      warm_start=False))],
         verbose=False)
2024-09-11 11:12:42,819:INFO:save_model() successfully completed......................................
2024-09-11 11:12:42,884:INFO:Initializing load_model()
2024-09-11 11:12:42,884:INFO:load_model(model_name=my_best_pipeline, platform=None, authentication=None, verbose=True)
2024-09-11 11:15:31,959:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-11 11:15:31,959:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-11 11:15:31,959:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-11 11:15:31,959:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-11 11:15:33,698:INFO:PyCaret ClassificationExperiment
2024-09-11 11:15:33,698:INFO:Logging name: clf-default-name
2024-09-11 11:15:33,698:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2024-09-11 11:15:33,698:INFO:version 3.3.2
2024-09-11 11:15:33,698:INFO:Initializing setup()
2024-09-11 11:15:33,699:INFO:self.USI: 44c9
2024-09-11 11:15:33,699:INFO:self._variable_keys: {'y_train', 'fix_imbalance', 'logging_param', 'seed', 'exp_name_log', 'exp_id', '_ml_usecase', 'log_plots_param', 'memory', 'data', 'fold_generator', 'pipeline', 'USI', 'gpu_param', 'gpu_n_jobs_param', 'y', 'n_jobs_param', 'fold_groups_param', 'fold_shuffle_param', 'is_multiclass', 'target_param', 'X', 'idx', 'X_test', 'X_train', '_available_plots', 'y_test', 'html_param'}
2024-09-11 11:15:33,699:INFO:Checking environment
2024-09-11 11:15:33,699:INFO:python_version: 3.11.8
2024-09-11 11:15:33,699:INFO:python_build: ('tags/v3.11.8:db85d51', 'Feb  6 2024 22:03:32')
2024-09-11 11:15:33,699:INFO:machine: AMD64
2024-09-11 11:15:33,699:INFO:platform: Windows-10-10.0.22631-SP0
2024-09-11 11:15:33,703:INFO:Memory: svmem(total=17096892416, available=6292197376, percent=63.2, used=10804695040, free=6292197376)
2024-09-11 11:15:33,703:INFO:Physical Core: 6
2024-09-11 11:15:33,703:INFO:Logical Core: 12
2024-09-11 11:15:33,703:INFO:Checking libraries
2024-09-11 11:15:33,703:INFO:System:
2024-09-11 11:15:33,703:INFO:    python: 3.11.8 (tags/v3.11.8:db85d51, Feb  6 2024, 22:03:32) [MSC v.1937 64 bit (AMD64)]
2024-09-11 11:15:33,704:INFO:executable: h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Scripts\python.exe
2024-09-11 11:15:33,704:INFO:   machine: Windows-10-10.0.22631-SP0
2024-09-11 11:15:33,704:INFO:PyCaret required dependencies:
2024-09-11 11:15:33,748:INFO:                 pip: 24.2
2024-09-11 11:15:33,748:INFO:          setuptools: 65.5.0
2024-09-11 11:15:33,748:INFO:             pycaret: 3.3.2
2024-09-11 11:15:33,748:INFO:             IPython: 8.27.0
2024-09-11 11:15:33,748:INFO:          ipywidgets: 8.1.5
2024-09-11 11:15:33,748:INFO:                tqdm: 4.66.5
2024-09-11 11:15:33,748:INFO:               numpy: 1.26.4
2024-09-11 11:15:33,748:INFO:              pandas: 2.1.4
2024-09-11 11:15:33,748:INFO:              jinja2: 3.1.4
2024-09-11 11:15:33,749:INFO:               scipy: 1.11.4
2024-09-11 11:15:33,749:INFO:              joblib: 1.3.2
2024-09-11 11:15:33,749:INFO:             sklearn: 1.4.2
2024-09-11 11:15:33,749:INFO:                pyod: 2.0.2
2024-09-11 11:15:33,749:INFO:            imblearn: 0.12.3
2024-09-11 11:15:33,749:INFO:   category_encoders: 2.6.3
2024-09-11 11:15:33,749:INFO:            lightgbm: 4.5.0
2024-09-11 11:15:33,750:INFO:               numba: 0.60.0
2024-09-11 11:15:33,750:INFO:            requests: 2.32.3
2024-09-11 11:15:33,750:INFO:          matplotlib: 3.7.5
2024-09-11 11:15:33,750:INFO:          scikitplot: 0.3.7
2024-09-11 11:15:33,750:INFO:         yellowbrick: 1.5
2024-09-11 11:15:33,750:INFO:              plotly: 5.24.0
2024-09-11 11:15:33,750:INFO:    plotly-resampler: Not installed
2024-09-11 11:15:33,750:INFO:             kaleido: 0.2.1
2024-09-11 11:15:33,750:INFO:           schemdraw: 0.15
2024-09-11 11:15:33,750:INFO:         statsmodels: 0.14.2
2024-09-11 11:15:33,750:INFO:              sktime: 0.26.0
2024-09-11 11:15:33,750:INFO:               tbats: 1.1.3
2024-09-11 11:15:33,751:INFO:            pmdarima: 2.0.4
2024-09-11 11:15:33,751:INFO:              psutil: 6.0.0
2024-09-11 11:15:33,751:INFO:          markupsafe: 2.1.5
2024-09-11 11:15:33,751:INFO:             pickle5: Not installed
2024-09-11 11:15:33,751:INFO:         cloudpickle: 3.0.0
2024-09-11 11:15:33,751:INFO:         deprecation: 2.1.0
2024-09-11 11:15:33,751:INFO:              xxhash: 3.5.0
2024-09-11 11:15:33,751:INFO:           wurlitzer: Not installed
2024-09-11 11:15:33,751:INFO:PyCaret optional dependencies:
2024-09-11 11:15:33,781:INFO:                shap: Not installed
2024-09-11 11:15:33,782:INFO:           interpret: Not installed
2024-09-11 11:15:33,782:INFO:                umap: Not installed
2024-09-11 11:15:33,782:INFO:     ydata_profiling: Not installed
2024-09-11 11:15:33,782:INFO:  explainerdashboard: Not installed
2024-09-11 11:15:33,782:INFO:             autoviz: Not installed
2024-09-11 11:15:33,782:INFO:           fairlearn: Not installed
2024-09-11 11:15:33,782:INFO:          deepchecks: Not installed
2024-09-11 11:15:33,782:INFO:             xgboost: Not installed
2024-09-11 11:15:33,782:INFO:            catboost: Not installed
2024-09-11 11:15:33,782:INFO:              kmodes: Not installed
2024-09-11 11:15:33,782:INFO:             mlxtend: Not installed
2024-09-11 11:15:33,782:INFO:       statsforecast: Not installed
2024-09-11 11:15:33,783:INFO:        tune_sklearn: Not installed
2024-09-11 11:15:33,783:INFO:                 ray: Not installed
2024-09-11 11:15:33,783:INFO:            hyperopt: Not installed
2024-09-11 11:15:33,783:INFO:              optuna: Not installed
2024-09-11 11:15:33,783:INFO:               skopt: Not installed
2024-09-11 11:15:33,783:INFO:              mlflow: Not installed
2024-09-11 11:15:33,783:INFO:              gradio: Not installed
2024-09-11 11:15:33,783:INFO:             fastapi: Not installed
2024-09-11 11:15:33,783:INFO:             uvicorn: Not installed
2024-09-11 11:15:33,783:INFO:              m2cgen: Not installed
2024-09-11 11:15:33,783:INFO:           evidently: Not installed
2024-09-11 11:15:33,783:INFO:               fugue: Not installed
2024-09-11 11:15:33,783:INFO:           streamlit: Not installed
2024-09-11 11:15:33,783:INFO:             prophet: Not installed
2024-09-11 11:15:33,783:INFO:None
2024-09-11 11:15:33,783:INFO:Set up data.
2024-09-11 11:15:33,795:INFO:Set up folding strategy.
2024-09-11 11:15:33,797:INFO:Set up train/test split.
2024-09-11 11:15:33,808:INFO:Set up index.
2024-09-11 11:15:33,808:INFO:Assigning column types.
2024-09-11 11:15:33,817:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2024-09-11 11:15:33,861:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-09-11 11:15:33,867:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-09-11 11:15:33,905:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-11 11:15:33,905:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-11 11:15:33,948:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-09-11 11:15:33,949:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-09-11 11:15:33,976:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-11 11:15:33,978:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-11 11:15:33,978:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2024-09-11 11:15:34,022:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-09-11 11:15:34,050:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-11 11:15:34,051:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-11 11:15:34,094:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-09-11 11:15:34,122:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-11 11:15:34,123:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-11 11:15:34,123:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2024-09-11 11:15:34,194:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-11 11:15:34,194:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-11 11:15:34,265:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-11 11:15:34,265:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-11 11:15:34,270:INFO:Preparing preprocessing pipeline...
2024-09-11 11:15:34,271:INFO:Set up label encoding.
2024-09-11 11:15:34,271:INFO:Set up simple imputation.
2024-09-11 11:15:34,275:INFO:Set up encoding of categorical features.
2024-09-11 11:15:34,412:INFO:Finished creating preprocessing pipeline.
2024-09-11 11:15:34,428:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\ipkov\AppData\Local\Temp\joblib),
         steps=[('label_encoding',
                 TransformerWrapperWithInverse(exclude=None, include=None,
                                               transformer=LabelEncoder())),
                ('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['MPG', 'Cylinders', 'Displacement',
                                             'Horsepower', 'Weight',
                                             'Acceleration', 'Model'],
                                    transformer=SimpleImp...
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='most_frequent'))),
                ('rest_encoding',
                 TransformerWrapper(exclude=None, include=['Car'],
                                    transformer=TargetEncoder(cols=['Car'],
                                                              drop_invariant=False,
                                                              handle_missing='return_nan',
                                                              handle_unknown='value',
                                                              hierarchy=None,
                                                              min_samples_leaf=20,
                                                              return_df=True,
                                                              smoothing=10,
                                                              verbose=0)))],
         verbose=False)
2024-09-11 11:15:34,428:INFO:Creating final display dataframe.
2024-09-11 11:15:34,865:INFO:Setup _display_container:                     Description                       Value
0                    Session id                        8408
1                        Target                      Origin
2                   Target type                  Multiclass
3                Target mapping  Europe: 0, Japan: 1, US: 2
4           Original data shape                    (406, 9)
5        Transformed data shape                    (406, 9)
6   Transformed train set shape                    (284, 9)
7    Transformed test set shape                    (122, 9)
8              Numeric features                           7
9          Categorical features                           1
10                   Preprocess                        True
11              Imputation type                      simple
12           Numeric imputation                        mean
13       Categorical imputation                        mode
14     Maximum one-hot encoding                          25
15              Encoding method                        None
16               Fold Generator             StratifiedKFold
17                  Fold Number                          10
18                     CPU Jobs                          -1
19                      Use GPU                       False
20               Log Experiment                       False
21              Experiment Name            clf-default-name
22                          USI                        44c9
2024-09-11 11:15:34,951:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-11 11:15:34,951:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-11 11:15:35,025:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-11 11:15:35,025:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-11 11:15:35,028:INFO:setup() successfully completed in 1.33s...............
2024-09-11 11:15:35,029:INFO:Initializing compare_models()
2024-09-11 11:15:35,029:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002B5C8A9AC10>, include=None, exclude=None, fold=None, round=4, cross_validation=True, sort=Accuracy, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x000002B5C8A9AC10>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'Accuracy', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>})
2024-09-11 11:15:35,029:INFO:Checking exceptions
2024-09-11 11:15:35,034:INFO:Preparing display monitor
2024-09-11 11:15:35,041:INFO:Initializing Logistic Regression
2024-09-11 11:15:35,041:INFO:Total runtime is 0.0 minutes
2024-09-11 11:15:35,041:INFO:SubProcess create_model() called ==================================
2024-09-11 11:15:35,042:INFO:Initializing create_model()
2024-09-11 11:15:35,042:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002B5C8A9AC10>, estimator=lr, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002B5C8CA7D90>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-11 11:15:35,042:INFO:Checking exceptions
2024-09-11 11:15:35,042:INFO:Importing libraries
2024-09-11 11:15:35,043:INFO:Copying training dataset
2024-09-11 11:15:35,050:INFO:Defining folds
2024-09-11 11:15:35,050:INFO:Declaring metric variables
2024-09-11 11:15:35,051:INFO:Importing untrained model
2024-09-11 11:15:35,052:INFO:Logistic Regression Imported successfully
2024-09-11 11:15:35,052:INFO:Starting cross validation
2024-09-11 11:15:35,055:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-11 11:15:53,673:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-11 11:15:53,780:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-11 11:15:53,800:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-11 11:15:53,805:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 11:15:53,815:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-11 11:15:53,817:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:15:53,835:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:15:53,851:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:15:53,877:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-11 11:15:53,885:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-11 11:15:53,916:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 11:15:53,917:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 11:15:53,926:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:15:53,929:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:15:53,941:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 11:15:53,941:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:15:53,948:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:15:53,952:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:15:53,954:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:15:53,954:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-11 11:15:53,959:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:15:53,967:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:15:53,985:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:15:54,002:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 11:15:54,011:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 11:15:54,013:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:15:54,025:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:15:54,033:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:15:54,039:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:15:54,047:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:15:54,051:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:15:54,076:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 11:15:54,077:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-11 11:15:54,085:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:15:54,092:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-11 11:15:54,100:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:15:54,114:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:15:54,172:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 11:15:54,172:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 11:15:54,175:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 11:15:54,179:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:15:54,181:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:15:54,184:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:15:54,191:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:15:54,192:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:15:54,194:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:15:54,200:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:15:54,202:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:15:54,203:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:15:54,227:INFO:Calculating mean and std
2024-09-11 11:15:54,231:INFO:Creating metrics dataframe
2024-09-11 11:15:54,237:INFO:Uploading results into container
2024-09-11 11:15:54,238:INFO:Uploading model into container now
2024-09-11 11:15:54,239:INFO:_master_model_container: 1
2024-09-11 11:15:54,239:INFO:_display_container: 2
2024-09-11 11:15:54,240:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=8408, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2024-09-11 11:15:54,240:INFO:create_model() successfully completed......................................
2024-09-11 11:15:54,360:INFO:SubProcess create_model() end ==================================
2024-09-11 11:15:54,360:INFO:Creating metrics dataframe
2024-09-11 11:15:54,364:INFO:Initializing K Neighbors Classifier
2024-09-11 11:15:54,364:INFO:Total runtime is 0.3220411618550619 minutes
2024-09-11 11:15:54,364:INFO:SubProcess create_model() called ==================================
2024-09-11 11:15:54,364:INFO:Initializing create_model()
2024-09-11 11:15:54,365:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002B5C8A9AC10>, estimator=knn, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002B5C8CA7D90>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-11 11:15:54,365:INFO:Checking exceptions
2024-09-11 11:15:54,365:INFO:Importing libraries
2024-09-11 11:15:54,365:INFO:Copying training dataset
2024-09-11 11:15:54,372:INFO:Defining folds
2024-09-11 11:15:54,372:INFO:Declaring metric variables
2024-09-11 11:15:54,372:INFO:Importing untrained model
2024-09-11 11:15:54,373:INFO:K Neighbors Classifier Imported successfully
2024-09-11 11:15:54,373:INFO:Starting cross validation
2024-09-11 11:15:54,375:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-11 11:15:55,228:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:15:55,237:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:15:55,243:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:15:55,256:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:15:55,258:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:15:55,259:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:15:55,260:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:15:55,262:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:15:55,269:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:15:55,275:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:15:55,281:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:15:55,282:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:15:55,284:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:15:55,288:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:15:55,293:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:15:55,298:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:15:55,299:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:15:55,304:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:15:55,305:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:15:55,320:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:15:55,325:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:15:55,341:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:15:55,343:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:15:55,359:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:15:59,707:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:15:59,715:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:15:59,721:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:15:59,722:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:15:59,725:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:15:59,732:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:15:59,743:INFO:Calculating mean and std
2024-09-11 11:15:59,744:INFO:Creating metrics dataframe
2024-09-11 11:15:59,748:INFO:Uploading results into container
2024-09-11 11:15:59,748:INFO:Uploading model into container now
2024-09-11 11:15:59,749:INFO:_master_model_container: 2
2024-09-11 11:15:59,749:INFO:_display_container: 2
2024-09-11 11:15:59,750:INFO:KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform')
2024-09-11 11:15:59,750:INFO:create_model() successfully completed......................................
2024-09-11 11:15:59,844:INFO:SubProcess create_model() end ==================================
2024-09-11 11:15:59,844:INFO:Creating metrics dataframe
2024-09-11 11:15:59,847:INFO:Initializing Naive Bayes
2024-09-11 11:15:59,847:INFO:Total runtime is 0.41343259811401367 minutes
2024-09-11 11:15:59,848:INFO:SubProcess create_model() called ==================================
2024-09-11 11:15:59,848:INFO:Initializing create_model()
2024-09-11 11:15:59,848:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002B5C8A9AC10>, estimator=nb, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002B5C8CA7D90>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-11 11:15:59,848:INFO:Checking exceptions
2024-09-11 11:15:59,848:INFO:Importing libraries
2024-09-11 11:15:59,848:INFO:Copying training dataset
2024-09-11 11:15:59,855:INFO:Defining folds
2024-09-11 11:15:59,855:INFO:Declaring metric variables
2024-09-11 11:15:59,855:INFO:Importing untrained model
2024-09-11 11:15:59,855:INFO:Naive Bayes Imported successfully
2024-09-11 11:15:59,857:INFO:Starting cross validation
2024-09-11 11:15:59,859:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-11 11:16:00,232:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:00,237:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:00,252:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:00,255:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:00,258:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:00,260:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 11:16:00,263:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:00,273:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:00,275:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:00,277:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:00,279:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:00,280:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:00,280:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:00,281:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:00,282:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:00,288:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:00,289:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 11:16:00,298:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:00,301:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:00,301:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:00,301:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:00,303:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:00,308:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:00,308:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:00,311:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 11:16:00,315:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:00,317:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:00,318:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:00,323:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:00,325:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:00,339:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:00,344:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:00,357:INFO:Calculating mean and std
2024-09-11 11:16:00,358:INFO:Creating metrics dataframe
2024-09-11 11:16:00,360:INFO:Uploading results into container
2024-09-11 11:16:00,361:INFO:Uploading model into container now
2024-09-11 11:16:00,362:INFO:_master_model_container: 3
2024-09-11 11:16:00,362:INFO:_display_container: 2
2024-09-11 11:16:00,362:INFO:GaussianNB(priors=None, var_smoothing=1e-09)
2024-09-11 11:16:00,362:INFO:create_model() successfully completed......................................
2024-09-11 11:16:00,428:INFO:SubProcess create_model() end ==================================
2024-09-11 11:16:00,428:INFO:Creating metrics dataframe
2024-09-11 11:16:00,434:INFO:Initializing Decision Tree Classifier
2024-09-11 11:16:00,434:INFO:Total runtime is 0.42321592966715493 minutes
2024-09-11 11:16:00,435:INFO:SubProcess create_model() called ==================================
2024-09-11 11:16:00,435:INFO:Initializing create_model()
2024-09-11 11:16:00,435:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002B5C8A9AC10>, estimator=dt, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002B5C8CA7D90>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-11 11:16:00,435:INFO:Checking exceptions
2024-09-11 11:16:00,435:INFO:Importing libraries
2024-09-11 11:16:00,435:INFO:Copying training dataset
2024-09-11 11:16:00,441:INFO:Defining folds
2024-09-11 11:16:00,442:INFO:Declaring metric variables
2024-09-11 11:16:00,442:INFO:Importing untrained model
2024-09-11 11:16:00,443:INFO:Decision Tree Classifier Imported successfully
2024-09-11 11:16:00,443:INFO:Starting cross validation
2024-09-11 11:16:00,446:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-11 11:16:00,828:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:00,838:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:00,849:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:00,852:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:00,852:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:00,855:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:00,858:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:00,859:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:00,860:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:00,861:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:00,864:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 11:16:00,867:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 11:16:00,869:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:00,870:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:00,872:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:00,875:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:00,876:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:00,876:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:00,879:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:00,880:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:00,883:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:00,889:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:00,890:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:00,892:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:00,893:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:00,894:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:00,895:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:00,896:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 11:16:00,898:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:00,900:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:00,908:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:00,915:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:00,930:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:00,940:INFO:Calculating mean and std
2024-09-11 11:16:00,941:INFO:Creating metrics dataframe
2024-09-11 11:16:00,944:INFO:Uploading results into container
2024-09-11 11:16:00,944:INFO:Uploading model into container now
2024-09-11 11:16:00,945:INFO:_master_model_container: 4
2024-09-11 11:16:00,945:INFO:_display_container: 2
2024-09-11 11:16:00,946:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=8408, splitter='best')
2024-09-11 11:16:00,946:INFO:create_model() successfully completed......................................
2024-09-11 11:16:01,012:INFO:SubProcess create_model() end ==================================
2024-09-11 11:16:01,012:INFO:Creating metrics dataframe
2024-09-11 11:16:01,015:INFO:Initializing SVM - Linear Kernel
2024-09-11 11:16:01,015:INFO:Total runtime is 0.432896347840627 minutes
2024-09-11 11:16:01,017:INFO:SubProcess create_model() called ==================================
2024-09-11 11:16:01,017:INFO:Initializing create_model()
2024-09-11 11:16:01,017:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002B5C8A9AC10>, estimator=svm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002B5C8CA7D90>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-11 11:16:01,017:INFO:Checking exceptions
2024-09-11 11:16:01,017:INFO:Importing libraries
2024-09-11 11:16:01,017:INFO:Copying training dataset
2024-09-11 11:16:01,023:INFO:Defining folds
2024-09-11 11:16:01,023:INFO:Declaring metric variables
2024-09-11 11:16:01,024:INFO:Importing untrained model
2024-09-11 11:16:01,025:INFO:SVM - Linear Kernel Imported successfully
2024-09-11 11:16:01,025:INFO:Starting cross validation
2024-09-11 11:16:01,027:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-11 11:16:01,528:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 11:16:01,562:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 11:16:01,572:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:01,581:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:01,583:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 11:16:01,591:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 11:16:01,593:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:01,594:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:01,601:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:01,604:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 11:16:01,604:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:01,607:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 11:16:01,617:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:01,620:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:01,635:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:01,636:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 11:16:01,640:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:01,640:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 11:16:01,642:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 11:16:01,645:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 11:16:01,642:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:01,651:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 11:16:01,651:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 11:16:01,651:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:01,654:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:01,655:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 11:16:01,659:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:01,659:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:01,661:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:01,663:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:01,671:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:01,680:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 11:16:01,683:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:01,684:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 11:16:01,687:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:01,691:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:01,692:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 11:16:01,693:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:01,695:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:01,707:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:01,708:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:01,716:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:01,729:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 11:16:01,742:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:01,767:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 11:16:01,770:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:01,774:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:01,777:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 11:16:01,779:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:01,794:INFO:Calculating mean and std
2024-09-11 11:16:01,795:INFO:Creating metrics dataframe
2024-09-11 11:16:01,799:INFO:Uploading results into container
2024-09-11 11:16:01,800:INFO:Uploading model into container now
2024-09-11 11:16:01,801:INFO:_master_model_container: 5
2024-09-11 11:16:01,801:INFO:_display_container: 2
2024-09-11 11:16:01,802:INFO:SGDClassifier(alpha=0.0001, average=False, class_weight=None,
              early_stopping=False, epsilon=0.1, eta0=0.001, fit_intercept=True,
              l1_ratio=0.15, learning_rate='optimal', loss='hinge',
              max_iter=1000, n_iter_no_change=5, n_jobs=-1, penalty='l2',
              power_t=0.5, random_state=8408, shuffle=True, tol=0.001,
              validation_fraction=0.1, verbose=0, warm_start=False)
2024-09-11 11:16:01,802:INFO:create_model() successfully completed......................................
2024-09-11 11:16:01,869:INFO:SubProcess create_model() end ==================================
2024-09-11 11:16:01,869:INFO:Creating metrics dataframe
2024-09-11 11:16:01,873:INFO:Initializing Ridge Classifier
2024-09-11 11:16:01,873:INFO:Total runtime is 0.44719841082890827 minutes
2024-09-11 11:16:01,874:INFO:SubProcess create_model() called ==================================
2024-09-11 11:16:01,874:INFO:Initializing create_model()
2024-09-11 11:16:01,874:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002B5C8A9AC10>, estimator=ridge, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002B5C8CA7D90>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-11 11:16:01,874:INFO:Checking exceptions
2024-09-11 11:16:01,874:INFO:Importing libraries
2024-09-11 11:16:01,874:INFO:Copying training dataset
2024-09-11 11:16:01,881:INFO:Defining folds
2024-09-11 11:16:01,882:INFO:Declaring metric variables
2024-09-11 11:16:01,882:INFO:Importing untrained model
2024-09-11 11:16:01,882:INFO:Ridge Classifier Imported successfully
2024-09-11 11:16:01,883:INFO:Starting cross validation
2024-09-11 11:16:01,885:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-11 11:16:02,264:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 11:16:02,275:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:02,277:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:02,283:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 11:16:02,296:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 11:16:02,296:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:02,297:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:02,299:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:02,300:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:02,301:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 11:16:02,304:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 11:16:02,306:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 11:16:02,308:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:02,314:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:02,314:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:02,316:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:02,316:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:02,317:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:02,319:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 11:16:02,320:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:02,321:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:02,324:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:02,327:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 11:16:02,331:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:02,334:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:02,335:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:02,337:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:02,339:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:02,342:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:02,345:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:02,348:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:02,351:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 11:16:02,354:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:02,355:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:02,357:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:02,362:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:02,368:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 11:16:02,371:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:02,375:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:02,387:INFO:Calculating mean and std
2024-09-11 11:16:02,388:INFO:Creating metrics dataframe
2024-09-11 11:16:02,392:INFO:Uploading results into container
2024-09-11 11:16:02,392:INFO:Uploading model into container now
2024-09-11 11:16:02,393:INFO:_master_model_container: 6
2024-09-11 11:16:02,393:INFO:_display_container: 2
2024-09-11 11:16:02,394:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=8408, solver='auto',
                tol=0.0001)
2024-09-11 11:16:02,394:INFO:create_model() successfully completed......................................
2024-09-11 11:16:02,459:INFO:SubProcess create_model() end ==================================
2024-09-11 11:16:02,459:INFO:Creating metrics dataframe
2024-09-11 11:16:02,463:INFO:Initializing Random Forest Classifier
2024-09-11 11:16:02,463:INFO:Total runtime is 0.45703327655792236 minutes
2024-09-11 11:16:02,464:INFO:SubProcess create_model() called ==================================
2024-09-11 11:16:02,464:INFO:Initializing create_model()
2024-09-11 11:16:02,464:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002B5C8A9AC10>, estimator=rf, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002B5C8CA7D90>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-11 11:16:02,464:INFO:Checking exceptions
2024-09-11 11:16:02,464:INFO:Importing libraries
2024-09-11 11:16:02,464:INFO:Copying training dataset
2024-09-11 11:16:02,474:INFO:Defining folds
2024-09-11 11:16:02,475:INFO:Declaring metric variables
2024-09-11 11:16:02,475:INFO:Importing untrained model
2024-09-11 11:16:02,476:INFO:Random Forest Classifier Imported successfully
2024-09-11 11:16:02,476:INFO:Starting cross validation
2024-09-11 11:16:02,479:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-11 11:16:04,563:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:04,581:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:04,592:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:04,597:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:04,597:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 11:16:04,600:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:04,608:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:04,608:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:04,613:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:04,614:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:04,619:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:04,621:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:04,624:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:04,628:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:04,633:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 11:16:04,633:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:04,638:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:04,639:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:04,647:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:04,650:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:04,650:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:04,651:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:04,651:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:04,659:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:04,661:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:04,670:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:04,684:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:04,687:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:04,691:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:04,698:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:04,714:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:04,727:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:04,775:INFO:Calculating mean and std
2024-09-11 11:16:04,780:INFO:Creating metrics dataframe
2024-09-11 11:16:04,793:INFO:Uploading results into container
2024-09-11 11:16:04,794:INFO:Uploading model into container now
2024-09-11 11:16:04,795:INFO:_master_model_container: 7
2024-09-11 11:16:04,795:INFO:_display_container: 2
2024-09-11 11:16:04,796:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=8408, verbose=0,
                       warm_start=False)
2024-09-11 11:16:04,796:INFO:create_model() successfully completed......................................
2024-09-11 11:16:04,876:INFO:SubProcess create_model() end ==================================
2024-09-11 11:16:04,876:INFO:Creating metrics dataframe
2024-09-11 11:16:04,881:INFO:Initializing Quadratic Discriminant Analysis
2024-09-11 11:16:04,881:INFO:Total runtime is 0.4973306854565938 minutes
2024-09-11 11:16:04,881:INFO:SubProcess create_model() called ==================================
2024-09-11 11:16:04,882:INFO:Initializing create_model()
2024-09-11 11:16:04,882:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002B5C8A9AC10>, estimator=qda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002B5C8CA7D90>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-11 11:16:04,882:INFO:Checking exceptions
2024-09-11 11:16:04,882:INFO:Importing libraries
2024-09-11 11:16:04,882:INFO:Copying training dataset
2024-09-11 11:16:04,889:INFO:Defining folds
2024-09-11 11:16:04,890:INFO:Declaring metric variables
2024-09-11 11:16:04,890:INFO:Importing untrained model
2024-09-11 11:16:04,890:INFO:Quadratic Discriminant Analysis Imported successfully
2024-09-11 11:16:04,891:INFO:Starting cross validation
2024-09-11 11:16:04,893:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-11 11:16:05,245:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 11:16:05,246:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 11:16:05,256:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:05,259:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:05,263:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 11:16:05,270:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 11:16:05,274:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 11:16:05,275:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:05,277:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 11:16:05,278:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:05,278:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:05,279:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:05,282:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 11:16:05,284:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:05,286:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:05,295:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:05,295:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:05,298:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:05,301:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:05,303:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 11:16:05,305:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 11:16:05,306:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:05,308:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:05,309:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:05,306:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 11:16:05,315:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:05,315:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:05,316:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 11:16:05,316:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:05,320:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:05,322:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:05,323:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 11:16:05,329:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:05,330:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:05,333:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:05,335:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:05,335:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:05,352:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:05,352:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:05,355:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:05,359:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:05,372:INFO:Calculating mean and std
2024-09-11 11:16:05,373:INFO:Creating metrics dataframe
2024-09-11 11:16:05,375:INFO:Uploading results into container
2024-09-11 11:16:05,376:INFO:Uploading model into container now
2024-09-11 11:16:05,376:INFO:_master_model_container: 8
2024-09-11 11:16:05,377:INFO:_display_container: 2
2024-09-11 11:16:05,377:INFO:QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,
                              store_covariance=False, tol=0.0001)
2024-09-11 11:16:05,377:INFO:create_model() successfully completed......................................
2024-09-11 11:16:05,453:INFO:SubProcess create_model() end ==================================
2024-09-11 11:16:05,453:INFO:Creating metrics dataframe
2024-09-11 11:16:05,457:INFO:Initializing Ada Boost Classifier
2024-09-11 11:16:05,458:INFO:Total runtime is 0.5069512526194254 minutes
2024-09-11 11:16:05,458:INFO:SubProcess create_model() called ==================================
2024-09-11 11:16:05,458:INFO:Initializing create_model()
2024-09-11 11:16:05,459:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002B5C8A9AC10>, estimator=ada, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002B5C8CA7D90>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-11 11:16:05,459:INFO:Checking exceptions
2024-09-11 11:16:05,459:INFO:Importing libraries
2024-09-11 11:16:05,459:INFO:Copying training dataset
2024-09-11 11:16:05,464:INFO:Defining folds
2024-09-11 11:16:05,464:INFO:Declaring metric variables
2024-09-11 11:16:05,465:INFO:Importing untrained model
2024-09-11 11:16:05,465:INFO:Ada Boost Classifier Imported successfully
2024-09-11 11:16:05,466:INFO:Starting cross validation
2024-09-11 11:16:05,468:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-11 11:16:05,670:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-11 11:16:05,675:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-11 11:16:05,686:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-11 11:16:05,693:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-11 11:16:05,694:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-11 11:16:05,695:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-11 11:16:05,695:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-11 11:16:05,713:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-11 11:16:05,718:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-11 11:16:05,734:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-11 11:16:06,495:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 11:16:06,495:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 11:16:06,502:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 11:16:06,508:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 11:16:06,504:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 11:16:06,510:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:06,513:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:06,514:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:06,517:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 11:16:06,521:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:06,521:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 11:16:06,522:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:06,527:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:06,529:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 11:16:06,531:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:06,531:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:06,535:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 11:16:06,537:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:06,540:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:06,541:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 11:16:06,543:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:06,545:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:06,549:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:06,551:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:06,553:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:06,553:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:06,554:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:06,555:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:06,557:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 11:16:06,563:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 11:16:06,566:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:06,567:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:06,568:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:06,572:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:06,575:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:06,575:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:06,577:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:06,579:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 11:16:06,592:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:06,592:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:06,595:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:06,601:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:06,613:INFO:Calculating mean and std
2024-09-11 11:16:06,614:INFO:Creating metrics dataframe
2024-09-11 11:16:06,617:INFO:Uploading results into container
2024-09-11 11:16:06,618:INFO:Uploading model into container now
2024-09-11 11:16:06,619:INFO:_master_model_container: 9
2024-09-11 11:16:06,619:INFO:_display_container: 2
2024-09-11 11:16:06,619:INFO:AdaBoostClassifier(algorithm='SAMME.R', estimator=None, learning_rate=1.0,
                   n_estimators=50, random_state=8408)
2024-09-11 11:16:06,619:INFO:create_model() successfully completed......................................
2024-09-11 11:16:06,689:INFO:SubProcess create_model() end ==================================
2024-09-11 11:16:06,689:INFO:Creating metrics dataframe
2024-09-11 11:16:06,693:INFO:Initializing Gradient Boosting Classifier
2024-09-11 11:16:06,693:INFO:Total runtime is 0.5275364160537719 minutes
2024-09-11 11:16:06,693:INFO:SubProcess create_model() called ==================================
2024-09-11 11:16:06,694:INFO:Initializing create_model()
2024-09-11 11:16:06,694:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002B5C8A9AC10>, estimator=gbc, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002B5C8CA7D90>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-11 11:16:06,694:INFO:Checking exceptions
2024-09-11 11:16:06,694:INFO:Importing libraries
2024-09-11 11:16:06,694:INFO:Copying training dataset
2024-09-11 11:16:06,702:INFO:Defining folds
2024-09-11 11:16:06,702:INFO:Declaring metric variables
2024-09-11 11:16:06,702:INFO:Importing untrained model
2024-09-11 11:16:06,703:INFO:Gradient Boosting Classifier Imported successfully
2024-09-11 11:16:06,704:INFO:Starting cross validation
2024-09-11 11:16:06,705:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-11 11:16:08,594:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 11:16:08,605:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:08,612:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 11:16:08,622:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:08,638:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 11:16:08,642:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:08,643:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:08,643:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 11:16:08,650:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:08,654:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:08,654:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 11:16:08,657:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 11:16:08,665:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:08,665:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:08,667:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:08,672:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:08,672:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:08,674:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 11:16:08,678:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 11:16:08,684:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:08,686:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:08,688:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:08,689:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:08,691:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:08,701:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 11:16:08,708:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:08,710:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:08,712:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:08,714:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:08,723:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:08,725:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:08,732:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 11:16:08,735:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 11:16:08,736:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:08,737:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:08,744:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 11:16:08,745:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:08,748:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:08,748:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 11:16:08,753:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:08,756:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:08,760:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:08,772:INFO:Calculating mean and std
2024-09-11 11:16:08,774:INFO:Creating metrics dataframe
2024-09-11 11:16:08,777:INFO:Uploading results into container
2024-09-11 11:16:08,778:INFO:Uploading model into container now
2024-09-11 11:16:08,778:INFO:_master_model_container: 10
2024-09-11 11:16:08,778:INFO:_display_container: 2
2024-09-11 11:16:08,779:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=8408, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2024-09-11 11:16:08,779:INFO:create_model() successfully completed......................................
2024-09-11 11:16:08,844:INFO:SubProcess create_model() end ==================================
2024-09-11 11:16:08,845:INFO:Creating metrics dataframe
2024-09-11 11:16:08,850:INFO:Initializing Linear Discriminant Analysis
2024-09-11 11:16:08,850:INFO:Total runtime is 0.56347918510437 minutes
2024-09-11 11:16:08,851:INFO:SubProcess create_model() called ==================================
2024-09-11 11:16:08,851:INFO:Initializing create_model()
2024-09-11 11:16:08,851:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002B5C8A9AC10>, estimator=lda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002B5C8CA7D90>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-11 11:16:08,852:INFO:Checking exceptions
2024-09-11 11:16:08,852:INFO:Importing libraries
2024-09-11 11:16:08,852:INFO:Copying training dataset
2024-09-11 11:16:08,857:INFO:Defining folds
2024-09-11 11:16:08,857:INFO:Declaring metric variables
2024-09-11 11:16:08,857:INFO:Importing untrained model
2024-09-11 11:16:08,858:INFO:Linear Discriminant Analysis Imported successfully
2024-09-11 11:16:08,858:INFO:Starting cross validation
2024-09-11 11:16:08,860:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-11 11:16:09,169:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 11:16:09,178:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:09,183:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 11:16:09,186:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 11:16:09,194:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:09,195:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 11:16:09,195:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:09,203:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:09,204:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 11:16:09,205:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:09,214:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:09,214:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:09,214:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 11:16:09,219:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:09,220:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 11:16:09,220:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:09,226:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 11:16:09,226:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:09,228:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 11:16:09,229:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:09,229:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:09,230:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:09,231:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:09,236:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 11:16:09,236:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:09,243:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:09,247:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:09,250:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:09,251:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:09,253:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:09,255:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:09,255:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 11:16:09,257:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 11:16:09,263:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 11:16:09,264:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:09,264:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:09,265:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:09,265:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:09,270:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:09,274:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:09,278:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:09,284:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:09,297:INFO:Calculating mean and std
2024-09-11 11:16:09,298:INFO:Creating metrics dataframe
2024-09-11 11:16:09,301:INFO:Uploading results into container
2024-09-11 11:16:09,302:INFO:Uploading model into container now
2024-09-11 11:16:09,303:INFO:_master_model_container: 11
2024-09-11 11:16:09,303:INFO:_display_container: 2
2024-09-11 11:16:09,304:INFO:LinearDiscriminantAnalysis(covariance_estimator=None, n_components=None,
                           priors=None, shrinkage=None, solver='svd',
                           store_covariance=False, tol=0.0001)
2024-09-11 11:16:09,304:INFO:create_model() successfully completed......................................
2024-09-11 11:16:09,367:INFO:SubProcess create_model() end ==================================
2024-09-11 11:16:09,368:INFO:Creating metrics dataframe
2024-09-11 11:16:09,372:INFO:Initializing Extra Trees Classifier
2024-09-11 11:16:09,372:INFO:Total runtime is 0.5721847613652546 minutes
2024-09-11 11:16:09,372:INFO:SubProcess create_model() called ==================================
2024-09-11 11:16:09,372:INFO:Initializing create_model()
2024-09-11 11:16:09,373:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002B5C8A9AC10>, estimator=et, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002B5C8CA7D90>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-11 11:16:09,373:INFO:Checking exceptions
2024-09-11 11:16:09,373:INFO:Importing libraries
2024-09-11 11:16:09,373:INFO:Copying training dataset
2024-09-11 11:16:09,379:INFO:Defining folds
2024-09-11 11:16:09,379:INFO:Declaring metric variables
2024-09-11 11:16:09,379:INFO:Importing untrained model
2024-09-11 11:16:09,380:INFO:Extra Trees Classifier Imported successfully
2024-09-11 11:16:09,380:INFO:Starting cross validation
2024-09-11 11:16:09,384:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-11 11:16:11,285:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:11,295:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:11,312:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:11,324:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:11,340:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:11,333:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:11,342:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:11,344:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:11,345:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:11,348:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:11,362:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:11,363:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:11,369:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:11,370:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:11,376:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:11,378:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:11,380:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 11:16:11,386:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:11,389:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:11,389:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:11,393:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:11,400:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:11,407:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:11,430:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:11,439:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 11:16:11,447:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:11,486:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:11,486:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:11,491:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:11,491:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:11,493:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 11:16:11,498:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:11,500:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:11,516:INFO:Calculating mean and std
2024-09-11 11:16:11,516:INFO:Creating metrics dataframe
2024-09-11 11:16:11,520:INFO:Uploading results into container
2024-09-11 11:16:11,521:INFO:Uploading model into container now
2024-09-11 11:16:11,521:INFO:_master_model_container: 12
2024-09-11 11:16:11,521:INFO:_display_container: 2
2024-09-11 11:16:11,522:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=8408, verbose=0,
                     warm_start=False)
2024-09-11 11:16:11,522:INFO:create_model() successfully completed......................................
2024-09-11 11:16:11,587:INFO:SubProcess create_model() end ==================================
2024-09-11 11:16:11,587:INFO:Creating metrics dataframe
2024-09-11 11:16:11,592:INFO:Initializing Light Gradient Boosting Machine
2024-09-11 11:16:11,593:INFO:Total runtime is 0.6091794331868489 minutes
2024-09-11 11:16:11,593:INFO:SubProcess create_model() called ==================================
2024-09-11 11:16:11,594:INFO:Initializing create_model()
2024-09-11 11:16:11,594:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002B5C8A9AC10>, estimator=lightgbm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002B5C8CA7D90>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-11 11:16:11,594:INFO:Checking exceptions
2024-09-11 11:16:11,594:INFO:Importing libraries
2024-09-11 11:16:11,594:INFO:Copying training dataset
2024-09-11 11:16:11,601:INFO:Defining folds
2024-09-11 11:16:11,601:INFO:Declaring metric variables
2024-09-11 11:16:11,601:INFO:Importing untrained model
2024-09-11 11:16:11,602:INFO:Light Gradient Boosting Machine Imported successfully
2024-09-11 11:16:11,603:INFO:Starting cross validation
2024-09-11 11:16:11,605:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-11 11:16:16,063:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:16,075:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:16,097:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:16,114:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:16,128:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:16,133:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 11:16:16,145:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:16,149:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:16,159:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:16,176:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:16,281:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:16,308:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:16,323:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:16,452:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:16,477:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:16,499:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:16,836:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:16,851:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:16,862:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 11:16:16,877:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:16,885:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:16,908:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:16,915:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 11:16:16,927:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:16,928:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:16,961:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:16,981:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:17,085:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:17,093:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:17,095:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:17,103:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:17,105:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:17,116:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:17,137:INFO:Calculating mean and std
2024-09-11 11:16:17,140:INFO:Creating metrics dataframe
2024-09-11 11:16:17,148:INFO:Uploading results into container
2024-09-11 11:16:17,150:INFO:Uploading model into container now
2024-09-11 11:16:17,151:INFO:_master_model_container: 13
2024-09-11 11:16:17,151:INFO:_display_container: 2
2024-09-11 11:16:17,154:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=8408, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-09-11 11:16:17,154:INFO:create_model() successfully completed......................................
2024-09-11 11:16:17,243:INFO:SubProcess create_model() end ==================================
2024-09-11 11:16:17,243:INFO:Creating metrics dataframe
2024-09-11 11:16:17,248:INFO:Initializing Dummy Classifier
2024-09-11 11:16:17,248:INFO:Total runtime is 0.7034501274426778 minutes
2024-09-11 11:16:17,249:INFO:SubProcess create_model() called ==================================
2024-09-11 11:16:17,249:INFO:Initializing create_model()
2024-09-11 11:16:17,249:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002B5C8A9AC10>, estimator=dummy, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002B5C8CA7D90>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-11 11:16:17,249:INFO:Checking exceptions
2024-09-11 11:16:17,250:INFO:Importing libraries
2024-09-11 11:16:17,250:INFO:Copying training dataset
2024-09-11 11:16:17,256:INFO:Defining folds
2024-09-11 11:16:17,256:INFO:Declaring metric variables
2024-09-11 11:16:17,257:INFO:Importing untrained model
2024-09-11 11:16:17,257:INFO:Dummy Classifier Imported successfully
2024-09-11 11:16:17,257:INFO:Starting cross validation
2024-09-11 11:16:17,260:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-11 11:16:17,585:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:17,590:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:17,595:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:17,601:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:17,603:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:17,611:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:17,617:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:17,619:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:17,619:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 11:16:17,622:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:17,624:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:17,626:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 11:16:17,627:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:17,629:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:17,632:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:17,633:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 11:16:17,634:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:17,636:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 11:16:17,637:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:17,638:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:17,643:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:17,644:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:17,646:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 11:16:17,646:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:17,649:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:17,653:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 11:16:17,657:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:17,658:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 11:16:17,658:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:17,662:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:17,663:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:17,664:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:17,668:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 11:16:17,669:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 11:16:17,680:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:17,682:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:17,683:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:17,683:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:17,690:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 11:16:17,694:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:17,716:INFO:Calculating mean and std
2024-09-11 11:16:17,717:INFO:Creating metrics dataframe
2024-09-11 11:16:17,720:INFO:Uploading results into container
2024-09-11 11:16:17,720:INFO:Uploading model into container now
2024-09-11 11:16:17,721:INFO:_master_model_container: 14
2024-09-11 11:16:17,721:INFO:_display_container: 2
2024-09-11 11:16:17,721:INFO:DummyClassifier(constant=None, random_state=8408, strategy='prior')
2024-09-11 11:16:17,721:INFO:create_model() successfully completed......................................
2024-09-11 11:16:17,785:INFO:SubProcess create_model() end ==================================
2024-09-11 11:16:17,785:INFO:Creating metrics dataframe
2024-09-11 11:16:17,792:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:339: FutureWarning: Styler.applymap has been deprecated. Use Styler.map instead.
  .applymap(highlight_cols, subset=["TT (Sec)"])

2024-09-11 11:16:17,796:INFO:Initializing create_model()
2024-09-11 11:16:17,796:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002B5C8A9AC10>, estimator=ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=8408, verbose=0,
                     warm_start=False), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-11 11:16:17,796:INFO:Checking exceptions
2024-09-11 11:16:17,798:INFO:Importing libraries
2024-09-11 11:16:17,798:INFO:Copying training dataset
2024-09-11 11:16:17,804:INFO:Defining folds
2024-09-11 11:16:17,804:INFO:Declaring metric variables
2024-09-11 11:16:17,804:INFO:Importing untrained model
2024-09-11 11:16:17,804:INFO:Declaring custom model
2024-09-11 11:16:17,807:INFO:Extra Trees Classifier Imported successfully
2024-09-11 11:16:17,810:INFO:Cross validation set to False
2024-09-11 11:16:17,810:INFO:Fitting Model
2024-09-11 11:16:18,091:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=8408, verbose=0,
                     warm_start=False)
2024-09-11 11:16:18,091:INFO:create_model() successfully completed......................................
2024-09-11 11:16:18,190:INFO:_master_model_container: 14
2024-09-11 11:16:18,190:INFO:_display_container: 2
2024-09-11 11:16:18,192:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=8408, verbose=0,
                     warm_start=False)
2024-09-11 11:16:18,192:INFO:compare_models() successfully completed......................................
2024-09-11 11:16:18,193:INFO:Initializing predict_model()
2024-09-11 11:16:18,193:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002B5C8A9AC10>, estimator=ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=8408, verbose=0,
                     warm_start=False), probability_threshold=None, encoded_labels=False, raw_score=True, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x000002B591397420>)
2024-09-11 11:16:18,193:INFO:Checking exceptions
2024-09-11 11:16:18,193:INFO:Preloading libraries
2024-09-11 11:16:18,193:INFO:Set up data.
2024-09-11 11:16:18,201:INFO:Set up index.
2024-09-11 11:16:18,474:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:18,480:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:18,485:WARNING:h:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 11:16:18,612:INFO:Initializing save_model()
2024-09-11 11:16:18,612:INFO:save_model(model=ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=8408, verbose=0,
                     warm_start=False), model_name=my_best_pipeline, prep_pipe_=Pipeline(memory=FastMemory(location=C:\Users\ipkov\AppData\Local\Temp\joblib),
         steps=[('label_encoding',
                 TransformerWrapperWithInverse(exclude=None, include=None,
                                               transformer=LabelEncoder())),
                ('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['MPG', 'Cylinders', 'Displacement',
                                             'Horsepower', 'Weight',
                                             'Acceleration', 'Model'],
                                    transformer=SimpleImp...
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='most_frequent'))),
                ('rest_encoding',
                 TransformerWrapper(exclude=None, include=['Car'],
                                    transformer=TargetEncoder(cols=['Car'],
                                                              drop_invariant=False,
                                                              handle_missing='return_nan',
                                                              handle_unknown='value',
                                                              hierarchy=None,
                                                              min_samples_leaf=20,
                                                              return_df=True,
                                                              smoothing=10,
                                                              verbose=0)))],
         verbose=False), verbose=True, use_case=MLUsecase.CLASSIFICATION, kwargs={})
2024-09-11 11:16:18,613:INFO:Adding model into prep_pipe
2024-09-11 11:16:18,729:INFO:my_best_pipeline.pkl saved in current working directory
2024-09-11 11:16:18,748:INFO:Pipeline(memory=Memory(location=None),
         steps=[('label_encoding',
                 TransformerWrapperWithInverse(exclude=None, include=None,
                                               transformer=LabelEncoder())),
                ('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['MPG', 'Cylinders', 'Displacement',
                                             'Horsepower', 'Weight',
                                             'Acceleration', 'Model'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_...
                 ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0,
                                      class_weight=None, criterion='gini',
                                      max_depth=None, max_features='sqrt',
                                      max_leaf_nodes=None, max_samples=None,
                                      min_impurity_decrease=0.0,
                                      min_samples_leaf=1, min_samples_split=2,
                                      min_weight_fraction_leaf=0.0,
                                      monotonic_cst=None, n_estimators=100,
                                      n_jobs=-1, oob_score=False,
                                      random_state=8408, verbose=0,
                                      warm_start=False))],
         verbose=False)
2024-09-11 11:16:18,748:INFO:save_model() successfully completed......................................
2024-09-11 12:26:43,382:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-11 12:26:43,382:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-11 12:26:43,382:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-11 12:26:43,382:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-11 12:26:45,601:INFO:PyCaret ClassificationExperiment
2024-09-11 12:26:45,601:INFO:Logging name: cars
2024-09-11 12:26:45,601:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2024-09-11 12:26:45,601:INFO:version 3.3.2
2024-09-11 12:26:45,601:INFO:Initializing setup()
2024-09-11 12:26:45,601:INFO:self.USI: e5a6
2024-09-11 12:26:45,601:INFO:self._variable_keys: {'gpu_param', 'y_test', 'memory', 'target_param', 'fold_groups_param', 'exp_id', 'fix_imbalance', 'log_plots_param', 'logging_param', 'pipeline', 'USI', '_available_plots', 'X_train', 'fold_generator', 'is_multiclass', 'html_param', 'idx', 'n_jobs_param', '_ml_usecase', 'y', 'X', 'data', 'seed', 'y_train', 'X_test', 'exp_name_log', 'gpu_n_jobs_param', 'fold_shuffle_param'}
2024-09-11 12:26:45,601:INFO:Checking environment
2024-09-11 12:26:45,601:INFO:python_version: 3.11.8
2024-09-11 12:26:45,601:INFO:python_build: ('tags/v3.11.8:db85d51', 'Feb  6 2024 22:03:32')
2024-09-11 12:26:45,601:INFO:machine: AMD64
2024-09-11 12:26:45,611:INFO:platform: Windows-10-10.0.22631-SP0
2024-09-11 12:26:45,614:INFO:Memory: svmem(total=17096892416, available=4654600192, percent=72.8, used=12442292224, free=4654600192)
2024-09-11 12:26:45,615:INFO:Physical Core: 6
2024-09-11 12:26:45,615:INFO:Logical Core: 12
2024-09-11 12:26:45,615:INFO:Checking libraries
2024-09-11 12:26:45,615:INFO:System:
2024-09-11 12:26:45,615:INFO:    python: 3.11.8 (tags/v3.11.8:db85d51, Feb  6 2024, 22:03:32) [MSC v.1937 64 bit (AMD64)]
2024-09-11 12:26:45,615:INFO:executable: H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Scripts\python.exe
2024-09-11 12:26:45,615:INFO:   machine: Windows-10-10.0.22631-SP0
2024-09-11 12:26:45,615:INFO:PyCaret required dependencies:
2024-09-11 12:26:45,645:INFO:                 pip: 24.2
2024-09-11 12:26:45,645:INFO:          setuptools: 65.5.0
2024-09-11 12:26:45,645:INFO:             pycaret: 3.3.2
2024-09-11 12:26:45,645:INFO:             IPython: 8.27.0
2024-09-11 12:26:45,645:INFO:          ipywidgets: 8.1.5
2024-09-11 12:26:45,645:INFO:                tqdm: 4.66.5
2024-09-11 12:26:45,645:INFO:               numpy: 1.26.4
2024-09-11 12:26:45,645:INFO:              pandas: 2.1.4
2024-09-11 12:26:45,645:INFO:              jinja2: 3.1.4
2024-09-11 12:26:45,645:INFO:               scipy: 1.11.4
2024-09-11 12:26:45,645:INFO:              joblib: 1.3.2
2024-09-11 12:26:45,645:INFO:             sklearn: 1.4.2
2024-09-11 12:26:45,645:INFO:                pyod: 2.0.2
2024-09-11 12:26:45,645:INFO:            imblearn: 0.12.3
2024-09-11 12:26:45,645:INFO:   category_encoders: 2.6.3
2024-09-11 12:26:45,645:INFO:            lightgbm: 4.5.0
2024-09-11 12:26:45,646:INFO:               numba: 0.60.0
2024-09-11 12:26:45,646:INFO:            requests: 2.32.3
2024-09-11 12:26:45,646:INFO:          matplotlib: 3.7.5
2024-09-11 12:26:45,646:INFO:          scikitplot: 0.3.7
2024-09-11 12:26:45,646:INFO:         yellowbrick: 1.5
2024-09-11 12:26:45,646:INFO:              plotly: 5.24.0
2024-09-11 12:26:45,646:INFO:    plotly-resampler: Not installed
2024-09-11 12:26:45,646:INFO:             kaleido: 0.2.1
2024-09-11 12:26:45,646:INFO:           schemdraw: 0.15
2024-09-11 12:26:45,646:INFO:         statsmodels: 0.14.2
2024-09-11 12:26:45,646:INFO:              sktime: 0.26.0
2024-09-11 12:26:45,646:INFO:               tbats: 1.1.3
2024-09-11 12:26:45,646:INFO:            pmdarima: 2.0.4
2024-09-11 12:26:45,646:INFO:              psutil: 6.0.0
2024-09-11 12:26:45,646:INFO:          markupsafe: 2.1.5
2024-09-11 12:26:45,646:INFO:             pickle5: Not installed
2024-09-11 12:26:45,646:INFO:         cloudpickle: 3.0.0
2024-09-11 12:26:45,646:INFO:         deprecation: 2.1.0
2024-09-11 12:26:45,646:INFO:              xxhash: 3.5.0
2024-09-11 12:26:45,646:INFO:           wurlitzer: Not installed
2024-09-11 12:26:45,646:INFO:PyCaret optional dependencies:
2024-09-11 12:26:45,667:INFO:                shap: Not installed
2024-09-11 12:26:45,667:INFO:           interpret: Not installed
2024-09-11 12:26:45,667:INFO:                umap: Not installed
2024-09-11 12:26:45,667:INFO:     ydata_profiling: Not installed
2024-09-11 12:26:45,667:INFO:  explainerdashboard: Not installed
2024-09-11 12:26:45,667:INFO:             autoviz: Not installed
2024-09-11 12:26:45,667:INFO:           fairlearn: Not installed
2024-09-11 12:26:45,667:INFO:          deepchecks: Not installed
2024-09-11 12:26:45,667:INFO:             xgboost: Not installed
2024-09-11 12:26:45,667:INFO:            catboost: Not installed
2024-09-11 12:26:45,667:INFO:              kmodes: Not installed
2024-09-11 12:26:45,667:INFO:             mlxtend: Not installed
2024-09-11 12:26:45,667:INFO:       statsforecast: Not installed
2024-09-11 12:26:45,667:INFO:        tune_sklearn: Not installed
2024-09-11 12:26:45,667:INFO:                 ray: Not installed
2024-09-11 12:26:45,667:INFO:            hyperopt: Not installed
2024-09-11 12:26:45,667:INFO:              optuna: Not installed
2024-09-11 12:26:45,667:INFO:               skopt: Not installed
2024-09-11 12:26:45,667:INFO:              mlflow: 2.16.0
2024-09-11 12:26:45,667:INFO:              gradio: Not installed
2024-09-11 12:26:45,667:INFO:             fastapi: Not installed
2024-09-11 12:26:45,667:INFO:             uvicorn: Not installed
2024-09-11 12:26:45,667:INFO:              m2cgen: Not installed
2024-09-11 12:26:45,667:INFO:           evidently: Not installed
2024-09-11 12:26:45,667:INFO:               fugue: Not installed
2024-09-11 12:26:45,667:INFO:           streamlit: Not installed
2024-09-11 12:26:45,667:INFO:             prophet: Not installed
2024-09-11 12:26:45,668:INFO:None
2024-09-11 12:26:45,668:INFO:Set up data.
2024-09-11 12:26:45,672:INFO:Set up folding strategy.
2024-09-11 12:26:45,672:INFO:Set up train/test split.
2024-09-11 12:26:45,678:INFO:Set up index.
2024-09-11 12:26:45,678:INFO:Assigning column types.
2024-09-11 12:26:45,681:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2024-09-11 12:26:45,721:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-09-11 12:26:45,726:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-09-11 12:26:45,758:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-11 12:26:45,758:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-11 12:26:45,794:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-09-11 12:26:45,795:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-09-11 12:26:45,817:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-11 12:26:45,817:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-11 12:26:45,817:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2024-09-11 12:26:45,853:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-09-11 12:26:45,876:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-11 12:26:45,877:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-11 12:26:45,913:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-09-11 12:26:45,937:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-11 12:26:45,937:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-11 12:26:45,938:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2024-09-11 12:26:45,999:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-11 12:26:46,000:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-11 12:26:46,061:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-11 12:26:46,061:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-11 12:26:46,063:INFO:Preparing preprocessing pipeline...
2024-09-11 12:26:46,063:INFO:Set up label encoding.
2024-09-11 12:26:46,064:INFO:Set up simple imputation.
2024-09-11 12:26:46,065:INFO:Set up encoding of categorical features.
2024-09-11 12:26:46,105:INFO:Finished creating preprocessing pipeline.
2024-09-11 12:26:46,112:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\ipkov\AppData\Local\Temp\joblib),
         steps=[('label_encoding',
                 TransformerWrapperWithInverse(exclude=None, include=None,
                                               transformer=LabelEncoder())),
                ('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['MPG', 'Cylinders', 'Displacement',
                                             'Horsepower', 'Weight',
                                             'Acceleration', 'Model'],
                                    transformer=SimpleImp...
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='most_frequent'))),
                ('rest_encoding',
                 TransformerWrapper(exclude=None, include=['Car'],
                                    transformer=TargetEncoder(cols=['Car'],
                                                              drop_invariant=False,
                                                              handle_missing='return_nan',
                                                              handle_unknown='value',
                                                              hierarchy=None,
                                                              min_samples_leaf=20,
                                                              return_df=True,
                                                              smoothing=10,
                                                              verbose=0)))],
         verbose=False)
2024-09-11 12:26:46,112:INFO:Creating final display dataframe.
2024-09-11 12:26:46,240:INFO:Setup _display_container:                     Description                       Value
0                    Session id                        1521
1                        Target                      Origin
2                   Target type                  Multiclass
3                Target mapping  Europe: 0, Japan: 1, US: 2
4           Original data shape                    (406, 9)
5        Transformed data shape                    (406, 9)
6   Transformed train set shape                    (284, 9)
7    Transformed test set shape                    (122, 9)
8              Numeric features                           7
9          Categorical features                           1
10                   Preprocess                        True
11              Imputation type                      simple
12           Numeric imputation                        mean
13       Categorical imputation                        mode
14     Maximum one-hot encoding                          25
15              Encoding method                        None
16               Fold Generator             StratifiedKFold
17                  Fold Number                          10
18                     CPU Jobs                          -1
19                      Use GPU                       False
20               Log Experiment                MlflowLogger
21              Experiment Name                        cars
22                          USI                        e5a6
2024-09-11 12:26:46,306:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-11 12:26:46,306:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-11 12:26:46,368:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-11 12:26:46,368:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-11 12:26:46,369:INFO:Logging experiment in loggers
2024-09-11 12:26:46,768:INFO:SubProcess save_model() called ==================================
2024-09-11 12:26:46,775:INFO:Initializing save_model()
2024-09-11 12:26:46,775:INFO:save_model(model=Pipeline(memory=FastMemory(location=C:\Users\ipkov\AppData\Local\Temp\joblib),
         steps=[('label_encoding',
                 TransformerWrapperWithInverse(exclude=None, include=None,
                                               transformer=LabelEncoder())),
                ('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['MPG', 'Cylinders', 'Displacement',
                                             'Horsepower', 'Weight',
                                             'Acceleration', 'Model'],
                                    transformer=SimpleImp...
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='most_frequent'))),
                ('rest_encoding',
                 TransformerWrapper(exclude=None, include=['Car'],
                                    transformer=TargetEncoder(cols=['Car'],
                                                              drop_invariant=False,
                                                              handle_missing='return_nan',
                                                              handle_unknown='value',
                                                              hierarchy=None,
                                                              min_samples_leaf=20,
                                                              return_df=True,
                                                              smoothing=10,
                                                              verbose=0)))],
         verbose=False), model_name=C:\Users\ipkov\AppData\Local\Temp\tmppjqojtxk\Transformation Pipeline, prep_pipe_=Pipeline(memory=FastMemory(location=C:\Users\ipkov\AppData\Local\Temp\joblib),
         steps=[('label_encoding',
                 TransformerWrapperWithInverse(exclude=None, include=None,
                                               transformer=LabelEncoder())),
                ('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['MPG', 'Cylinders', 'Displacement',
                                             'Horsepower', 'Weight',
                                             'Acceleration', 'Model'],
                                    transformer=SimpleImp...
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='most_frequent'))),
                ('rest_encoding',
                 TransformerWrapper(exclude=None, include=['Car'],
                                    transformer=TargetEncoder(cols=['Car'],
                                                              drop_invariant=False,
                                                              handle_missing='return_nan',
                                                              handle_unknown='value',
                                                              hierarchy=None,
                                                              min_samples_leaf=20,
                                                              return_df=True,
                                                              smoothing=10,
                                                              verbose=0)))],
         verbose=False), verbose=False, use_case=MLUsecase.CLASSIFICATION, kwargs={})
2024-09-11 12:26:46,775:INFO:Adding model into prep_pipe
2024-09-11 12:26:46,775:WARNING:Only Model saved as it was a pipeline.
2024-09-11 12:26:46,780:INFO:C:\Users\ipkov\AppData\Local\Temp\tmppjqojtxk\Transformation Pipeline.pkl saved in current working directory
2024-09-11 12:26:46,784:INFO:Pipeline(memory=FastMemory(location=C:\Users\ipkov\AppData\Local\Temp\joblib),
         steps=[('label_encoding',
                 TransformerWrapperWithInverse(exclude=None, include=None,
                                               transformer=LabelEncoder())),
                ('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['MPG', 'Cylinders', 'Displacement',
                                             'Horsepower', 'Weight',
                                             'Acceleration', 'Model'],
                                    transformer=SimpleImp...
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='most_frequent'))),
                ('rest_encoding',
                 TransformerWrapper(exclude=None, include=['Car'],
                                    transformer=TargetEncoder(cols=['Car'],
                                                              drop_invariant=False,
                                                              handle_missing='return_nan',
                                                              handle_unknown='value',
                                                              hierarchy=None,
                                                              min_samples_leaf=20,
                                                              return_df=True,
                                                              smoothing=10,
                                                              verbose=0)))],
         verbose=False)
2024-09-11 12:26:46,785:INFO:save_model() successfully completed......................................
2024-09-11 12:26:46,851:INFO:SubProcess save_model() end ==================================
2024-09-11 12:26:46,967:INFO:setup() successfully completed in 0.77s...............
2024-09-11 12:26:46,967:INFO:Initializing compare_models()
2024-09-11 12:26:46,967:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002522F2CFB10>, include=None, exclude=None, fold=None, round=4, cross_validation=True, sort=Accuracy, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x000002522F2CFB10>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'Accuracy', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>})
2024-09-11 12:26:46,967:INFO:Checking exceptions
2024-09-11 12:26:46,969:INFO:Preparing display monitor
2024-09-11 12:26:46,972:INFO:Initializing Logistic Regression
2024-09-11 12:26:46,972:INFO:Total runtime is 0.0 minutes
2024-09-11 12:26:46,972:INFO:SubProcess create_model() called ==================================
2024-09-11 12:26:46,972:INFO:Initializing create_model()
2024-09-11 12:26:46,972:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002522F2CFB10>, estimator=lr, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000252691C6910>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-11 12:26:46,972:INFO:Checking exceptions
2024-09-11 12:26:46,972:INFO:Importing libraries
2024-09-11 12:26:46,972:INFO:Copying training dataset
2024-09-11 12:26:46,976:INFO:Defining folds
2024-09-11 12:26:46,977:INFO:Declaring metric variables
2024-09-11 12:26:46,977:INFO:Importing untrained model
2024-09-11 12:26:46,977:INFO:Logistic Regression Imported successfully
2024-09-11 12:26:46,977:INFO:Starting cross validation
2024-09-11 12:26:46,978:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-11 12:26:56,963:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-11 12:26:56,998:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 12:26:57,001:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:26:57,004:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:26:57,009:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:26:57,012:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-11 12:26:57,031:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-11 12:26:57,052:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 12:26:57,056:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:26:57,064:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:26:57,066:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 12:26:57,068:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:26:57,070:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:26:57,074:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:26:57,078:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-11 12:26:57,084:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:26:57,089:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-11 12:26:57,115:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 12:26:57,120:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:26:57,124:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 12:26:57,124:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-11 12:26:57,125:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:26:57,128:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:26:57,130:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:26:57,130:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-11 12:26:57,135:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:26:57,140:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:26:57,146:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-11 12:26:57,157:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 12:26:57,163:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-11 12:26:57,165:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:26:57,165:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 12:26:57,168:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-11 12:26:57,169:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:26:57,170:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:26:57,171:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:26:57,174:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:26:57,179:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:26:57,180:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 12:26:57,186:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:26:57,192:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:26:57,196:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:26:57,203:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 12:26:57,204:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 12:26:57,207:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:26:57,208:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:26:57,213:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:26:57,213:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:26:57,217:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:26:57,238:INFO:Calculating mean and std
2024-09-11 12:26:57,240:INFO:Creating metrics dataframe
2024-09-11 12:26:57,245:INFO:Uploading results into container
2024-09-11 12:26:57,246:INFO:Uploading model into container now
2024-09-11 12:26:57,247:INFO:_master_model_container: 1
2024-09-11 12:26:57,247:INFO:_display_container: 2
2024-09-11 12:26:57,247:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=1521, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2024-09-11 12:26:57,248:INFO:create_model() successfully completed......................................
2024-09-11 12:26:57,411:INFO:SubProcess create_model() end ==================================
2024-09-11 12:26:57,411:INFO:Creating metrics dataframe
2024-09-11 12:26:57,413:INFO:Initializing K Neighbors Classifier
2024-09-11 12:26:57,413:INFO:Total runtime is 0.17401588360468548 minutes
2024-09-11 12:26:57,413:INFO:SubProcess create_model() called ==================================
2024-09-11 12:26:57,413:INFO:Initializing create_model()
2024-09-11 12:26:57,413:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002522F2CFB10>, estimator=knn, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000252691C6910>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-11 12:26:57,413:INFO:Checking exceptions
2024-09-11 12:26:57,413:INFO:Importing libraries
2024-09-11 12:26:57,413:INFO:Copying training dataset
2024-09-11 12:26:57,417:INFO:Defining folds
2024-09-11 12:26:57,417:INFO:Declaring metric variables
2024-09-11 12:26:57,417:INFO:Importing untrained model
2024-09-11 12:26:57,417:INFO:K Neighbors Classifier Imported successfully
2024-09-11 12:26:57,418:INFO:Starting cross validation
2024-09-11 12:26:57,418:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-11 12:26:57,604:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:26:57,608:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:26:57,609:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:26:57,611:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:26:57,611:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:26:57,612:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:26:57,614:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:26:57,618:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:26:57,618:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:26:57,620:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:26:57,621:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:26:57,622:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:26:57,624:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:26:57,626:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:26:57,627:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:26:57,632:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:26:57,633:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:26:57,633:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:26:57,637:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:26:57,652:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:26:57,654:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:26:57,655:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:00,595:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:00,595:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:00,598:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:00,598:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:00,599:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:00,600:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:00,612:INFO:Calculating mean and std
2024-09-11 12:27:00,613:INFO:Creating metrics dataframe
2024-09-11 12:27:00,614:INFO:Uploading results into container
2024-09-11 12:27:00,614:INFO:Uploading model into container now
2024-09-11 12:27:00,615:INFO:_master_model_container: 2
2024-09-11 12:27:00,615:INFO:_display_container: 2
2024-09-11 12:27:00,615:INFO:KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform')
2024-09-11 12:27:00,615:INFO:create_model() successfully completed......................................
2024-09-11 12:27:00,750:INFO:SubProcess create_model() end ==================================
2024-09-11 12:27:00,750:INFO:Creating metrics dataframe
2024-09-11 12:27:00,752:INFO:Initializing Naive Bayes
2024-09-11 12:27:00,752:INFO:Total runtime is 0.2296650528907776 minutes
2024-09-11 12:27:00,752:INFO:SubProcess create_model() called ==================================
2024-09-11 12:27:00,752:INFO:Initializing create_model()
2024-09-11 12:27:00,752:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002522F2CFB10>, estimator=nb, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000252691C6910>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-11 12:27:00,752:INFO:Checking exceptions
2024-09-11 12:27:00,752:INFO:Importing libraries
2024-09-11 12:27:00,752:INFO:Copying training dataset
2024-09-11 12:27:00,756:INFO:Defining folds
2024-09-11 12:27:00,756:INFO:Declaring metric variables
2024-09-11 12:27:00,756:INFO:Importing untrained model
2024-09-11 12:27:00,756:INFO:Naive Bayes Imported successfully
2024-09-11 12:27:00,756:INFO:Starting cross validation
2024-09-11 12:27:00,757:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-11 12:27:00,876:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:00,881:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:00,883:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:00,883:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:00,887:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:00,888:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:00,888:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:00,889:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:00,893:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:00,893:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:00,894:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:00,896:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:00,899:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:00,899:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:00,900:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:00,902:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:00,903:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:00,903:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:00,904:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:00,904:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 12:27:00,905:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:00,906:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:00,907:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:00,907:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:00,908:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:00,908:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:00,908:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:00,909:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:00,911:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:00,912:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:00,912:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:00,919:INFO:Calculating mean and std
2024-09-11 12:27:00,919:INFO:Creating metrics dataframe
2024-09-11 12:27:00,920:INFO:Uploading results into container
2024-09-11 12:27:00,921:INFO:Uploading model into container now
2024-09-11 12:27:00,921:INFO:_master_model_container: 3
2024-09-11 12:27:00,921:INFO:_display_container: 2
2024-09-11 12:27:00,921:INFO:GaussianNB(priors=None, var_smoothing=1e-09)
2024-09-11 12:27:00,921:INFO:create_model() successfully completed......................................
2024-09-11 12:27:00,994:INFO:SubProcess create_model() end ==================================
2024-09-11 12:27:00,994:INFO:Creating metrics dataframe
2024-09-11 12:27:00,996:INFO:Initializing Decision Tree Classifier
2024-09-11 12:27:00,996:INFO:Total runtime is 0.23373222748438519 minutes
2024-09-11 12:27:00,996:INFO:SubProcess create_model() called ==================================
2024-09-11 12:27:00,996:INFO:Initializing create_model()
2024-09-11 12:27:00,997:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002522F2CFB10>, estimator=dt, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000252691C6910>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-11 12:27:00,997:INFO:Checking exceptions
2024-09-11 12:27:00,997:INFO:Importing libraries
2024-09-11 12:27:00,997:INFO:Copying training dataset
2024-09-11 12:27:00,999:INFO:Defining folds
2024-09-11 12:27:01,000:INFO:Declaring metric variables
2024-09-11 12:27:01,000:INFO:Importing untrained model
2024-09-11 12:27:01,000:INFO:Decision Tree Classifier Imported successfully
2024-09-11 12:27:01,000:INFO:Starting cross validation
2024-09-11 12:27:01,001:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-11 12:27:01,111:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:01,113:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:01,114:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:01,116:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:01,116:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:01,120:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:01,121:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:01,121:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:01,122:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:01,122:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:01,123:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:01,124:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 12:27:01,127:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:01,127:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:01,128:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:01,129:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:01,130:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:01,130:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:01,130:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:01,134:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:01,134:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:01,134:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:01,137:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:01,137:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:01,137:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:01,140:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:01,142:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:01,142:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:01,144:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:01,145:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:01,153:INFO:Calculating mean and std
2024-09-11 12:27:01,153:INFO:Creating metrics dataframe
2024-09-11 12:27:01,154:INFO:Uploading results into container
2024-09-11 12:27:01,155:INFO:Uploading model into container now
2024-09-11 12:27:01,155:INFO:_master_model_container: 4
2024-09-11 12:27:01,155:INFO:_display_container: 2
2024-09-11 12:27:01,155:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=1521, splitter='best')
2024-09-11 12:27:01,155:INFO:create_model() successfully completed......................................
2024-09-11 12:27:01,225:INFO:SubProcess create_model() end ==================================
2024-09-11 12:27:01,226:INFO:Creating metrics dataframe
2024-09-11 12:27:01,228:INFO:Initializing SVM - Linear Kernel
2024-09-11 12:27:01,228:INFO:Total runtime is 0.23759998083114625 minutes
2024-09-11 12:27:01,228:INFO:SubProcess create_model() called ==================================
2024-09-11 12:27:01,228:INFO:Initializing create_model()
2024-09-11 12:27:01,228:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002522F2CFB10>, estimator=svm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000252691C6910>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-11 12:27:01,228:INFO:Checking exceptions
2024-09-11 12:27:01,228:INFO:Importing libraries
2024-09-11 12:27:01,228:INFO:Copying training dataset
2024-09-11 12:27:01,231:INFO:Defining folds
2024-09-11 12:27:01,231:INFO:Declaring metric variables
2024-09-11 12:27:01,231:INFO:Importing untrained model
2024-09-11 12:27:01,231:INFO:SVM - Linear Kernel Imported successfully
2024-09-11 12:27:01,232:INFO:Starting cross validation
2024-09-11 12:27:01,232:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-11 12:27:01,363:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 12:27:01,366:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:01,368:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 12:27:01,368:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 12:27:01,368:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 12:27:01,370:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:01,372:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:01,374:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:01,375:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:01,378:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:01,380:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 12:27:01,381:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 12:27:01,381:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 12:27:01,383:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:01,383:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 12:27:01,384:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:01,384:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:01,385:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 12:27:01,385:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:01,386:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 12:27:01,386:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:01,386:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:01,387:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 12:27:01,389:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 12:27:01,389:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:01,389:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 12:27:01,390:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:01,391:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:01,393:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:01,393:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:01,394:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:01,394:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:01,398:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 12:27:01,398:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:01,398:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 12:27:01,399:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:01,399:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:01,400:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 12:27:01,401:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

fier, f"{metric.capitalize()} is", len(result))

2024-09-11 12:27:01,402:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 12:27:01,402:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:01,403:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:01,403:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:01,404:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:01,404:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:01,406:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:01,409:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 12:27:01,410:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:01,419:INFO:Calculating mean and std
2024-09-11 12:27:01,419:INFO:Creating metrics dataframe
2024-09-11 12:27:01,421:INFO:Uploading results into container
2024-09-11 12:27:01,421:INFO:Uploading model into container now
2024-09-11 12:27:01,421:INFO:_master_model_container: 5
2024-09-11 12:27:01,421:INFO:_display_container: 2
2024-09-11 12:27:01,422:INFO:SGDClassifier(alpha=0.0001, average=False, class_weight=None,
              early_stopping=False, epsilon=0.1, eta0=0.001, fit_intercept=True,
              l1_ratio=0.15, learning_rate='optimal', loss='hinge',
              max_iter=1000, n_iter_no_change=5, n_jobs=-1, penalty='l2',
              power_t=0.5, random_state=1521, shuffle=True, tol=0.001,
              validation_fraction=0.1, verbose=0, warm_start=False)
2024-09-11 12:27:01,422:INFO:create_model() successfully completed......................................
2024-09-11 12:27:01,492:INFO:SubProcess create_model() end ==================================
2024-09-11 12:27:01,493:INFO:Creating metrics dataframe
2024-09-11 12:27:01,495:INFO:Initializing Ridge Classifier
2024-09-11 12:27:01,495:INFO:Total runtime is 0.24204299052556358 minutes
2024-09-11 12:27:01,495:INFO:SubProcess create_model() called ==================================
2024-09-11 12:27:01,496:INFO:Initializing create_model()
2024-09-11 12:27:01,496:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002522F2CFB10>, estimator=ridge, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000252691C6910>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-11 12:27:01,496:INFO:Checking exceptions
2024-09-11 12:27:01,496:INFO:Importing libraries
2024-09-11 12:27:01,496:INFO:Copying training dataset
2024-09-11 12:27:01,500:INFO:Defining folds
2024-09-11 12:27:01,500:INFO:Declaring metric variables
2024-09-11 12:27:01,500:INFO:Importing untrained model
2024-09-11 12:27:01,500:INFO:Ridge Classifier Imported successfully
2024-09-11 12:27:01,501:INFO:Starting cross validation
2024-09-11 12:27:01,501:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-11 12:27:01,604:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 12:27:01,604:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 12:27:01,607:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:01,609:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 12:27:01,612:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:01,612:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:01,612:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:01,614:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 12:27:01,617:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:01,619:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:01,618:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 12:27:01,620:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:01,622:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:01,623:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 12:27:01,626:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:01,628:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:01,628:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:01,630:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 12:27:01,630:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:01,630:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 12:27:01,632:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:01,632:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:01,633:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:01,633:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:01,633:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

at the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 12:27:01,633:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:01,635:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 12:27:01,635:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 12:27:01,637:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:01,638:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:01,638:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:01,638:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:01,638:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:01,639:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:01,641:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:01,641:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:01,642:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:01,642:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:01,644:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:01,644:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:01,654:INFO:Calculating mean and std
2024-09-11 12:27:01,655:INFO:Creating metrics dataframe
2024-09-11 12:27:01,656:INFO:Uploading results into container
2024-09-11 12:27:01,656:INFO:Uploading model into container now
2024-09-11 12:27:01,656:INFO:_master_model_container: 6
2024-09-11 12:27:01,656:INFO:_display_container: 2
2024-09-11 12:27:01,657:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=1521, solver='auto',
                tol=0.0001)
2024-09-11 12:27:01,657:INFO:create_model() successfully completed......................................
2024-09-11 12:27:01,731:INFO:SubProcess create_model() end ==================================
2024-09-11 12:27:01,731:INFO:Creating metrics dataframe
2024-09-11 12:27:01,733:INFO:Initializing Random Forest Classifier
2024-09-11 12:27:01,733:INFO:Total runtime is 0.24600881735483807 minutes
2024-09-11 12:27:01,733:INFO:SubProcess create_model() called ==================================
2024-09-11 12:27:01,733:INFO:Initializing create_model()
2024-09-11 12:27:01,733:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002522F2CFB10>, estimator=rf, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000252691C6910>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-11 12:27:01,733:INFO:Checking exceptions
2024-09-11 12:27:01,733:INFO:Importing libraries
2024-09-11 12:27:01,733:INFO:Copying training dataset
2024-09-11 12:27:01,737:INFO:Defining folds
2024-09-11 12:27:01,737:INFO:Declaring metric variables
2024-09-11 12:27:01,737:INFO:Importing untrained model
2024-09-11 12:27:01,738:INFO:Random Forest Classifier Imported successfully
2024-09-11 12:27:01,738:INFO:Starting cross validation
2024-09-11 12:27:01,738:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-11 12:27:02,369:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:02,369:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:02,371:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:02,375:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:02,376:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:02,376:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:02,380:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:02,380:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:02,381:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:02,382:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:02,383:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:02,383:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:02,384:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 12:27:02,387:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:02,388:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:02,389:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:02,389:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:02,393:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:02,396:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:02,397:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:02,400:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:02,402:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:02,402:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:02,407:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:02,410:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:02,429:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:02,431:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:02,433:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:02,485:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:02,487:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:02,488:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:02,495:INFO:Calculating mean and std
2024-09-11 12:27:02,496:INFO:Creating metrics dataframe
2024-09-11 12:27:02,498:INFO:Uploading results into container
2024-09-11 12:27:02,498:INFO:Uploading model into container now
2024-09-11 12:27:02,499:INFO:_master_model_container: 7
2024-09-11 12:27:02,499:INFO:_display_container: 2
2024-09-11 12:27:02,499:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=1521, verbose=0,
                       warm_start=False)
2024-09-11 12:27:02,499:INFO:create_model() successfully completed......................................
2024-09-11 12:27:02,582:INFO:SubProcess create_model() end ==================================
2024-09-11 12:27:02,582:INFO:Creating metrics dataframe
2024-09-11 12:27:02,584:INFO:Initializing Quadratic Discriminant Analysis
2024-09-11 12:27:02,585:INFO:Total runtime is 0.2602069854736328 minutes
2024-09-11 12:27:02,585:INFO:SubProcess create_model() called ==================================
2024-09-11 12:27:02,585:INFO:Initializing create_model()
2024-09-11 12:27:02,585:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002522F2CFB10>, estimator=qda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000252691C6910>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-11 12:27:02,585:INFO:Checking exceptions
2024-09-11 12:27:02,585:INFO:Importing libraries
2024-09-11 12:27:02,585:INFO:Copying training dataset
2024-09-11 12:27:02,587:INFO:Defining folds
2024-09-11 12:27:02,587:INFO:Declaring metric variables
2024-09-11 12:27:02,588:INFO:Importing untrained model
2024-09-11 12:27:02,588:INFO:Quadratic Discriminant Analysis Imported successfully
2024-09-11 12:27:02,588:INFO:Starting cross validation
2024-09-11 12:27:02,589:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-11 12:27:02,696:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 12:27:02,698:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 12:27:02,700:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:02,700:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 12:27:02,703:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:02,704:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:02,704:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:02,708:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:02,708:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:02,709:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 12:27:02,710:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:02,712:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:02,713:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:02,714:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:02,716:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 12:27:02,718:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 12:27:02,719:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 12:27:02,720:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:02,720:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 12:27:02,720:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 12:27:02,720:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:02,722:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:02,723:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:02,723:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:02,723:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:02,726:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:02,726:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:02,727:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:02,727:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:02,727:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:02,729:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:02,730:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 12:27:02,731:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:02,732:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:02,732:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:02,733:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:02,733:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:02,741:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 12:27:02,742:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:02,745:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:02,747:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:02,753:INFO:Calculating mean and std
2024-09-11 12:27:02,753:INFO:Creating metrics dataframe
2024-09-11 12:27:02,755:INFO:Uploading results into container
2024-09-11 12:27:02,755:INFO:Uploading model into container now
2024-09-11 12:27:02,755:INFO:_master_model_container: 8
2024-09-11 12:27:02,755:INFO:_display_container: 2
2024-09-11 12:27:02,755:INFO:QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,
                              store_covariance=False, tol=0.0001)
2024-09-11 12:27:02,755:INFO:create_model() successfully completed......................................
2024-09-11 12:27:02,833:INFO:SubProcess create_model() end ==================================
2024-09-11 12:27:02,833:INFO:Creating metrics dataframe
2024-09-11 12:27:02,835:INFO:Initializing Ada Boost Classifier
2024-09-11 12:27:02,835:INFO:Total runtime is 0.26437681913375854 minutes
2024-09-11 12:27:02,835:INFO:SubProcess create_model() called ==================================
2024-09-11 12:27:02,835:INFO:Initializing create_model()
2024-09-11 12:27:02,836:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002522F2CFB10>, estimator=ada, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000252691C6910>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-11 12:27:02,836:INFO:Checking exceptions
2024-09-11 12:27:02,836:INFO:Importing libraries
2024-09-11 12:27:02,836:INFO:Copying training dataset
2024-09-11 12:27:02,839:INFO:Defining folds
2024-09-11 12:27:02,839:INFO:Declaring metric variables
2024-09-11 12:27:02,839:INFO:Importing untrained model
2024-09-11 12:27:02,839:INFO:Ada Boost Classifier Imported successfully
2024-09-11 12:27:02,839:INFO:Starting cross validation
2024-09-11 12:27:02,840:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-11 12:27:02,906:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-11 12:27:02,910:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-11 12:27:02,914:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-11 12:27:02,918:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-11 12:27:02,918:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-11 12:27:02,919:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-11 12:27:02,922:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-11 12:27:02,922:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-11 12:27:02,926:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-11 12:27:02,926:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-11 12:27:03,194:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 12:27:03,197:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:03,197:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 12:27:03,197:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 12:27:03,199:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 12:27:03,202:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:03,202:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 12:27:03,204:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:03,204:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:03,205:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:03,206:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:03,208:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:03,209:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:03,211:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:03,211:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:03,212:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 12:27:03,212:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:03,215:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:03,217:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:03,218:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:03,219:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:03,226:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 12:27:03,230:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:03,231:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 12:27:03,231:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 12:27:03,232:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 12:27:03,232:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:03,232:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:03,232:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:03,232:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:03,232:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:03,234:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:03,234:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:03,234:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:03,234:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:03,234:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:03,235:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:03,237:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:03,237:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:03,238:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:03,242:INFO:Calculating mean and std
2024-09-11 12:27:03,242:INFO:Creating metrics dataframe
2024-09-11 12:27:03,244:INFO:Uploading results into container
2024-09-11 12:27:03,245:INFO:Uploading model into container now
2024-09-11 12:27:03,245:INFO:_master_model_container: 9
2024-09-11 12:27:03,246:INFO:_display_container: 2
2024-09-11 12:27:03,246:INFO:AdaBoostClassifier(algorithm='SAMME.R', estimator=None, learning_rate=1.0,
                   n_estimators=50, random_state=1521)
2024-09-11 12:27:03,246:INFO:create_model() successfully completed......................................
2024-09-11 12:27:03,318:INFO:SubProcess create_model() end ==================================
2024-09-11 12:27:03,319:INFO:Creating metrics dataframe
2024-09-11 12:27:03,320:INFO:Initializing Gradient Boosting Classifier
2024-09-11 12:27:03,321:INFO:Total runtime is 0.2724668622016907 minutes
2024-09-11 12:27:03,321:INFO:SubProcess create_model() called ==================================
2024-09-11 12:27:03,321:INFO:Initializing create_model()
2024-09-11 12:27:03,321:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002522F2CFB10>, estimator=gbc, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000252691C6910>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-11 12:27:03,321:INFO:Checking exceptions
2024-09-11 12:27:03,321:INFO:Importing libraries
2024-09-11 12:27:03,321:INFO:Copying training dataset
2024-09-11 12:27:03,324:INFO:Defining folds
2024-09-11 12:27:03,324:INFO:Declaring metric variables
2024-09-11 12:27:03,324:INFO:Importing untrained model
2024-09-11 12:27:03,324:INFO:Gradient Boosting Classifier Imported successfully
2024-09-11 12:27:03,324:INFO:Starting cross validation
2024-09-11 12:27:03,325:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-11 12:27:03,937:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 12:27:03,940:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 12:27:03,941:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:03,943:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 12:27:03,944:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:03,945:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 12:27:03,946:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:03,947:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:03,949:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:03,950:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:03,953:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 12:27:03,954:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:03,954:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:03,955:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:03,955:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 12:27:03,956:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:03,956:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 12:27:03,958:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:03,958:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 12:27:03,959:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:03,960:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 12:27:03,961:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:03,961:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:03,962:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:03,963:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:03,965:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:03,965:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 12:27:03,966:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:03,966:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 12:27:03,968:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:03,969:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:03,969:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:03,969:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:03,970:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:03,971:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:03,971:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:03,974:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:03,974:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:03,977:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:03,978:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:03,984:INFO:Calculating mean and std
2024-09-11 12:27:03,984:INFO:Creating metrics dataframe
2024-09-11 12:27:03,985:INFO:Uploading results into container
2024-09-11 12:27:03,985:INFO:Uploading model into container now
2024-09-11 12:27:03,986:INFO:_master_model_container: 10
2024-09-11 12:27:03,986:INFO:_display_container: 2
2024-09-11 12:27:03,986:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=1521, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2024-09-11 12:27:03,986:INFO:create_model() successfully completed......................................
2024-09-11 12:27:04,058:INFO:SubProcess create_model() end ==================================
2024-09-11 12:27:04,058:INFO:Creating metrics dataframe
2024-09-11 12:27:04,061:INFO:Initializing Linear Discriminant Analysis
2024-09-11 12:27:04,061:INFO:Total runtime is 0.28480104207992557 minutes
2024-09-11 12:27:04,061:INFO:SubProcess create_model() called ==================================
2024-09-11 12:27:04,061:INFO:Initializing create_model()
2024-09-11 12:27:04,061:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002522F2CFB10>, estimator=lda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000252691C6910>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-11 12:27:04,061:INFO:Checking exceptions
2024-09-11 12:27:04,061:INFO:Importing libraries
2024-09-11 12:27:04,061:INFO:Copying training dataset
2024-09-11 12:27:04,064:INFO:Defining folds
2024-09-11 12:27:04,064:INFO:Declaring metric variables
2024-09-11 12:27:04,064:INFO:Importing untrained model
2024-09-11 12:27:04,064:INFO:Linear Discriminant Analysis Imported successfully
2024-09-11 12:27:04,064:INFO:Starting cross validation
2024-09-11 12:27:04,065:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-11 12:27:04,181:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 12:27:04,185:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:04,185:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 12:27:04,185:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 12:27:04,186:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 12:27:04,189:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:04,190:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:04,190:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:04,191:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:04,194:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 12:27:04,195:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:04,196:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:04,197:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:04,198:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:04,201:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 12:27:04,201:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:04,202:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 12:27:04,203:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 12:27:04,204:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:04,204:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:04,205:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:04,205:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:04,205:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 12:27:04,206:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:04,206:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-11 12:27:04,208:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 12:27:04,210:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:04,210:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:04,210:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:04,211:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:04,212:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:04,213:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:04,217:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:04,217:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:04,217:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:04,219:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:04,220:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:04,221:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:04,222:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:04,229:INFO:Calculating mean and std
2024-09-11 12:27:04,229:INFO:Creating metrics dataframe
2024-09-11 12:27:04,231:INFO:Uploading results into container
2024-09-11 12:27:04,231:INFO:Uploading model into container now
2024-09-11 12:27:04,231:INFO:_master_model_container: 11
2024-09-11 12:27:04,231:INFO:_display_container: 2
2024-09-11 12:27:04,232:INFO:LinearDiscriminantAnalysis(covariance_estimator=None, n_components=None,
                           priors=None, shrinkage=None, solver='svd',
                           store_covariance=False, tol=0.0001)
2024-09-11 12:27:04,232:INFO:create_model() successfully completed......................................
2024-09-11 12:27:04,302:INFO:SubProcess create_model() end ==================================
2024-09-11 12:27:04,302:INFO:Creating metrics dataframe
2024-09-11 12:27:04,304:INFO:Initializing Extra Trees Classifier
2024-09-11 12:27:04,304:INFO:Total runtime is 0.28885494867960615 minutes
2024-09-11 12:27:04,304:INFO:SubProcess create_model() called ==================================
2024-09-11 12:27:04,304:INFO:Initializing create_model()
2024-09-11 12:27:04,304:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002522F2CFB10>, estimator=et, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000252691C6910>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-11 12:27:04,304:INFO:Checking exceptions
2024-09-11 12:27:04,304:INFO:Importing libraries
2024-09-11 12:27:04,304:INFO:Copying training dataset
2024-09-11 12:27:04,306:INFO:Defining folds
2024-09-11 12:27:04,306:INFO:Declaring metric variables
2024-09-11 12:27:04,306:INFO:Importing untrained model
2024-09-11 12:27:04,307:INFO:Extra Trees Classifier Imported successfully
2024-09-11 12:27:04,307:INFO:Starting cross validation
2024-09-11 12:27:04,308:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-11 12:27:04,846:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:04,849:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:04,853:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:04,857:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:04,860:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:04,862:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:04,863:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:04,865:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:04,866:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:04,869:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:04,874:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:04,877:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:04,878:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:04,878:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:04,879:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:04,882:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:04,883:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:04,885:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:04,886:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:04,891:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:04,891:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:04,892:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:04,894:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 12:27:04,896:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:04,900:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:04,902:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:04,904:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:04,930:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:04,933:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:04,936:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:04,949:INFO:Calculating mean and std
2024-09-11 12:27:04,949:INFO:Creating metrics dataframe
2024-09-11 12:27:04,951:INFO:Uploading results into container
2024-09-11 12:27:04,951:INFO:Uploading model into container now
2024-09-11 12:27:04,951:INFO:_master_model_container: 12
2024-09-11 12:27:04,951:INFO:_display_container: 2
2024-09-11 12:27:04,952:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=1521, verbose=0,
                     warm_start=False)
2024-09-11 12:27:04,952:INFO:create_model() successfully completed......................................
2024-09-11 12:27:05,029:INFO:SubProcess create_model() end ==================================
2024-09-11 12:27:05,030:INFO:Creating metrics dataframe
2024-09-11 12:27:05,031:INFO:Initializing Light Gradient Boosting Machine
2024-09-11 12:27:05,031:INFO:Total runtime is 0.3009804805119833 minutes
2024-09-11 12:27:05,032:INFO:SubProcess create_model() called ==================================
2024-09-11 12:27:05,032:INFO:Initializing create_model()
2024-09-11 12:27:05,032:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002522F2CFB10>, estimator=lightgbm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000252691C6910>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-11 12:27:05,032:INFO:Checking exceptions
2024-09-11 12:27:05,032:INFO:Importing libraries
2024-09-11 12:27:05,032:INFO:Copying training dataset
2024-09-11 12:27:05,035:INFO:Defining folds
2024-09-11 12:27:05,035:INFO:Declaring metric variables
2024-09-11 12:27:05,035:INFO:Importing untrained model
2024-09-11 12:27:05,035:INFO:Light Gradient Boosting Machine Imported successfully
2024-09-11 12:27:05,035:INFO:Starting cross validation
2024-09-11 12:27:05,036:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-11 12:27:06,432:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:06,438:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:06,444:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:06,449:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:06,456:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:06,462:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:06,466:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:06,468:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:06,473:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:06,490:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:06,502:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:06,506:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:06,531:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:06,536:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:06,542:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:06,553:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:06,563:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:06,569:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:06,587:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:06,593:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:06,596:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:06,598:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:06,600:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:06,603:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 12:27:06,605:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:06,619:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:06,627:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:06,633:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:06,679:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:06,683:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:06,686:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:06,697:INFO:Calculating mean and std
2024-09-11 12:27:06,698:INFO:Creating metrics dataframe
2024-09-11 12:27:06,700:INFO:Uploading results into container
2024-09-11 12:27:06,701:INFO:Uploading model into container now
2024-09-11 12:27:06,701:INFO:_master_model_container: 13
2024-09-11 12:27:06,701:INFO:_display_container: 2
2024-09-11 12:27:06,702:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=1521, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-09-11 12:27:06,702:INFO:create_model() successfully completed......................................
2024-09-11 12:27:06,804:INFO:SubProcess create_model() end ==================================
2024-09-11 12:27:06,804:INFO:Creating metrics dataframe
2024-09-11 12:27:06,806:INFO:Initializing Dummy Classifier
2024-09-11 12:27:06,806:INFO:Total runtime is 0.33055552641550706 minutes
2024-09-11 12:27:06,806:INFO:SubProcess create_model() called ==================================
2024-09-11 12:27:06,807:INFO:Initializing create_model()
2024-09-11 12:27:06,807:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002522F2CFB10>, estimator=dummy, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000252691C6910>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-11 12:27:06,807:INFO:Checking exceptions
2024-09-11 12:27:06,807:INFO:Importing libraries
2024-09-11 12:27:06,807:INFO:Copying training dataset
2024-09-11 12:27:06,810:INFO:Defining folds
2024-09-11 12:27:06,811:INFO:Declaring metric variables
2024-09-11 12:27:06,811:INFO:Importing untrained model
2024-09-11 12:27:06,811:INFO:Dummy Classifier Imported successfully
2024-09-11 12:27:06,811:INFO:Starting cross validation
2024-09-11 12:27:06,812:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-11 12:27:06,924:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:06,927:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:06,928:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:06,930:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:06,930:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:06,932:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:06,933:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:06,933:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:06,933:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 12:27:06,934:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:06,935:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:06,935:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:06,936:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 12:27:06,936:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:06,936:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 12:27:06,938:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:06,938:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:06,938:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 12:27:06,939:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:06,939:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:06,940:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:06,940:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:06,941:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 12:27:06,941:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

fier, f"{metric.capitalize()} is", len(result))

2024-09-11 12:27:06,942:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:06,944:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 12:27:06,944:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:06,945:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:06,946:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:06,947:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:06,947:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 12:27:06,950:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:06,950:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:06,951:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:06,952:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 12:27:06,955:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:06,955:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:06,956:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-11 12:27:06,959:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:06,970:INFO:Calculating mean and std
2024-09-11 12:27:06,970:INFO:Creating metrics dataframe
2024-09-11 12:27:06,972:INFO:Uploading results into container
2024-09-11 12:27:06,972:INFO:Uploading model into container now
2024-09-11 12:27:06,972:INFO:_master_model_container: 14
2024-09-11 12:27:06,972:INFO:_display_container: 2
2024-09-11 12:27:06,972:INFO:DummyClassifier(constant=None, random_state=1521, strategy='prior')
2024-09-11 12:27:06,973:INFO:create_model() successfully completed......................................
2024-09-11 12:27:07,045:INFO:SubProcess create_model() end ==================================
2024-09-11 12:27:07,045:INFO:Creating metrics dataframe
2024-09-11 12:27:07,048:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:339: FutureWarning: Styler.applymap has been deprecated. Use Styler.map instead.
  .applymap(highlight_cols, subset=["TT (Sec)"])

2024-09-11 12:27:07,049:INFO:Initializing create_model()
2024-09-11 12:27:07,049:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002522F2CFB10>, estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=1521, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-11 12:27:07,049:INFO:Checking exceptions
2024-09-11 12:27:07,049:INFO:Importing libraries
2024-09-11 12:27:07,050:INFO:Copying training dataset
2024-09-11 12:27:07,052:INFO:Defining folds
2024-09-11 12:27:07,052:INFO:Declaring metric variables
2024-09-11 12:27:07,052:INFO:Importing untrained model
2024-09-11 12:27:07,052:INFO:Declaring custom model
2024-09-11 12:27:07,053:INFO:Logistic Regression Imported successfully
2024-09-11 12:27:07,053:INFO:Cross validation set to False
2024-09-11 12:27:07,053:INFO:Fitting Model
2024-09-11 12:27:07,195:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-11 12:27:07,196:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=1521, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2024-09-11 12:27:07,196:INFO:create_model() successfully completed......................................
2024-09-11 12:27:07,263:INFO:Creating Dashboard logs
2024-09-11 12:27:07,263:INFO:Model: Logistic Regression
2024-09-11 12:27:07,329:INFO:Logged params: {'C': 1.0, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 1000, 'multi_class': 'auto', 'n_jobs': None, 'penalty': 'l2', 'random_state': 1521, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}
2024-09-11 12:27:07,503:INFO:Initializing predict_model()
2024-09-11 12:27:07,503:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002522F2CFB10>, estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=1521, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=False, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x0000025268E33880>)
2024-09-11 12:27:07,503:INFO:Checking exceptions
2024-09-11 12:27:07,503:INFO:Preloading libraries
2024-09-11 12:27:07,586:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:07,588:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:07,590:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:09,287:INFO:Creating Dashboard logs
2024-09-11 12:27:09,288:INFO:Model: Extra Trees Classifier
2024-09-11 12:27:09,351:INFO:Logged params: {'bootstrap': False, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': None, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 100, 'n_jobs': -1, 'oob_score': False, 'random_state': 1521, 'verbose': 0, 'warm_start': False}
2024-09-11 12:27:09,642:INFO:Creating Dashboard logs
2024-09-11 12:27:09,642:INFO:Model: Ridge Classifier
2024-09-11 12:27:09,704:INFO:Logged params: {'alpha': 1.0, 'class_weight': None, 'copy_X': True, 'fit_intercept': True, 'max_iter': None, 'positive': False, 'random_state': 1521, 'solver': 'auto', 'tol': 0.0001}
2024-09-11 12:27:09,992:INFO:Creating Dashboard logs
2024-09-11 12:27:09,992:INFO:Model: Random Forest Classifier
2024-09-11 12:27:10,056:INFO:Logged params: {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': None, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 100, 'n_jobs': -1, 'oob_score': False, 'random_state': 1521, 'verbose': 0, 'warm_start': False}
2024-09-11 12:27:10,364:INFO:Creating Dashboard logs
2024-09-11 12:27:10,364:INFO:Model: Naive Bayes
2024-09-11 12:27:10,482:INFO:Logged params: {'priors': None, 'var_smoothing': 1e-09}
2024-09-11 12:27:10,754:INFO:Creating Dashboard logs
2024-09-11 12:27:10,754:INFO:Model: Quadratic Discriminant Analysis
2024-09-11 12:27:10,818:INFO:Logged params: {'priors': None, 'reg_param': 0.0, 'store_covariance': False, 'tol': 0.0001}
2024-09-11 12:27:11,077:INFO:Creating Dashboard logs
2024-09-11 12:27:11,077:INFO:Model: K Neighbors Classifier
2024-09-11 12:27:11,143:INFO:Logged params: {'algorithm': 'auto', 'leaf_size': 30, 'metric': 'minkowski', 'metric_params': None, 'n_jobs': -1, 'n_neighbors': 5, 'p': 2, 'weights': 'uniform'}
2024-09-11 12:27:11,421:INFO:Creating Dashboard logs
2024-09-11 12:27:11,422:INFO:Model: SVM - Linear Kernel
2024-09-11 12:27:11,487:INFO:Logged params: {'alpha': 0.0001, 'average': False, 'class_weight': None, 'early_stopping': False, 'epsilon': 0.1, 'eta0': 0.001, 'fit_intercept': True, 'l1_ratio': 0.15, 'learning_rate': 'optimal', 'loss': 'hinge', 'max_iter': 1000, 'n_iter_no_change': 5, 'n_jobs': -1, 'penalty': 'l2', 'power_t': 0.5, 'random_state': 1521, 'shuffle': True, 'tol': 0.001, 'validation_fraction': 0.1, 'verbose': 0, 'warm_start': False}
2024-09-11 12:27:11,799:INFO:Creating Dashboard logs
2024-09-11 12:27:11,799:INFO:Model: Dummy Classifier
2024-09-11 12:27:11,863:INFO:Logged params: {'constant': None, 'random_state': 1521, 'strategy': 'prior'}
2024-09-11 12:27:12,128:INFO:Creating Dashboard logs
2024-09-11 12:27:12,128:INFO:Model: Linear Discriminant Analysis
2024-09-11 12:27:12,198:INFO:Logged params: {'covariance_estimator': None, 'n_components': None, 'priors': None, 'shrinkage': None, 'solver': 'svd', 'store_covariance': False, 'tol': 0.0001}
2024-09-11 12:27:12,465:INFO:Creating Dashboard logs
2024-09-11 12:27:12,465:INFO:Model: Decision Tree Classifier
2024-09-11 12:27:12,530:INFO:Logged params: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': None, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': 1521, 'splitter': 'best'}
2024-09-11 12:27:12,807:INFO:Creating Dashboard logs
2024-09-11 12:27:12,807:INFO:Model: Ada Boost Classifier
2024-09-11 12:27:12,873:INFO:Logged params: {'algorithm': 'SAMME.R', 'estimator': None, 'learning_rate': 1.0, 'n_estimators': 50, 'random_state': 1521}
2024-09-11 12:27:13,137:INFO:Creating Dashboard logs
2024-09-11 12:27:13,137:INFO:Model: Gradient Boosting Classifier
2024-09-11 12:27:13,203:INFO:Logged params: {'ccp_alpha': 0.0, 'criterion': 'friedman_mse', 'init': None, 'learning_rate': 0.1, 'loss': 'log_loss', 'max_depth': 3, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 100, 'n_iter_no_change': None, 'random_state': 1521, 'subsample': 1.0, 'tol': 0.0001, 'validation_fraction': 0.1, 'verbose': 0, 'warm_start': False}
2024-09-11 12:27:13,499:INFO:Creating Dashboard logs
2024-09-11 12:27:13,499:INFO:Model: Light Gradient Boosting Machine
2024-09-11 12:27:13,562:INFO:Logged params: {'boosting_type': 'gbdt', 'class_weight': None, 'colsample_bytree': 1.0, 'importance_type': 'split', 'learning_rate': 0.1, 'max_depth': -1, 'min_child_samples': 20, 'min_child_weight': 0.001, 'min_split_gain': 0.0, 'n_estimators': 100, 'n_jobs': -1, 'num_leaves': 31, 'objective': None, 'random_state': 1521, 'reg_alpha': 0.0, 'reg_lambda': 0.0, 'subsample': 1.0, 'subsample_for_bin': 200000, 'subsample_freq': 0}
2024-09-11 12:27:13,867:INFO:_master_model_container: 14
2024-09-11 12:27:13,867:INFO:_display_container: 2
2024-09-11 12:27:13,868:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=1521, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2024-09-11 12:27:13,868:INFO:compare_models() successfully completed......................................
2024-09-11 12:27:13,868:INFO:Initializing predict_model()
2024-09-11 12:27:13,868:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002522F2CFB10>, estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=1521, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False), probability_threshold=None, encoded_labels=False, raw_score=True, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x0000025247846200>)
2024-09-11 12:27:13,868:INFO:Checking exceptions
2024-09-11 12:27:13,868:INFO:Preloading libraries
2024-09-11 12:27:13,868:INFO:Set up data.
2024-09-11 12:27:13,872:INFO:Set up index.
2024-09-11 12:27:13,912:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:13,915:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:13,918:WARNING:H:\Projects\MI_OPS_2024\PE_MI_OPS\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'US') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2024-09-11 12:27:14,006:INFO:Initializing save_model()
2024-09-11 12:27:14,007:INFO:save_model(model=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=1521, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False), model_name=my_best_pipeline, prep_pipe_=Pipeline(memory=FastMemory(location=C:\Users\ipkov\AppData\Local\Temp\joblib),
         steps=[('label_encoding',
                 TransformerWrapperWithInverse(exclude=None, include=None,
                                               transformer=LabelEncoder())),
                ('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['MPG', 'Cylinders', 'Displacement',
                                             'Horsepower', 'Weight',
                                             'Acceleration', 'Model'],
                                    transformer=SimpleImp...
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='most_frequent'))),
                ('rest_encoding',
                 TransformerWrapper(exclude=None, include=['Car'],
                                    transformer=TargetEncoder(cols=['Car'],
                                                              drop_invariant=False,
                                                              handle_missing='return_nan',
                                                              handle_unknown='value',
                                                              hierarchy=None,
                                                              min_samples_leaf=20,
                                                              return_df=True,
                                                              smoothing=10,
                                                              verbose=0)))],
         verbose=False), verbose=True, use_case=MLUsecase.CLASSIFICATION, kwargs={})
2024-09-11 12:27:14,007:INFO:Adding model into prep_pipe
2024-09-11 12:27:14,011:INFO:my_best_pipeline.pkl saved in current working directory
2024-09-11 12:27:14,015:INFO:Pipeline(memory=Memory(location=None),
         steps=[('label_encoding',
                 TransformerWrapperWithInverse(exclude=None, include=None,
                                               transformer=LabelEncoder())),
                ('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['MPG', 'Cylinders', 'Displacement',
                                             'Horsepower', 'Weight',
                                             'Acceleration', 'Model'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_...
                                                              handle_unknown='value',
                                                              hierarchy=None,
                                                              min_samples_leaf=20,
                                                              return_df=True,
                                                              smoothing=10,
                                                              verbose=0))),
                ('trained_model',
                 LogisticRegression(C=1.0, class_weight=None, dual=False,
                                    fit_intercept=True, intercept_scaling=1,
                                    l1_ratio=None, max_iter=1000,
                                    multi_class='auto', n_jobs=None,
                                    penalty='l2', random_state=1521,
                                    solver='lbfgs', tol=0.0001, verbose=0,
                                    warm_start=False))],
         verbose=False)
2024-09-11 12:27:14,015:INFO:save_model() successfully completed......................................
2024-09-11 12:41:54,129:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-11 12:41:54,129:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-11 12:41:54,130:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-11 12:41:54,130:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-11 12:41:57,281:INFO:Initializing load_model()
2024-09-11 12:41:57,282:INFO:load_model(model_name=file:///H:\Projects\MI_OPS_2024\PE_MI_OPS///artifacts/Transformation Pipeline, platform=None, authentication=None, verbose=True)
2024-09-11 12:43:03,914:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-11 12:43:03,914:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-11 12:43:03,915:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-11 12:43:03,915:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-11 12:43:07,018:INFO:Initializing load_model()
2024-09-11 12:43:07,018:INFO:load_model(model_name=file:///H:\Projects\MI_OPS_2024\PE_MI_OPS\mlruns\854418700138634618\763bb719c017444f8c0e79c9153f9e32rtifacts\Transformation Pipeline.pkl/854418700138634618/763bb719c017444f8c0e79c9153f9e32/artifacts/Transformation Pipeline, platform=None, authentication=None, verbose=True)
2024-09-11 12:43:30,388:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-11 12:43:30,388:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-11 12:43:30,388:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-11 12:43:30,389:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-11 12:43:33,471:INFO:Initializing load_model()
2024-09-11 12:43:33,471:INFO:load_model(model_name=file:///H:\Projects\MI_OPS_2024\PE_MI_OPS\mlruns\854418700138634618\763bb719c017444f8c0e79c9153f9e32rtifacts\Transformation Pipeline.pkl/854418700138634618/763bb719c017444f8c0e79c9153f9e32/artifacts/Transformation Pipeline, platform=None, authentication=None, verbose=True)
2024-09-11 12:44:14,263:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-11 12:44:14,264:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-11 12:44:14,264:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-11 12:44:14,264:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-11 12:44:17,352:INFO:Initializing load_model()
2024-09-11 12:44:17,352:INFO:load_model(model_name=file:///H:\Projects\MI_OPS_2024\PE_MI_OPS\mlruns\854418700138634618\763bb719c017444f8c0e79c9153f9e32\artifacts\Transformation Pipeline.pkl/854418700138634618/763bb719c017444f8c0e79c9153f9e32/artifacts/Transformation Pipeline, platform=None, authentication=None, verbose=True)
2024-09-11 12:44:40,144:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-11 12:44:40,145:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-11 12:44:40,145:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-11 12:44:40,145:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-11 12:45:12,958:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-11 12:45:12,958:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-11 12:45:12,959:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-11 12:45:12,959:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-11 12:45:16,652:INFO:Initializing load_model()
2024-09-11 12:45:16,652:INFO:load_model(model_name=H:\Projects\MI_OPS_2024\PE_MI_OPS\mlruns\854418700138634618\763bb719c017444f8c0e79c9153f9e32\artifacts\Transformation Pipeline.pkl, platform=None, authentication=None, verbose=True)
2024-09-11 12:45:33,436:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-11 12:45:33,438:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-11 12:45:33,438:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-11 12:45:33,438:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-11 12:45:38,863:INFO:Initializing load_model()
2024-09-11 12:45:38,863:INFO:load_model(model_name=H:\Projects\MI_OPS_2024\PE_MI_OPS\mlruns\854418700138634618\763bb719c017444f8c0e79c9153f9e32\artifacts\Transformation Pipeline, platform=None, authentication=None, verbose=True)
2024-09-11 12:48:34,871:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-11 12:48:34,871:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-11 12:48:34,871:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-11 12:48:34,871:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-11 12:48:36,983:INFO:Initializing load_model()
2024-09-11 12:48:36,984:INFO:load_model(model_name=H:\Projects\MI_OPS_2024\PE_MI_OPS\mlruns\854418700138634618\763bb719c017444f8c0e79c9153f9e32\artifacts\Transformation Pipeline, platform=None, authentication=None, verbose=True)
2024-09-11 12:50:40,889:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-11 12:50:40,889:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-11 12:50:40,889:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-11 12:50:40,889:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-11 12:50:42,983:INFO:Initializing load_model()
2024-09-11 12:50:42,983:INFO:load_model(model_name=H:\Projects\MI_OPS_2024\PE_MI_OPS\mlruns\854418700138634618\763bb719c017444f8c0e79c9153f9e32\artifacts\Transformation Pipeline, platform=None, authentication=None, verbose=True)
2024-09-11 12:50:43,023:INFO:Initializing predict_model()
2024-09-11 12:50:43,023:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001F63DBE7B90>, estimator=Pipeline(memory=Memory(location=None),
         steps=[('label_encoding',
                 TransformerWrapperWithInverse(transformer=LabelEncoder())),
                ('numerical_imputer',
                 TransformerWrapper(include=['MPG', 'Cylinders', 'Displacement',
                                             'Horsepower', 'Weight',
                                             'Acceleration', 'Model'],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=['Car'],
                                    transformer=SimpleImputer(strategy='most_frequent'))),
                ('rest_encoding',
                 TransformerWrapper(include=['Car'],
                                    transformer=TargetEncoder(cols=['Car'],
                                                              handle_missing='return_nan')))]), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x000001F677745F80>)
2024-09-11 12:50:43,023:INFO:Checking exceptions
2024-09-11 12:50:43,023:INFO:Preloading libraries
2024-09-11 12:50:43,023:INFO:Set up data.
2024-09-11 12:50:43,028:INFO:Set up index.
2024-09-11 12:50:57,801:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-11 12:50:57,801:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-11 12:50:57,801:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-11 12:50:57,801:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-11 12:50:59,884:INFO:Initializing load_model()
2024-09-11 12:50:59,884:INFO:load_model(model_name=H:\Projects\MI_OPS_2024\PE_MI_OPS\mlruns\854418700138634618\763bb719c017444f8c0e79c9153f9e32\artifacts\Transformation Pipeline, platform=None, authentication=None, verbose=True)
2024-09-11 12:50:59,924:INFO:Initializing predict_model()
2024-09-11 12:50:59,924:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000023CA69E7B90>, estimator=Pipeline(memory=Memory(location=None),
         steps=[('label_encoding',
                 TransformerWrapperWithInverse(transformer=LabelEncoder())),
                ('numerical_imputer',
                 TransformerWrapper(include=['MPG', 'Cylinders', 'Displacement',
                                             'Horsepower', 'Weight',
                                             'Acceleration', 'Model'],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=['Car'],
                                    transformer=SimpleImputer(strategy='most_frequent'))),
                ('rest_encoding',
                 TransformerWrapper(include=['Car'],
                                    transformer=TargetEncoder(cols=['Car'],
                                                              handle_missing='return_nan')))]), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x0000023CE0585F80>)
2024-09-11 12:50:59,924:INFO:Checking exceptions
2024-09-11 12:50:59,924:INFO:Preloading libraries
2024-09-11 12:50:59,925:INFO:Set up data.
2024-09-11 12:50:59,929:INFO:Set up index.
2024-09-11 12:51:56,859:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-11 12:51:56,859:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-11 12:51:56,860:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-11 12:51:56,860:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-11 12:51:59,994:INFO:Initializing load_model()
2024-09-11 12:51:59,995:INFO:load_model(model_name=H:\Projects\MI_OPS_2024\PE_MI_OPS\mlruns\854418700138634618\763bb719c017444f8c0e79c9153f9e32\artifacts\Transformation Pipeline, platform=None, authentication=None, verbose=True)
2024-09-11 12:53:31,676:INFO:Initializing predict_model()
2024-09-11 12:53:31,676:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002364D9D3C50>, estimator=Pipeline(memory=Memory(location=None),
         steps=[('label_encoding',
                 TransformerWrapperWithInverse(transformer=LabelEncoder())),
                ('numerical_imputer',
                 TransformerWrapper(include=['MPG', 'Cylinders', 'Displacement',
                                             'Horsepower', 'Weight',
                                             'Acceleration', 'Model'],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=['Car'],
                                    transformer=SimpleImputer(strategy='most_frequent'))),
                ('rest_encoding',
                 TransformerWrapper(include=['Car'],
                                    transformer=TargetEncoder(cols=['Car'],
                                                              handle_missing='return_nan')))]), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x000002364D9E4400>)
2024-09-11 12:53:31,676:INFO:Checking exceptions
2024-09-11 12:53:31,676:INFO:Preloading libraries
2024-09-11 12:53:31,676:INFO:Set up data.
2024-09-11 12:53:31,684:INFO:Set up index.
